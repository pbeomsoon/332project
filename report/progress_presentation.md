# ë¶„ì‚° ì •ë ¬ ì‹œìŠ¤í…œ ì¤‘ê°„ ë°œí‘œ
**Team Silver** - ê¶Œë™ì—°, ë°•ë²”ìˆœ, ì„ì§€í›ˆ

**ë°œí‘œì¼**: 2025-11-16
**í”„ë¡œì íŠ¸**: Fault-Tolerant Distributed Sorting System

---

## ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [Challenges (ë„ì „ ê³¼ì œ)](#2-challenges-ë„ì „-ê³¼ì œ)
3. [í•´ê²° ë°©ì•ˆ](#3-í•´ê²°-ë°©ì•ˆ)
4. [ê°œë°œ ë°©ë²•ë¡ : TDD](#4-ê°œë°œ-ë°©ë²•ë¡ -tdd)
5. [í˜„ì¬ ì§„í–‰ ìƒí™© (Week 4-5)](#5-í˜„ì¬-ì§„í–‰-ìƒí™©-week-4-5)
6. [í–¥í›„ ê³„íš (Week 6-8)](#6-í–¥í›„-ê³„íš-week-6-8)
7. [Q&A ì¤€ë¹„](#7-qa-ì¤€ë¹„)
8. [ì°¸ê³  ë¬¸í—Œ](#8-ì°¸ê³ -ë¬¸í—Œ)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ

ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚° ì €ì¥ëœ ëŒ€ìš©ëŸ‰ key/value ë ˆì½”ë“œë¥¼ **ì •ë ¬**í•˜ëŠ” **ì¥ì•  í—ˆìš©ì„±** ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬í˜„

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­

| í•­ëª© | ìš”êµ¬ì‚¬í•­ |
|------|---------|
| **ì…ë ¥** | ì—¬ëŸ¬ Worker ë…¸ë“œì— ë¶„ì‚°ëœ ë¯¸ì •ë ¬ ë°ì´í„° |
| **ì¶œë ¥** | ì „ì—­ì ìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„° (partition.0, partition.1, ...) |
| **ë ˆì½”ë“œ** | 100-byte (10-byte key + 90-byte value) |
| **ì •ë ¬** | Keyë§Œ ì‚¬ìš© (unsigned byte ë¹„êµ) |
| **ê²€ì¦** | gensort/valsort ë„êµ¬ í™œìš© |

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

```
ì–¸ì–´:        Scala 2.13
ë¹Œë“œ ë„êµ¬:   SBT 1.9.7
RPC:         gRPC + Protocol Buffers
í…ŒìŠ¤íŠ¸:      ScalaTest (TDD)
ë²„ì „ ê´€ë¦¬:   Git
```

### 1.4 ë ˆì½”ë“œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Key (10B)    â”‚ Value (90B)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  0            10                                      100

íŠ¹ì§•:
- ê³ ì • ê¸¸ì´: 100 ë°”ì´íŠ¸
- Keyë§Œ ì •ë ¬ ê¸°ì¤€ (unsigned ë¹„êµ)
- ValueëŠ” Keyì™€ í•¨ê»˜ ì´ë™
```

---

## 2. Challenges (ë„ì „ ê³¼ì œ)

### Challenge 1: ì…ë ¥ì´ ë©”ëª¨ë¦¬ë³´ë‹¤ í¼

**ë¬¸ì œ**:
- ì…ë ¥ ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ë“¤ì–´ê°€ì§€ ì•ŠìŒ
- ì˜ˆ: ì…ë ¥ 50GB, ë©”ëª¨ë¦¬ 8GB

**í•´ê²°ì±…**: **Disk-based Merge Sort** (External Sort)

```
50GB ì…ë ¥
  â†“
Read â†’ 100MB chunks â†’ Sort/Write (ë³‘ë ¬)
  â†“
Merge (merging 500 files) using K-way merge
  â†“
50GB ì •ë ¬ëœ ì¶œë ¥
```

### Challenge 2: ì…ë ¥ì´ ë””ìŠ¤í¬ë³´ë‹¤ í¼ (ë¶„ì‚° í™˜ê²½)

**ë¬¸ì œ**:
- ì…ë ¥ ë°ì´í„°ê°€ ë‹¨ì¼ ë””ìŠ¤í¬ì— ë“¤ì–´ê°€ì§€ ì•ŠìŒ
- ì˜ˆ: ì…ë ¥ 10TB, ë””ìŠ¤í¬ 1TB
- ì…ë ¥/ì¶œë ¥ì´ **ì—¬ëŸ¬ ë¨¸ì‹ **ì— ë¶„ì‚° ì €ì¥

**í•´ê²°ì±…**: **Distributed Sorting** (Master-Worker)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 0   â”‚       â”‚  Worker 1   â”‚       â”‚  Worker 2   â”‚
â”‚  Input: 50GBâ”‚       â”‚  Input: 50GBâ”‚       â”‚  Input: 50GBâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                     â†“                     â†“
    Sort/Partition     Sort/Partition     Sort/Partition
       â†“                     â†“                     â†“
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Shuffle (Network) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                     Merge on each worker
                            â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â†“                â†“                â†“
       50GB #1          50GB #2          50GB #3
    (partition.0-2)  (partition.3-5)  (partition.6-8)
```

### Challenge 3: Workers may crash

**ë¬¸ì œ**:
- Workerê°€ ì‹¤í–‰ ì¤‘ crash (killed by OS)
- **ëª¨ë“  ì¤‘ê°„ ë°ì´í„° ì†ì‹¤** ("All its intermediate data is lost")
- ê°™ì€ ë…¸ë“œì—ì„œ ìƒˆ Worker ì‹œì‘ (ê°™ì€ íŒŒë¼ë¯¸í„°)

**ìš”êµ¬ì‚¬í•­**: **Fault-tolerant system**
- ìƒˆ Workerê°€ ê¸°ì¡´ Workerì™€ **ë™ì¼í•œ ì¶œë ¥ ìƒì„±** (deterministic)
- ì „ì²´ ì‹œìŠ¤í…œì´ ì •í™•í•œ ê²°ê³¼ ìƒì„±

**í•´ê²°ì±…**: **2-Layer Fault Tolerance (Checkpoint + Replication)**

```
Worker 2 crash during Shuffle:
  Last checkpoint: PHASE_SORTING (100% ì™„ë£Œ)
  Lost data: P6, P7, P8 (intermediate partition files)

Worker 2' restart:
  1. Load checkpoint (Layer 1)
     â†’ Restore state (shuffleMap, partitionBoundaries, ...)
     â†’ Resume point: PHASE_SORTING

  2. Recover lost data from replicas (Layer 2)
     â†’ Fetch P6 from Worker 1 (backup copy)
     â†’ Fetch P7 from Worker 0 (backup copy)
     â†’ Fetch P8 from Worker 3 (backup copy)
     â†’ Checksum verification âœ…

  3. Resume from last completed phase
     â†’ Shuffle â†’ Merge â†’ Complete
     â­ Sampling/Sort ìŠ¤í‚µ (ë¹ ë¥¸ ë³µêµ¬)

Result:
  âœ… Fault-tolerant system (ìŠ¬ë¼ì´ë“œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)
  âœ… ë™ì¼í•œ ìµœì¢… ì¶œë ¥ (deterministic recovery)
```

### Additional Requirements

#### Requirement 1: ASCII/Binary ìë™ ê°ì§€
- ASCIIì™€ Binary ì…ë ¥ì„ **ì˜µì…˜ ì—†ì´** ì²˜ë¦¬
- íŒŒì¼ í˜•ì‹ì„ ìë™ìœ¼ë¡œ ê°ì§€

#### Requirement 2: ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸
- ì…ë ¥ ë””ë ‰í† ë¦¬ëŠ” **ì½ê¸° ì „ìš©**
- ì…ë ¥ íŒŒì¼ ì‚­ì œ ê¸ˆì§€
- ì…ë ¥ ë””ë ‰í† ë¦¬ì— ìƒˆ íŒŒì¼ ìƒì„± ê¸ˆì§€

#### Requirement 3: ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬
- ì¶œë ¥ ë””ë ‰í† ë¦¬ëŠ” **ìµœì¢… íŒŒì¼ë§Œ** í¬í•¨
- ì„ì‹œ íŒŒì¼/ë””ë ‰í† ë¦¬ ìƒì„± ê°€ëŠ¥, ë‹¨ **ì‘ì—… ì™„ë£Œ í›„ ì‚­ì œ**

#### Requirement 4: í¬íŠ¸ í•˜ë“œì½”ë”© ê¸ˆì§€
- íŠ¹ì • í¬íŠ¸ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ë§ ê²ƒ
- ë‹¤ì–‘í•œ ì…ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ì—¬ëŸ¬ Worker ì‹¤í–‰ ê°€ëŠ¥

---

## 3. í•´ê²° ë°©ì•ˆ

### 3.1 ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Master Node                        â”‚
â”‚  - Worker ë“±ë¡ ê´€ë¦¬                                  â”‚
â”‚  - ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°                       â”‚
â”‚  - shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸                    â”‚
â”‚  - Phase ë™ê¸°í™” ì¡°ìœ¨                                 â”‚
â”‚  - ìµœì¢… Worker ìˆœì„œ ì¶œë ¥                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ gRPC
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚          â”‚          â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚ Worker 0 â”‚   â”‚Worker 1 â”‚ â”‚Worker 2 â”‚ â”‚Worker 3 â”‚
   â”‚          â”‚   â”‚         â”‚ â”‚         â”‚ â”‚         â”‚
   â”‚Input:    â”‚   â”‚Input:   â”‚ â”‚Input:   â”‚ â”‚Input:   â”‚
   â”‚50GB      â”‚   â”‚50GB     â”‚ â”‚50GB     â”‚ â”‚50GB     â”‚
   â”‚          â”‚   â”‚         â”‚ â”‚         â”‚ â”‚         â”‚
   â”‚Output:   â”‚   â”‚Output:  â”‚ â”‚Output:  â”‚ â”‚Output:  â”‚
   â”‚P0,P1,P2  â”‚   â”‚P3,P4,P5 â”‚ â”‚P6,P7,P8 â”‚ â”‚P9,P10,P11â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚          â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       Worker-to-Worker Shuffle (gRPC Streaming)
```

### 3.2 5-Phase ì‹¤í–‰ íë¦„

```
Phase 0: Initialization
  â”œâ”€ Master ì‹œì‘, Workerë“¤ ë“±ë¡ ëŒ€ê¸°
  â”œâ”€ ê° Worker ì‹œì‘, Masterì— ì—°ê²°
  â”œâ”€ Masterê°€ Workerì— index í• ë‹¹
  â””â”€ ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦

Phase 1: Sampling
  â”œâ”€ ê° Workerê°€ ì…ë ¥ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
  â”œâ”€ ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚° (0.01% ~ 1%)
  â”œâ”€ Masterì—ê²Œ ìƒ˜í”Œ ì „ì†¡
  â”œâ”€ Masterê°€ ì „ì²´ ìƒ˜í”Œ ì •ë ¬
  â”œâ”€ íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚° (Nê°œ or Mê°œ)
  â””â”€ shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸

Phase 2: Sort & Partition
  â”œâ”€ ê° Workerê°€ ì…ë ¥ íŒŒì¼ ì½ê¸° (ASCII/Binary ìë™ ì²˜ë¦¬)
  â”œâ”€ External Sort: Chunk ë‹¨ìœ„ ì •ë ¬ (ë³‘ë ¬)
  â”‚   â””â”€ ë©”ëª¨ë¦¬ ì œí•œ ì¤€ìˆ˜ (512MB chunks)
  â”œâ”€ íŒŒí‹°ì…˜ ê²½ê³„ì— ë”°ë¼ ë¶„í•  (Nê°œ ë˜ëŠ” Mê°œ)
  â””â”€ íŒŒí‹°ì…˜ë³„ ì„ì‹œ íŒŒì¼ ìƒì„±

Phase 3: Shuffle
  â”œâ”€ shuffleMapì— ë”°ë¼ íŒŒí‹°ì…˜ ì „ì†¡
  â”œâ”€ Worker ê°„ ë„¤íŠ¸ì›Œí¬ í†µì‹  (gRPC streaming)
  â”œâ”€ ì¬ì‹œë„ ë¡œì§ (ì§€ìˆ˜ ë°±ì˜¤í”„)
  â””â”€ ìˆ˜ì‹  í™•ì¸ ë° ì„ì‹œ ì €ì¥

Phase 4: Merge
  â”œâ”€ ê° Workerê°€ ë°›ì€ íŒŒí‹°ì…˜ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ K-way merge
  â”œâ”€ Priority Queue (min-heap) ì‚¬ìš©
  â”œâ”€ ìµœì¢… partition.n íŒŒì¼ ìƒì„±
  â””â”€ Atomic write ë³´ì¥ (temp + rename)

Phase 5: Completion
  â”œâ”€ ê° Workerê°€ Masterì—ê²Œ ì™„ë£Œ ë³´ê³ 
  â”œâ”€ Masterê°€ ì „ì²´ ì‘ì—… ì™„ë£Œ í™•ì¸
  â”œâ”€ Masterê°€ ì •ë ¬ëœ Worker ì£¼ì†Œ ì¶œë ¥ (stdout)
  â””â”€ ì„ì‹œ íŒŒì¼ ì •ë¦¬
```

---

#### ğŸ“Š Phase 1 Sampling ìƒì„¸ ì„¤ëª…

**í•µì‹¬ ì§ˆë¬¸**: "Masterê°€ ì „ì²´ ìƒ˜í”Œ ì •ë ¬"ì€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ê°€?

**ëª©ì **: ì „ì²´ ë°ì´í„°ì˜ key ë¶„í¬ë¥¼ íŒŒì•…í•˜ê³  **ê· ë“±í•œ í¬ê¸°ì˜ íŒŒí‹°ì…˜**ì„ ë§Œë“¤ê¸° ìœ„í•´

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ (4 Workers, 12 Partitions)**:

**Step 1: ê° Workerê°€ ìƒ˜í”Œ ì¶”ì¶œ**
```scala
class Sampler(sampleRate: Double = 0.1) {  // 10% ìƒ˜í”Œë§
  def extractSamples(file: File): Seq[Record] = {
    // Random sampling
    if (random.nextDouble() < sampleRate) {
      samples += record
    }
  }
}
```

```
Worker 0 ìƒ˜í”Œ: [0x15, 0x89, 0x23, 0xAA, 0x12, ...] (100ê°œ keys)
Worker 1 ìƒ˜í”Œ: [0x67, 0x12, 0xAB, 0x45, 0xF3, ...] (100ê°œ keys)
Worker 2 ìƒ˜í”Œ: [0x34, 0x78, 0xCC, 0x9A, 0x01, ...] (100ê°œ keys)
Worker 3 ìƒ˜í”Œ: [0x99, 0x3F, 0xDD, 0x6B, 0x28, ...] (100ê°œ keys)
```

**Step 2: Masterê°€ ëª¨ë“  ìƒ˜í”Œì„ ëª¨ìŒ**
```
allSamples (400ê°œ keys):
  [0x15, 0x89, ..., 0x67, 0x12, ..., 0x34, 0x78, ..., 0x99, 0x3F, ...]
```

**Step 3: â­ Masterê°€ ìƒ˜í”Œì„ ì •ë ¬ (Unsigned byte ë¹„êµ)**
```scala
// MasterService.scalaì˜ computePartitionBoundaries()
val sortedSamples = allSamples.sorted(byteArrayOrdering)

implicit val byteArrayOrdering: Ordering[Array[Byte]] = new Ordering[Array[Byte]] {
  override def compare(x: Array[Byte], y: Array[Byte]): Int = {
    var i = 0
    while (i < x.length && i < y.length) {
      val cmp = java.lang.Integer.compareUnsigned(x(i) & 0xFF, y(i) & 0xFF)
      if (cmp != 0) return cmp
      i += 1
    }
    x.length - y.length
  }
}
```

```
sortedSamples (400ê°œ keys, ì˜¤ë¦„ì°¨ìˆœ):
  [0x01, 0x12, 0x12, 0x15, 0x23, 0x28, 0x34, 0x3F, 0x45, ..., 0xF3, 0xFF]
  â†‘                                                                      â†‘
  ê°€ì¥ ì‘ì€ key                                                  ê°€ì¥ í° key
```

**Step 4: ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ íŒŒí‹°ì…˜ ê²½ê³„ ì„ íƒ**
```scala
val step = sortedSamples.length / numPartitions  // 400 / 12 = 33
partitionBoundaries = (1 until numPartitions).map { i =>
  sortedSamples(i * step)  // 33ë²ˆì§¸, 66ë²ˆì§¸, 99ë²ˆì§¸, ...
}
```

```
Partition Boundaries (11ê°œ ê²½ê³„, 12ê°œ íŒŒí‹°ì…˜ ìƒì„±):

partitionBoundaries[0]  = sortedSamples[33]  â†’ 0x2F
partitionBoundaries[1]  = sortedSamples[66]  â†’ 0x5A
partitionBoundaries[2]  = sortedSamples[99]  â†’ 0x78
partitionBoundaries[3]  = sortedSamples[132] â†’ 0x95
...
partitionBoundaries[10] = sortedSamples[363] â†’ 0xE8

ê²°ê³¼ì ìœ¼ë¡œ ìƒì„±ë˜ëŠ” íŒŒí‹°ì…˜:
  Partition 0:  [0x00, 0x2F) â† ê°€ì¥ ì‘ì€ keyë“¤
  Partition 1:  [0x2F, 0x5A)
  Partition 2:  [0x5A, 0x78)
  Partition 3:  [0x78, 0x95)
  ...
  Partition 11: [0xE8, 0xFF] â† ê°€ì¥ í° keyë“¤
```

**Step 5: shuffleMap ìƒì„± (Nâ†’M ì „ëµ)**
```scala
def createShuffleMap(numWorkers: Int, numPartitions: Int): Map[Int, Int] = {
  val partitionsPerWorker = numPartitions / numWorkers  // 12 / 4 = 3

  (0 until numPartitions).map { partitionID =>
    val workerID = partitionID / partitionsPerWorker
    partitionID -> workerID
  }.toMap
}

// ê²°ê³¼:
shuffleMap = {
  0â†’0, 1â†’0, 2â†’0,     // P0, P1, P2 â†’ Worker 0
  3â†’1, 4â†’1, 5â†’1,     // P3, P4, P5 â†’ Worker 1
  6â†’2, 7â†’2, 8â†’2,     // P6, P7, P8 â†’ Worker 2
  9â†’3, 10â†’3, 11â†’3    // P9, P10, P11 â†’ Worker 3
}
```

**ì‹œê°í™”**:
```
ì „ì²´ ë°ì´í„° ë¶„í¬ (50GB):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0x00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0xFF    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ ìƒ˜í”Œë§ (10%)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ìƒ˜í”Œ: 400ê°œ keys                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ ì •ë ¬
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sortedSamples: [0x01, 0x12, ..., 0xFF]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ ê· ë“± ë¶„í•  (step = 33)
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ P0 â”‚ P1 â”‚ P2 â”‚ P3 â”‚ P4 â”‚ P5 â”‚... â”‚P10 â”‚P11 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
0x00 0x2F 0x5A 0x78 0x95 ...               0xFF

ê° íŒŒí‹°ì…˜ ì˜ˆìƒ í¬ê¸°: 50GB / 12 â‰ˆ 4.17GB
```

**ì™œ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ëŠ”ê°€?**
1. **ë©”ëª¨ë¦¬ ì œí•œ**: 50GB ì „ì²´ë¥¼ Master ë©”ëª¨ë¦¬ì— ì˜¬ë¦´ ìˆ˜ ì—†ìŒ
2. **ì‹œê°„ ì ˆì•½**: 400ê°œ ìƒ˜í”Œ ì •ë ¬ vs 500,000,000ê°œ ë ˆì½”ë“œ ì •ë ¬
3. **ì¶©ë¶„íˆ ì •í™•**: ìƒ˜í”Œì´ ì „ì²´ ë°ì´í„° ë¶„í¬ë¥¼ ì˜ ëŒ€í‘œí•¨

**ì¥ì **:
- âœ… ê° íŒŒí‹°ì…˜ì˜ **ì˜ˆìƒ í¬ê¸°ê°€ ë¹„ìŠ·í•¨** (ë¡œë“œ ë°¸ëŸ°ì‹±)
- âœ… Workerë³„ ë¶€í•˜ê°€ ê· ë“±í•¨ (ê° Workerê°€ 3ê°œ íŒŒí‹°ì…˜ ì²˜ë¦¬)
- âœ… ìµœì¢… ì¶œë ¥ì´ ì •ë ¬ë¨ (P0 â†’ P1 â†’ ... â†’ P11 ìˆœì„œë¡œ ì „ì—­ ì •ë ¬)
- âœ… ì „ì²´ ë°ì´í„°ë¥¼ ì •ë ¬í•˜ì§€ ì•Šê³ ë„ ë¶„í¬ íŒŒì•… ê°€ëŠ¥

---

### 3.3 Challenge 1 í•´ê²°: External Sort (ë©”ëª¨ë¦¬ ì œí•œ)

**ë¬¸ì œ**: ì…ë ¥ì´ ë©”ëª¨ë¦¬ë³´ë‹¤ í¼ (50GB vs 8GB)

**í•´ê²°ì±…**: 2-Pass External Sort

#### Phase 1: Chunk Sort (ë³‘ë ¬)

```scala
class ExternalSorter(
  memoryLimit: Long = 512 * 1024 * 1024  // 512MB ì œí•œ
) {
  private val recordsPerChunk = (memoryLimit / 100).toInt

  def createSortedChunksParallel(records: Seq[Record]): Seq[File] = {
    val chunks = records.grouped(recordsPerChunk).toSeq  // â­ Chunkë¡œ ë¶„í• 

    // ë³‘ë ¬ ì •ë ¬ (ë©€í‹°ì½”ì–´ í™œìš©)
    val futures = chunks.zipWithIndex.map { case (chunk, index) =>
      Future {
        val sorted = chunk.sorted              // â­ ë©”ëª¨ë¦¬ ë‚´ ì •ë ¬
        writeChunkToFile(sorted, chunkFile)    // â­ ë””ìŠ¤í¬ì— ì €ì¥
        chunkFile
      }
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

#### Phase 2: K-way Merge (Priority Queue)

```scala
class KWayMerger(sortedChunks: Seq[File]) {

  def mergeWithCallback(callback: Record => Unit): Unit = {
    val readers = sortedChunks.map(RecordReader.create)
    val heap = mutable.PriorityQueue[HeapEntry]()  // â­ Min-heap

    // ê° chunkì—ì„œ ì²« ë ˆì½”ë“œ ì½ê¸°
    readers.zipWithIndex.foreach { case (reader, index) =>
      reader.readRecord() match {
        case Some(record) => heap.enqueue(HeapEntry(record, index))
        case None => // Empty chunk
      }
    }

    // ì •ë ¬ëœ ìˆœì„œë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    while (heap.nonEmpty) {
      val entry = heap.dequeue()
      callback(entry.record)  // â­ í•˜ë‚˜ì”© ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)

      // ê°™ì€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìŒ ë ˆì½”ë“œ ì½ê¸°
      readers(entry.sourceIndex).readRecord() match {
        case Some(nextRecord) =>
          heap.enqueue(HeapEntry(nextRecord, entry.sourceIndex))
        case None => // Source exhausted
      }
    }
  }
}
```

**ì¥ì **:
- âœ… ì„ì˜ í¬ê¸° ì…ë ¥ ì²˜ë¦¬ ê°€ëŠ¥ (1KB ~ 10TB+)
- âœ… ë³‘ë ¬ ì •ë ¬ (ë©€í‹°ì½”ì–´ í™œìš©)
- âœ… ìŠ¤íŠ¸ë¦¬ë° ë³‘í•© (ë©”ëª¨ë¦¬ íš¨ìœ¨)

### 3.4 Challenge 2 í•´ê²°: Distributed System (ë¶„ì‚°)

**ë¬¸ì œ**: ì…ë ¥ì´ ë””ìŠ¤í¬ë³´ë‹¤ í¼, ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚°

**í•´ê²°ì±…**: Master-Worker ì•„í‚¤í…ì²˜ + Nâ†’M Partition Strategy

#### Nâ†’M Partition Strategy

**ê°œë…**: íŒŒí‹°ì…˜ ìˆ˜ > Worker ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ M = 3N)

**ì˜ˆì‹œ: 4 Workers, 12 Partitions**

```
Phase 2: Sort & Partition
  Worker 0, 1, 2, 3 ê°ê° â†’ P0~P11 (12ê°œ) ìƒì„±

Phase 3: Shuffle
  P0, P1, P2   â†’ Worker 0
  P3, P4, P5   â†’ Worker 1
  P6, P7, P8   â†’ Worker 2
  P9, P10, P11 â†’ Worker 3

Phase 4: Merge
  Worker 0:
    - 4ê°œ P0 ì¡°ê° K-way merge â†’ partition.0
    - 4ê°œ P1 ì¡°ê° K-way merge â†’ partition.1
    - 4ê°œ P2 ì¡°ê° K-way merge â†’ partition.2

ìµœì¢… ì¶œë ¥:
  /worker0/output/partition.0  â† ê°€ì¥ ì‘ì€ key
  /worker0/output/partition.1
  /worker0/output/partition.2
  /worker1/output/partition.3
  ...
  /worker3/output/partition.11 â† ê°€ì¥ í° key

ì½ê¸° ìˆœì„œ: partition.0 â†’ 1 â†’ 2 â†’ ... â†’ 11 = ì „ì—­ ì •ë ¬ë¨
```

**shuffleMap ìƒì„±**:
```scala
def createShuffleMap(numWorkers: Int, numPartitions: Int): Map[Int, Int] = {
  val partitionsPerWorker = numPartitions / numWorkers

  (0 until numPartitions).map { partitionID =>
    val workerID = partitionID / partitionsPerWorker
    val finalWorkerID = if (workerID >= numWorkers) numWorkers - 1 else workerID
    partitionID -> finalWorkerID
  }.toMap
}

// ì˜ˆì‹œ: createShuffleMap(4, 12)
//   â†’ {0â†’0, 1â†’0, 2â†’0, 3â†’1, 4â†’1, 5â†’1, 6â†’2, 7â†’2, 8â†’2, 9â†’3, 10â†’3, 11â†’3}
```

**ì¥ì **:
- âœ… ë¡œë“œ ë°¸ëŸ°ì‹± ê°œì„  (íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€)
- âœ… ë©€í‹°ì½”ì–´ í™œìš© ì¦ê°€ (ë³‘ë ¬ merge)
- âœ… íŒŒí‹°ì…˜ í¬ê¸° ë¶ˆê· í˜• ì™„í™”

### 3.5 Challenge 3 í•´ê²°: 2-Layer Fault Tolerance

**ë¬¸ì œ**: Workerê°€ ì‹¤í–‰ ì¤‘ crash â†’ **ëª¨ë“  ì¤‘ê°„ ë°ì´í„° ì†ì‹¤**

**í•´ê²°ì±…**: 2-Layer Fault Tolerance
- **Layer 1**: Checkpoint (ì§„í–‰ ìƒíƒœ ë³µêµ¬)
- **Layer 2**: Replication (ì¤‘ê°„ ë°ì´í„° ë³µêµ¬)

---

#### Layer 1: Checkpoint (ì§„í–‰ ìƒíƒœ ë³µêµ¬)

**ëª©ì **: Workerê°€ ì–´ë””ê¹Œì§€ ì§„í–‰í–ˆëŠ”ì§€ ì¶”ì 

**Checkpoint ì €ì¥**

```scala
case class WorkerState(
  processedRecords: Long,                  // ì²˜ë¦¬í•œ ë ˆì½”ë“œ ìˆ˜
  partitionBoundaries: List[Array[Byte]], // íŒŒí‹°ì…˜ ê²½ê³„
  shuffleMap: Map[Int, Int],               // íŒŒí‹°ì…˜ â†’ Worker ë§¤í•‘
  completedPartitions: Set[Int],           // ì™„ë£Œí•œ íŒŒí‹°ì…˜ë“¤
  currentFiles: List[String],              // í˜„ì¬ íŒŒì¼ë“¤
  phaseMetadata: Map[String, String]       // Phase ë©”íƒ€ë°ì´í„°
)

class CheckpointManager(workerId: String) {
  def saveCheckpoint(
    phase: WorkerPhase,
    state: WorkerState,
    progress: Double
  ): Future[String] = Future {
    val checkpointId = s"checkpoint_${System.currentTimeMillis()}_${phase}"
    val checkpoint = Checkpoint(checkpointId, workerId, phase.toString,
                                 Instant.now(), progress, state)

    // JSONìœ¼ë¡œ ì €ì¥: /tmp/distsort/checkpoints/{workerId}/{checkpointId}.json
    val file = checkpointPath.resolve(s"$checkpointId.json").toFile
    val writer = new PrintWriter(file)
    writer.write(gson.toJson(checkpoint))
    writer.close()

    checkpointId
  }
}
```

**ì €ì¥ ì‹œì **:
```scala
performSampling()
  â†’ savePhaseCheckpoint(PHASE_SAMPLING, 1.0)       // âœ…

getPartitionConfiguration()
  â†’ savePhaseCheckpoint(PHASE_WAITING_FOR_PARTITIONS, 1.0)  // âœ…

performLocalSort()
  â†’ savePhaseCheckpoint(PHASE_SORTING, 1.0)        // âœ…

performShuffle()
  â†’ savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)      // âœ…

performMerge()
  â†’ savePhaseCheckpoint(PHASE_MERGING, 1.0)        // âœ…
  â†’ checkpointManager.deleteAllCheckpoints()       // ì„±ê³µ ì‹œ ì‚­ì œ
```

#### ë³µêµ¬ ë¡œì§

```scala
class Worker(...) {
  def run(): Unit = {
    // 1. Checkpointì—ì„œ ë³µêµ¬ ì‹œë„
    val recoveredFromCheckpoint = recoverFromCheckpoint()

    if (!recoveredFromCheckpoint || currentPhase.get() == PHASE_INITIALIZING) {
      performSampling()
    }

    // ë³µêµ¬ëœ Phaseì— ë”°ë¼ ì ì ˆí•œ ë‹¨ê³„ë¶€í„° ì¬ê°œ
    if (currentPhase.get() == PHASE_SAMPLING || ...) {
      getPartitionConfiguration()
      savePhaseCheckpoint(PHASE_WAITING_FOR_PARTITIONS, 1.0)
    }

    if (currentPhase.get() == PHASE_SORTING || ...) {
      performLocalSort()
      savePhaseCheckpoint(PHASE_SORTING, 1.0)

      performShuffle()
      savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)
    }

    performMerge()
    savePhaseCheckpoint(PHASE_MERGING, 1.0)

    checkpointManager.deleteAllCheckpoints()
  }

  private def recoverFromCheckpoint(): Boolean = {
    checkpointManager.loadLatestCheckpoint() match {
      case Some(checkpoint) =>
        // ìƒíƒœ ë³µì›
        partitionBoundaries = checkpoint.state.partitionBoundaries.toArray
        shuffleMap = checkpoint.state.shuffleMap
        completedPartitions = checkpoint.state.completedPartitions

        // Phase ë³µì›
        currentPhase.set(WorkerPhase.fromName(checkpoint.phase))
        true  // ë³µêµ¬ ì„±ê³µ

      case None =>
        false  // ì²˜ìŒë¶€í„° ì‹œì‘
    }
  }
}
```

**ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤**:
```
Worker crash during Shuffle:
  Last checkpoint: PHASE_SORTING (100% ì™„ë£Œ)

Worker restart:
  1. recoverFromCheckpoint() ì„±ê³µ
  2. currentPhase = PHASE_SORTING (ë³µì›)
  3. performShuffle() ì¬ì‹œì‘
     â­ Sampling/SortëŠ” ìŠ¤í‚µ (ì‹œê°„ ëŒ€í­ ì ˆì•½)
  4. Merge â†’ ì™„ë£Œ

Result:
  âœ… ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ
  âœ… ë¹ ë¥¸ ë³µêµ¬ (ì „ì²´ ì¬ì‹œì‘ ëŒ€ë¹„)
  âœ… ì •í™•ì„± ë³´ì¥
```

**Graceful Shutdown í†µí•©**:

```scala
class Worker(...) extends ShutdownAware {
  private val shutdownManager = GracefulShutdownManager(
    ShutdownConfig(
      gracePeriod = 30.seconds,       // â­ 30ì´ˆ ëŒ€ê¸°
      saveCheckpoint = true,           // â­ Checkpoint ì €ì¥
      waitForCurrentPhase = true       // â­ í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸°
    )
  )

  override def gracefulShutdown(): Future[Unit] = {
    // 1. í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
    // 2. Checkpoint ì €ì¥
    val currentState = getCurrentState()
    checkpointManager.saveCheckpoint(currentPhase.get(), currentState, 0.5)

    // 3. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cleanupResources()
  }
}
```

---

#### Layer 2: Replication (ì¤‘ê°„ ë°ì´í„° ë³µêµ¬)

**ëª©ì **: Worker crash ì‹œ ì†ì‹¤ëœ ì¤‘ê°„ íŒŒì¼ì„ ë‹¤ë¥¸ Workerì—ì„œ ë³µêµ¬

**Replication ì„¤ì •**:

```scala
case class ReplicationMetadata(
  partitionId: Int,
  primaryWorker: Int,
  backupWorkers: Seq[Int],    // â­ ë³µì œë³¸ ìœ„ì¹˜
  version: Long,
  checksum: String,            // â­ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
  size: Long
)

case class PartitionData(
  id: Int,
  records: Seq[Record],
  checksum: String,
  metadata: Map[String, String] = Map.empty
)

class ReplicationManager(
  replicationFactor: Int = 2,  // â­ ê¸°ë³¸ê°’: ì›ë³¸ + ë³µì œë³¸ 1ê°œ
  workerClients: Map[Int, WorkerClient]
) {

  /**
   * íŒŒí‹°ì…˜ì„ ë°±ì—… Workerë“¤ì— ë³µì œ
   */
  def replicatePartition(
    partitionData: PartitionData,
    replicas: Seq[Int],
    primaryWorker: Int
  ): Future[Unit] = {
    val version = replicationVersion.incrementAndGet()

    val metadata = ReplicationMetadata(
      partitionId = partitionData.id,
      primaryWorker = primaryWorker,
      backupWorkers = replicas,
      version = version,
      checksum = partitionData.checksum,
      size = partitionData.records.size
    )

    // ë³‘ë ¬ë¡œ replicasì— ì „ì†¡
    val replicationFutures = replicas.map { workerId =>
      sendToReplica(partitionData, workerId).map { success =>
        if (success) {
          logger.info(s"Replicated partition ${partitionData.id} to Worker $workerId")
        }
      }
    }

    Future.sequence(replicationFutures).map(_ => ())
  }

  /**
   * Replicaì—ì„œ íŒŒí‹°ì…˜ ë³µêµ¬
   */
  def recoverFromReplica(
    partitionId: Int,
    failedWorker: Int
  ): Future[PartitionData] = {
    replicationMetadata.get(partitionId) match {
      case Some(metadata) if metadata.primaryWorker == failedWorker =>
        // ë°±ì—… Worker ì¤‘ ì²« ë²ˆì§¸ì—ì„œ ë³µêµ¬
        val backupWorker = metadata.backupWorkers.head

        workerClients(backupWorker)
          .fetchPartition(partitionId)
          .map { data =>
            // â­ Checksum ê²€ì¦
            if (data.checksum == metadata.checksum) {
              logger.info(s"Successfully recovered partition $partitionId from Worker $backupWorker")
              data
            } else {
              throw new Exception("Checksum mismatch")
            }
          }

      case _ =>
        Future.failed(new Exception(s"No replica found for partition $partitionId"))
    }
  }

  /**
   * Replica Worker ì„ íƒ (Consistent Hashing)
   */
  def selectReplicas(
    partitionId: Int,
    excludeWorker: Int
  ): Seq[Int] = {
    val availableWorkers = workerClients.keys.filterNot(_ == excludeWorker).toSeq

    if (availableWorkers.size < replicationFactor - 1) {
      logger.warn(s"Not enough workers for replication factor $replicationFactor")
      availableWorkers
    } else {
      // Deterministic selection
      val random = new Random(partitionId)
      random.shuffle(availableWorkers).take(replicationFactor - 1)
    }
  }
}
```

**Replication ì‹œì **:
```
Phase 2: Sort & Partition ì™„ë£Œ í›„
  â”œâ”€ Worker 0ì´ P0, P1, P2 ìƒì„±
  â”œâ”€ ReplicationManager.replicatePartition(P0, Seq(Worker1), Worker0)
  â”œâ”€ ReplicationManager.replicatePartition(P1, Seq(Worker2), Worker0)
  â””â”€ ReplicationManager.replicatePartition(P2, Seq(Worker3), Worker0)

Result:
  Worker 0: P0 (primary), P3 (backup), P6 (backup), P9 (backup)
  Worker 1: P0 (backup), P3 (primary), P7 (backup), P10 (backup)
  ...
```

---

#### í†µí•© ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤

**Scenario: Worker 2 crashes during Phase 3 (Shuffle)**

```
Initial State:
  Worker 0, 1, 2, 3 â†’ Phase 2 ì™„ë£Œ, ê°ê° P0~P11 ìƒì„±
  Worker 2ê°€ Shuffle ì¤‘ crash

Crash Detection:
  Masterê°€ Worker 2ì˜ heartbeat timeout ê°ì§€
  â†’ Worker 2' (ìƒˆ í”„ë¡œì„¸ìŠ¤) ì‹œì‘

Recovery Process:

  Step 1: Checkpoint ë¡œë“œ (Layer 1)
    Worker 2'.recoverFromCheckpoint()
    â†’ currentPhase = PHASE_SORTING (ë§ˆì§€ë§‰ ì™„ë£Œ Phase)
    â†’ shuffleMap, partitionBoundaries ë³µêµ¬
    â†’ completedPartitions = {6, 7, 8} ë³µêµ¬

  Step 2: ì¤‘ê°„ íŒŒì¼ ë³µêµ¬ (Layer 2)
    Worker 2'ê°€ ì†ì‹¤ëœ íŒŒí‹°ì…˜ í™•ì¸:
      P6, P7, P8 (Worker 2ê°€ Sortì—ì„œ ìƒì„±í–ˆë˜ íŒŒì¼ë“¤)

    ReplicationManager.recoverFromReplica(6, 2)
      â†’ Worker 1ì˜ backupì—ì„œ P6 ë³µì‚¬
      â†’ Checksum ê²€ì¦ âœ…

    ReplicationManager.recoverFromReplica(7, 2)
      â†’ Worker 0ì˜ backupì—ì„œ P7 ë³µì‚¬
      â†’ Checksum ê²€ì¦ âœ…

    ReplicationManager.recoverFromReplica(8, 2)
      â†’ Worker 3ì˜ backupì—ì„œ P8 ë³µì‚¬
      â†’ Checksum ê²€ì¦ âœ…

  Step 3: ë§ˆì§€ë§‰ Phaseë¶€í„° ì¬ê°œ
    Worker 2'.performShuffle()  // â­ Sorting ìŠ¤í‚µ, Shuffleë¶€í„° ì‹œì‘
    Worker 2'.performMerge()
    Worker 2'.reportCompletion()

Result:
  âœ… ë¹ ë¥¸ ë³µêµ¬ (Sampling/Sorting ìŠ¤í‚µ)
  âœ… ë°ì´í„° ì†ì‹¤ ì—†ìŒ (Replicaì—ì„œ ë³µêµ¬)
  âœ… ì •í™•ì„± ë³´ì¥ (Checksum ê²€ì¦)
  âœ… ìµœì¢… ì¶œë ¥ ë™ì¼ (deterministic)
```

---

#### 2-Layer Fault Tolerance ì¥ì 

**Checkpoint (Layer 1)**:
- âœ… ë¹ ë¥¸ ë³µêµ¬ (ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ)
- âœ… ê°€ë²¼ìš´ ì˜¤ë²„í—¤ë“œ (JSON ì§ë ¬í™”)
- âœ… Process memory ë³µêµ¬
- âœ… Graceful Shutdown í†µí•©

**Replication (Layer 2)**:
- âœ… ë””ìŠ¤í¬ ì†ì‹¤ ëŒ€ì‘ (ì¤‘ê°„ íŒŒì¼ ë³µêµ¬)
- âœ… Checksum ê²€ì¦ (ë°ì´í„° ë¬´ê²°ì„±)
- âœ… Consistent Hashing (deterministic replica ì„ íƒ)
- âœ… Configurable replication factor (1~N)

**Combined**:
- âœ… **"All intermediate data is lost" ë¬¸ì œ ì™„ì „ í•´ê²°**
- âœ… ìŠ¬ë¼ì´ë“œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±: "fault-tolerant system"
- âœ… ìƒˆ Workerê°€ ë™ì¼í•œ ì¶œë ¥ ìƒì„± (deterministic)

### 3.6 Additional Requirements í•´ê²°

#### Requirement 1: ASCII/Binary ìë™ ê°ì§€

```scala
object InputFormatDetector {
  private val SAMPLE_SIZE = 1000      // ì²« 1000 ë°”ì´íŠ¸ ë¶„ì„
  private val ASCII_THRESHOLD = 0.9   // 90% ì´ìƒ ASCII

  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](SAMPLE_SIZE)
    val inputStream = new FileInputStream(file)
    val bytesRead = inputStream.read(buffer)

    // ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
    val asciiCount = buffer.take(bytesRead).count { b =>
      (b >= 32 && b <= 126) || b == '\n' || b == '\r'
    }

    val asciiRatio = asciiCount.toDouble / bytesRead

    if (asciiRatio > ASCII_THRESHOLD) DataFormat.Ascii
    else DataFormat.Binary
  }
}

// RecordReader Factory Pattern
object RecordReader {
  def create(file: File): RecordReader = {
    val format = InputFormatDetector.detectFormat(file)  // â­ ìë™ ê°ì§€
    format match {
      case DataFormat.Binary => new BinaryRecordReader()
      case DataFormat.Ascii  => new AsciiRecordReader()
    }
  }
}
```

**í˜¼í•© ì…ë ¥ ì²˜ë¦¬**:
- ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í˜•ì‹ ê°ì§€
- ASCIIì™€ Binary íŒŒì¼ í˜¼ì¬ ê°€ëŠ¥

#### Requirement 2 & 3: ì…ë ¥ ë³´í˜¸ + ì¶œë ¥ ì •ë¦¬

```scala
class FileLayout(
  inputDirs: List[File],    // ì½ê¸° ì „ìš©
  outputDir: File,          // ìµœì¢… partition.* íŒŒì¼ë§Œ
  tempBaseDir: File         // ì„ì‹œ íŒŒì¼ (ìë™ ì‚­ì œ)
) {
  /**
   * ì…ë ¥ ë””ë ‰í† ë¦¬ ê²€ì¦ (ì½ê¸° ì „ìš©)
   */
  def validateInputDirectories(): Unit = {
    inputDirs.foreach { dir =>
      require(dir.exists() && dir.canRead, s"Cannot read: $dir")
      // â­ ìˆ˜ì • ê¸ˆì§€ (ì½ê¸°ë§Œ)
    }
  }

  /**
   * ì„ì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
   */
  def createTemporaryStructure(): Unit = {
    val workDir = new File(tempBaseDir, s"sort_work_$workerId")
    val subdirs = List("samples", "sorted_chunks", "partitions", "received")

    subdirs.foreach { subdir =>
      val dir = new File(workDir, subdir)
      dir.mkdirs()  // â­ /tmp/distsort/ í•˜ìœ„ì— ìƒì„±
    }
  }

  /**
   * ì„ì‹œ íŒŒì¼ ì •ë¦¬
   */
  def cleanupTemporaryFiles(): Unit = {
    val workDir = new File(tempBaseDir, s"sort_work_$workerId")
    if (workDir.exists()) {
      deleteRecursively(workDir)  // â­ ì„ì‹œ íŒŒì¼ ëª¨ë‘ ì‚­ì œ
    }
  }
}
```

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
Input:    /data1/input/         (ì½ê¸° ì „ìš©, ìˆ˜ì • ê¸ˆì§€)
Output:   /home/gla/data/       (ìµœì¢… partition.* íŒŒì¼ë§Œ)
Temp:     /tmp/sort_work_W0/    (ì„ì‹œ íŒŒì¼, ìë™ ì‚­ì œ)
          â”œâ”€â”€ samples/
          â”œâ”€â”€ sorted_chunks/
          â”œâ”€â”€ partitions/
          â””â”€â”€ received/
```

#### Requirement 4: í¬íŠ¸ í•˜ë“œì½”ë”© ê¸ˆì§€

```scala
class Worker(
  workerId: String,
  masterHost: String,
  masterPort: Int,
  workerPort: Int = 0  // â­ 0 = ìë™ í• ë‹¹
) {
  def start(): Unit = {
    server = ServerBuilder
      .forPort(workerPort)  // â­ 0ì´ë©´ ìë™ í• ë‹¹
      .addService(...)
      .build()

    server.start()
    actualPort = server.getPort  // â­ ì‹¤ì œ í• ë‹¹ëœ í¬íŠ¸

    logger.info(s"Worker $workerId started on port $actualPort")
  }
}
```

**ì¥ì **:
- âœ… ì—¬ëŸ¬ Workerë¥¼ ê°™ì€ ë¨¸ì‹ ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- âœ… í¬íŠ¸ ì¶©ëŒ ë°©ì§€

### 3.7 gRPC ê¸°ë°˜ í†µì‹ 

#### ì™œ gRPCë¥¼ ì„ íƒí–ˆëŠ”ê°€?

| ë¹„êµ í•­ëª© | HTTP/REST | **gRPC** (ì„ íƒ) |
|----------|-----------|----------------|
| ì„±ëŠ¥ | JSON í…ìŠ¤íŠ¸ (ëŠë¦¼) | Protocol Buffers ë°”ì´ë„ˆë¦¬ (ë¹ ë¦„) |
| Streaming | ì œí•œì  (WebSocket í•„ìš”) | âœ… ì–‘ë°©í–¥ streaming ë„¤ì´í‹°ë¸Œ ì§€ì› |
| íƒ€ì… ì•ˆì •ì„± | ì—†ìŒ (ëŸ°íƒ€ì„ ì—ëŸ¬) | âœ… ì»´íŒŒì¼ íƒ€ì„ íƒ€ì… ì²´í¬ |
| Code generation | ìˆ˜ë™ ì‘ì„± | âœ… .proto â†’ Scala stub ìë™ ìƒì„± |
| ëŒ€ìš©ëŸ‰ ì „ì†¡ | Chunking ìˆ˜ë™ êµ¬í˜„ | âœ… Streamingìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬ |

**ê²°ë¡ **: ë¶„ì‚° ì •ë ¬ì—ì„œ **ëŒ€ìš©ëŸ‰ íŒŒí‹°ì…˜ ì „ì†¡**ê³¼ **ì–‘ë°©í–¥ í†µì‹ **ì´ í•„ìˆ˜ â†’ gRPCê°€ ìµœì 

---

#### Protocol Buffers ì •ì˜

```protobuf
// distsort.proto

service MasterService {
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);
  rpc SendSample(SampleData) returns (Ack);
  rpc NotifyPhaseComplete(PhaseCompleteRequest) returns (Ack);
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

service WorkerService {
  rpc SetPartitionBoundaries(PartitionConfig) returns (Ack);
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);  // â­ Streaming
  rpc StartShuffle(ShuffleSignal) returns (Ack);
  rpc StartMerge(MergeSignal) returns (Ack);
  rpc GetStatus(StatusRequest) returns (WorkerStatus);
}

message PartitionConfig {
  repeated bytes boundaries = 1;       // N-1 or M-1 ê°œì˜ ê²½ê³„
  int32 num_partitions = 2;            // N or M
  map<int32, int32> shuffle_map = 3;   // partitionID â†’ workerID
  repeated WorkerInfo all_workers = 4; // ì „ì²´ Worker ì •ë³´
}

message ShuffleDataChunk {
  int32 partition_id = 1;
  int32 source_worker = 2;
  bytes data = 3;                      // â­ 1MB chunk
  int64 sequence_number = 4;
  bool is_last_chunk = 5;
}
```

---

#### ê° RPC ë©”ì„œë“œì˜ ì—­í• ê³¼ í˜¸ì¶œ ì‹œì 

**MasterService (Worker â†’ Master í˜¸ì¶œ)**

| RPC ë©”ì„œë“œ | í˜¸ì¶œ ì‹œì  | ì—­í•  | ì‘ë‹µ |
|-----------|----------|------|------|
| `RegisterWorker` | Phase 0: Worker ì‹œì‘ ì§í›„ | Worker ë“±ë¡, index í• ë‹¹ | `workerIndex`, `numWorkers` |
| `SendSample` | Phase 1: Sampling ì™„ë£Œ í›„ | ìƒ˜í”Œ ì „ì†¡ (100~10,000ê°œ keys) | `Ack` |
| `NotifyPhaseComplete` | ê° Phase ì™„ë£Œ ì‹œ | Phase ì™„ë£Œ ë³´ê³  | `proceedToNext`, `nextPhase` |
| `Heartbeat` | ë§¤ 10ì´ˆ (ë°±ê·¸ë¼ìš´ë“œ) | Worker ìƒì¡´ í™•ì¸ | `timestamp` |

**WorkerService (Worker â†” Worker í˜¸ì¶œ, Master â†’ Worker í˜¸ì¶œ)**

| RPC ë©”ì„œë“œ | í˜¸ì¶œì | í˜¸ì¶œ ì‹œì  | ì—­í•  |
|-----------|-------|----------|------|
| `SetPartitionBoundaries` | Master | Phase 1 ì™„ë£Œ í›„ | íŒŒí‹°ì…˜ ê²½ê³„ ë¸Œë¡œë“œìºìŠ¤íŠ¸ |
| `ShuffleData` | Worker | Phase 3: Shuffle | **â­ íŒŒí‹°ì…˜ ì „ì†¡ (streaming)** |
| `StartShuffle` | Master | Phase 3 ì‹œì‘ | Shuffle ëª…ë ¹ |
| `StartMerge` | Master | Phase 4 ì‹œì‘ | Merge ëª…ë ¹ |
| `GetStatus` | Master/User | ì–¸ì œë“ ì§€ | Worker ìƒíƒœ ì¡°íšŒ |

---

#### Streamingì˜ í•„ìš”ì„±: ShuffleData

**ë¬¸ì œ**: Worker 0ì´ Worker 1ì—ê²Œ 4GB íŒŒí‹°ì…˜ì„ ì „ì†¡í•´ì•¼ í•¨

**Non-streaming (ì˜ëª»ëœ ë°©ì‹)**:
```scala
// âŒ 4GBë¥¼ í•œ ë²ˆì— ë©”ëª¨ë¦¬ì— ë¡œë“œ â†’ OutOfMemoryError
val allData = readFile(partitionFile)  // 4GB
stub.shuffleData(ShuffleDataChunk(partitionId, allData))
```

**Streaming (ì˜¬ë°”ë¥¸ ë°©ì‹)**:
```scala
// âœ… 1MBì”© chunkë¡œ ë‚˜ëˆ ì„œ ì „ì†¡
val CHUNK_SIZE = 1 * 1024 * 1024  // 1MB

def sendPartitionStreaming(partitionFile: File, partitionId: Int): Unit = {
  val inputStream = new FileInputStream(partitionFile)
  val buffer = new Array[Byte](CHUNK_SIZE)
  var sequenceNumber = 0L
  var bytesRead = 0

  val requestObserver = stub.shuffleData(responseObserver)

  while ({ bytesRead = inputStream.read(buffer); bytesRead > 0 }) {
    val chunk = ShuffleDataChunk(
      partitionId = partitionId,
      sourceWorker = myWorkerId,
      data = ByteString.copyFrom(buffer, 0, bytesRead),
      sequenceNumber = sequenceNumber,
      isLastChunk = false
    )

    requestObserver.onNext(chunk)  // â­ ë¹„ë™ê¸° ì „ì†¡
    sequenceNumber += 1
  }

  // ë§ˆì§€ë§‰ chunk í‘œì‹œ
  requestObserver.onNext(chunk.copy(isLastChunk = true))
  requestObserver.onCompleted()
}
```

**ì¥ì **:
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 4GB â†’ 1MB (4000ë°° ì ˆì•½)
- âœ… íŒŒì´í”„ë¼ì´ë‹: ì „ì†¡ê³¼ ì½ê¸° ë™ì‹œ ì§„í–‰
- âœ… ì¡°ê¸° ì˜¤ë¥˜ ê°ì§€: ì²« chunk ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨

---

#### Shuffle ë„¤íŠ¸ì›Œí¬ í†µì‹  íë¦„ë„

**ì‹œë‚˜ë¦¬ì˜¤**: 4 Workers, 12 Partitions (ê° Workerê°€ 3ê°œì”© ë‹´ë‹¹)

```
Phase 3: Shuffle (Worker-to-Worker í†µì‹ )

Worker 0ì´ ìƒì„±í•œ íŒŒí‹°ì…˜ë“¤:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ P0  â”‚ P1  â”‚ P2  â”‚ P3  â”‚ P4  â”‚ P5  â”‚ P6  â”‚ P7  â”‚ P8  â”‚ P9  â”‚ P10 â”‚ P11 â”‚
â””â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
   â”‚     â”‚     â”‚
   â”‚     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
   â”‚                                â”‚                â”‚
   â†“ gRPC streaming                 â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker 0 â”‚                   â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚
â”‚ (keep)   â”‚                   â”‚ (recv)   â”‚    â”‚ (recv)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Worker 0 â†’ Worker 0: P0 (ìê¸° ìì‹ , ë³µì‚¬ë§Œ)
Worker 0 â†’ Worker 0: P1 (ìê¸° ìì‹ , ë³µì‚¬ë§Œ)
Worker 0 â†’ Worker 0: P2 (ìê¸° ìì‹ , ë³µì‚¬ë§Œ)

Worker 0 â†’ Worker 1: P3 (gRPC streaming, 1MB chunks)
Worker 0 â†’ Worker 1: P4 (gRPC streaming, 1MB chunks)
Worker 0 â†’ Worker 1: P5 (gRPC streaming, 1MB chunks)

Worker 0 â†’ Worker 2: P6 (gRPC streaming, 1MB chunks)
Worker 0 â†’ Worker 2: P7 (gRPC streaming, 1MB chunks)
Worker 0 â†’ Worker 2: P8 (gRPC streaming, 1MB chunks)

... (ë‹¤ë¥¸ Workerë“¤ë„ ë™ì¼)

ì´ ë„¤íŠ¸ì›Œí¬ ì „ì†¡:
  - Worker 0 â†’ Worker 1: 3ê°œ íŒŒí‹°ì…˜
  - Worker 0 â†’ Worker 2: 3ê°œ íŒŒí‹°ì…˜
  - Worker 0 â†’ Worker 3: 3ê°œ íŒŒí‹°ì…˜
  (ìê¸° ìì‹  3ê°œëŠ” ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ì—†ìŒ, ë¡œì»¬ ë³µì‚¬)

ì „ì²´: 4 workers Ã— 9 network transfers = 36 transfers
```

---

#### ì¬ì‹œë„ ë¡œì§ê³¼ ì§€ìˆ˜ ë°±ì˜¤í”„

**ë¬¸ì œ**: ë„¤íŠ¸ì›Œí¬ëŠ” ë¶ˆì•ˆì •í•¨ (ì¼ì‹œì  ì¥ì• , í˜¼ì¡, Worker ì¬ì‹œì‘ ë“±)

**í•´ê²°**: Exponential Backoff + Retry

```scala
def sendPartitionWithRetry(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo,
    maxRetries: Int = 3
): Unit = {
  var attempt = 0
  var success = false

  while (attempt < maxRetries && !success) {
    try {
      logger.info(s"Sending partition $partitionId to ${targetWorker.workerId} (attempt ${attempt + 1}/$maxRetries)")

      sendPartition(partitionFile, partitionId, targetWorker)

      success = true
      logger.info(s"Successfully sent partition $partitionId")

    } catch {
      case e: StatusRuntimeException if isRetryable(e) =>
        attempt += 1

        if (attempt < maxRetries) {
          // â­ ì§€ìˆ˜ ë°±ì˜¤í”„: 1ì´ˆ â†’ 2ì´ˆ â†’ 4ì´ˆ
          val backoffMs = math.pow(2, attempt).toLong * 1000
          logger.warn(s"Retryable error for partition $partitionId: ${e.getMessage}. " +
                      s"Retrying in ${backoffMs}ms (attempt $attempt/$maxRetries)")
          Thread.sleep(backoffMs)
        } else {
          logger.error(s"Failed to send partition $partitionId after $maxRetries attempts")
          throw e
        }

      case e: Exception =>
        // ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ (ì˜ˆ: ì˜ëª»ëœ íŒŒí‹°ì…˜ ID)
        logger.error(s"Non-retryable error for partition $partitionId: ${e.getMessage}")
        throw e
    }
  }
}

def isRetryable(e: StatusRuntimeException): Boolean = {
  e.getStatus.getCode match {
    case Status.Code.UNAVAILABLE => true   // Worker ì¼ì‹œì ìœ¼ë¡œ ë‹¤ìš´
    case Status.Code.DEADLINE_EXCEEDED => true  // íƒ€ì„ì•„ì›ƒ
    case Status.Code.RESOURCE_EXHAUSTED => true  // í˜¼ì¡
    case _ => false
  }
}
```

**ì§€ìˆ˜ ë°±ì˜¤í”„ì˜ íš¨ê³¼**:
```
Attempt 1: ì¦‰ì‹œ ì‹œë„ â†’ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ í˜¼ì¡)
  â†“ 1ì´ˆ ëŒ€ê¸°
Attempt 2: ì¬ì‹œë„ â†’ ì‹¤íŒ¨ (ì—¬ì „íˆ í˜¼ì¡)
  â†“ 2ì´ˆ ëŒ€ê¸°
Attempt 3: ì¬ì‹œë„ â†’ ì„±ê³µ (í˜¼ì¡ í•´ì†Œë¨)

ì´ ì‹œê°„: 3ì´ˆ (ì‹¤íŒ¨ë¡œ ëë‚˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë‚˜ìŒ)
```

**ì™œ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ”ê°€?**
1. **í˜¼ì¡ ì™„í™”**: ëª¨ë“  Workerê°€ ë™ì‹œì— ì¬ì‹œë„í•˜ë©´ ë” í˜¼ì¡í•´ì§
2. **ì¼ì‹œì  ì¥ì•  ëŒ€ì‘**: ì§§ì€ ì¥ì• ëŠ” 1ì´ˆ, ê¸´ ì¥ì• ëŠ” 4ì´ˆ ëŒ€ê¸°
3. **Thundering Herd ë°©ì§€**: Workerë“¤ì˜ ì¬ì‹œë„ ì‹œê°„ì„ ë¶„ì‚°

---

#### Heartbeatì˜ ì—­í• 

**ëª©ì **: Masterê°€ Workerì˜ ìƒì¡´ ì—¬ë¶€ë¥¼ ì¶”ì 

```scala
// Worker: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ 10ì´ˆë§ˆë‹¤ heartbeat ì „ì†¡
val heartbeatExecutor = Executors.newSingleThreadScheduledExecutor()

heartbeatExecutor.scheduleAtFixedRate(
  new Runnable {
    def run(): Unit = {
      try {
        val request = HeartbeatRequest(workerId, currentPhase.toString)
        masterStub.heartbeat(request)
        logger.debug(s"Heartbeat sent: $workerId at phase $currentPhase")
      } catch {
        case e: Exception =>
          logger.error(s"Failed to send heartbeat: ${e.getMessage}")
      }
    }
  },
  0,        // initial delay
  10,       // period: 10ì´ˆ
  TimeUnit.SECONDS
)
```

```scala
// Master: 30ì´ˆ ë™ì•ˆ heartbeat ì—†ìœ¼ë©´ Worker ì œê±°
val HEARTBEAT_TIMEOUT = 30 * 1000  // 30ì´ˆ

def checkWorkerHealth(): Unit = {
  val now = System.currentTimeMillis()

  registeredWorkers.foreach { case (workerId, workerInfo) =>
    val lastHeartbeat = workerInfo.lastHeartbeatTime

    if (now - lastHeartbeat > HEARTBEAT_TIMEOUT) {
      logger.warn(s"Worker $workerId timeout (no heartbeat for 30s)")

      // Worker ì œê±° + ë‹¤ë¥¸ Workerë“¤ì—ê²Œ ì•Œë¦¼
      registeredWorkers.remove(workerId)
      notifyWorkerFailure(workerId)
    }
  }
}
```

**Heartbeatë¥¼ í†µí•œ Worker ì¥ì•  ê°ì§€**:
```
Worker 2 crashes at 10:00:00

10:00:10 - Master receives last heartbeat from Worker 2
10:00:20 - No heartbeat (10s elapsed)
10:00:30 - No heartbeat (20s elapsed)
10:00:40 - â­ TIMEOUT (30s elapsed)
         â†’ Master removes Worker 2
         â†’ Master triggers recovery:
           - Checkpoint + Replication recovery
           - ìƒˆ Worker 2' ì‹œì‘
```

---

#### gRPC í†µì‹  ì¥ì  ì •ë¦¬

| ì¥ì  | ì„¤ëª… |
|------|------|
| âœ… **ê³ ì„±ëŠ¥** | Protocol Buffers (ë°”ì´ë„ˆë¦¬) + HTTP/2 multiplexing |
| âœ… **Streaming** | ëŒ€ìš©ëŸ‰ íŒŒí‹°ì…˜ ì „ì†¡ (4GB â†’ 1MB chunks) |
| âœ… **íƒ€ì… ì•ˆì „ì„±** | Compile-time íƒ€ì… ì²´í¬ (.proto â†’ Scala stub) |
| âœ… **ì¬ì‹œë„ ë¡œì§** | StatusRuntimeExceptionìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë¶„ë¥˜ |
| âœ… **ì–‘ë°©í–¥ í†µì‹ ** | Master â†” Worker, Worker â†” Worker ëª¨ë‘ ì§€ì› |
| âœ… **Backpressure** | Streamingìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ flow control |

---

## 4. ê°œë°œ ë°©ë²•ë¡ : TDD

### 4.1 TDDë€?

**Test-Driven Development**: êµ¬í˜„ ì „ì— í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ëŠ” ê°œë°œ ë°©ì‹

#### Red-Green-Refactor Cycle

```
1. ğŸ”´ RED     : ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±
                (ì›í•˜ëŠ” ë™ì‘ ì •ì˜)

2. ğŸŸ¢ GREEN   : ìµœì†Œí•œì˜ ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸ í†µê³¼
                (ì¼ë‹¨ ë™ì‘í•˜ê²Œ ë§Œë“¤ê¸°)

3. ğŸ”µ REFACTOR: ì½”ë“œ ê°œì„  (í…ŒìŠ¤íŠ¸ëŠ” ê³„ì† í†µê³¼)
                (ê¹”ë”í•˜ê²Œ ë§Œë“¤ê¸°)
```

### 4.2 ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ TDDì˜ ì¤‘ìš”ì„±

| ë¬¸ì œ | TDDë¥¼ í†µí•œ í•´ê²° |
|------|----------------|
| ë³µì¡í•œ ìƒíƒœ ì „ì´ | í…ŒìŠ¤íŠ¸ë¡œ ëª…í™•í•˜ê²Œ ê²€ì¦ |
| gRPC í†µì‹  ì˜¤ë¥˜ | ì‹œë‚˜ë¦¬ì˜¤ ì‚¬ì „ ì •ì˜ |
| Race condition | ì¡°ê¸° ë°œê²¬ ê°€ëŠ¥ |
| Fault tolerance | ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦ |
| ë¦¬íŒ©í† ë§ | ì•ˆì „ì„± ë³´ì¥ |

### 4.3 TDD ì‹¤ì „ ì˜ˆì‹œ - Record í´ë˜ìŠ¤

#### Step 1: ğŸ”´ RED - ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±

```scala
class RecordSpec extends AnyFlatSpec with Matchers {
  "Record" should "store 10-byte key and 90-byte value" in {
    val key = Array.fill[Byte](10)(1)
    val value = Array.fill[Byte](90)(2)

    val record = Record(key, value)

    record.key.length shouldBe 10
    record.value.length shouldBe 90
  }

  it should "compare records by key only (unsigned)" in {
    // â­ ì¤‘ìš”: 0xFF (255) > 0x01 (1) in unsigned comparison
    val rec1 = Record(Array[Byte](0xFF.toByte) ++ Array.fill[Byte](9)(0),
                      Array.fill[Byte](90)(0))
    val rec2 = Record(Array[Byte](0x01) ++ Array.fill[Byte](9)(0),
                      Array.fill[Byte](90)(0))

    rec1.compare(rec2) should be > 0  // â­ Unsigned ë¹„êµ
  }
}
```

**ì‹¤í–‰ ê²°ê³¼**: âŒ ì»´íŒŒì¼ ì—ëŸ¬ (Record í´ë˜ìŠ¤ ë¯¸ì¡´ì¬)

#### Step 2: ğŸŸ¢ GREEN - ìµœì†Œ êµ¬í˜„

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes")
  require(value.length == 90, s"Value must be 90 bytes")

  override def compare(that: Record): Int = {
    // âš ï¸ ì¤‘ìš”: Unsigned ë¹„êµ í•„ìˆ˜!
    var i = 0
    while (i < 10) {
      val byte1 = this.key(i) & 0xFF  // Signed â†’ Unsigned ë³€í™˜
      val byte2 = that.key(i) & 0xFF
      if (byte1 != byte2) return byte1 - byte2
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value
}
```

**ì‹¤í–‰ ê²°ê³¼**: âœ… í…ŒìŠ¤íŠ¸ í†µê³¼!

#### Step 3: ğŸ”µ REFACTOR - ì½”ë“œ ê°œì„ 

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes, got ${key.length}")
  require(value.length == 90, s"Value must be 90 bytes, got ${value.length}")

  override def compare(that: Record): Int = {
    var i = 0
    while (i < 10) {
      val unsigned1 = this.key(i) & 0xFF
      val unsigned2 = that.key(i) & 0xFF
      val diff = unsigned1 - unsigned2
      if (diff != 0) return diff
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value

  override def toString: String =
    s"Record(key=${key.take(3).map("%02X".format(_)).mkString("")}..., value=${value.length}B)"
}
```

**ì‹¤í–‰ ê²°ê³¼**: âœ… í…ŒìŠ¤íŠ¸ ì—¬ì „íˆ í†µê³¼ + ê°€ë…ì„± í–¥ìƒ

### 4.4 Testing Pyramid

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ E2E Tests   â”‚  ~ 10%
            â”‚ (ëŠë¦¼)      â”‚  (ì „ì²´ ì‹œìŠ¤í…œ)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Integration Testsâ”‚  ~ 20%
         â”‚ (ë³´í†µ)           â”‚  (ì»´í¬ë„ŒíŠ¸ ê°„)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Unit Tests            â”‚  ~ 70%
    â”‚   (ë¹ ë¦„)                â”‚  (í•¨ìˆ˜/í´ë˜ìŠ¤)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í…ŒìŠ¤íŠ¸ ì‘ì„± í˜„í™© (Week 4-5 ê¸°ì¤€)

| ì»´í¬ë„ŒíŠ¸ | í…ŒìŠ¤íŠ¸ ê°œìˆ˜ | ìƒíƒœ |
|---------|-----------|------|
| Record | 5 tests | âœ… ì™„ë£Œ (Week 4) |
| RecordReader (Binary) | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| RecordReader (ASCII) | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| InputFormatDetector | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| FileLayout | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| ExternalSorter | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| KWayMerger | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |

---

## 5. í˜„ì¬ ì§„í–‰ ìƒí™© (Week 4-5)

### 5.1 ì „ì²´ ì§„í–‰ë¥ 

```
Week 1-2: ì„¤ê³„ ë‹¨ê³„ (100% ì™„ë£Œ)
  â”œâ”€ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì •ì˜
  â”œâ”€ Protocol Buffers ì„¤ê³„
  â”œâ”€ Fault Tolerance ì „ëµ ê²°ì • (Checkpoint)
  â””â”€ 7ê°œ ì„¤ê³„ ë¬¸ì„œ ì‘ì„±

Week 3: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° Record (100% ì™„ë£Œ)
  â”œâ”€ SBT í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
  â”œâ”€ gRPC stub ìƒì„±
  â””â”€ Record í´ë˜ìŠ¤ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ (5/5 passing)

Week 4-5: Core I/O Components (100% ì™„ë£Œ)
  â”œâ”€ RecordReader ì¶”ìƒí™” (Factory Pattern)
  â”œâ”€ BinaryRecordReader (100-byte ê³ ì • ê¸¸ì´)
  â”œâ”€ AsciiRecordReader (102-byte: key+space+value+newline)
  â”œâ”€ InputFormatDetector (ìë™ í˜•ì‹ ê°ì§€)
  â”œâ”€ FileLayout (íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬)
  â””â”€ RecordWriter (Binary/ASCII ì¶œë ¥)

Week 5: Algorithms (100% ì™„ë£Œ) 
  â”œâ”€ ExternalSorter (2-Pass External Sort)
  â”œâ”€ Partitioner (Range-based partitioning)
  â””â”€ KWayMerger (Priority Queue ê¸°ë°˜ ë³‘í•©)

Week 6: Master/Worker êµ¬í˜„ (ì§„í–‰ ì¤‘)
  ğŸ“‹ Master êµ¬í˜„
  ğŸ“‹ Worker êµ¬í˜„
  ğŸ“‹ Phase ë™ê¸°í™”
  ğŸ“‹ Checkpoint í†µí•©

Week 7-8: í†µí•© í…ŒìŠ¤íŠ¸ (ì˜ˆì •)
  ğŸ“‹ ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
  ğŸ“‹ Fault Tolerance ê²€ì¦
  ğŸ“‹ ìµœì í™”
```

### 5.2 êµ¬í˜„ ì™„ë£Œ í•­ëª© ìƒì„¸

#### âœ… 1. Record í´ë˜ìŠ¤ (Week 4)

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10)
  require(value.length == 90)

  override def compare(that: Record): Int = {
    var i = 0
    while (i < 10) {
      val unsigned1 = this.key(i) & 0xFF  // â­ Unsigned ë³€í™˜
      val unsigned2 = that.key(i) & 0xFF
      val diff = unsigned1 - unsigned2
      if (diff != 0) return diff
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value
}
```

**í…ŒìŠ¤íŠ¸**: 5/5 passing

#### âœ… 2. InputFormatDetector (Week 5)

```scala
object InputFormatDetector {
  private val SAMPLE_SIZE = 1000
  private val ASCII_THRESHOLD = 0.9

  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](SAMPLE_SIZE)
    // ì²« 1000 bytes ì½ê¸°
    val bytesRead = inputStream.read(buffer)

    // ASCII ë¹„ìœ¨ ê³„ì‚°
    val asciiCount = buffer.take(bytesRead).count(isAsciiPrintable)
    val asciiRatio = asciiCount.toDouble / bytesRead

    if (asciiRatio > ASCII_THRESHOLD) DataFormat.Ascii
    else DataFormat.Binary
  }
}
```

**í•´ê²°**: Additional Requirement 1 (ASCII/Binary ìë™ ê°ì§€)

#### âœ… 3. RecordReader ì¶”ìƒí™” (Week 5)

```scala
trait RecordReader {
  def readRecord(input: InputStream): Option[Array[Byte]]
  def close(): Unit
}

object RecordReader {
  def create(file: File): RecordReader = {
    val format = InputFormatDetector.detectFormat(file)
    format match {
      case DataFormat.Binary => new BinaryRecordReader()
      case DataFormat.Ascii  => new AsciiRecordReader()
    }
  }
}
```

**BinaryRecordReader**: 100-byte ê³ ì • ê¸¸ì´
**AsciiRecordReader**: 102-byte (key + space + value + newline)

#### âœ… 4. FileLayout (Week 5)

```scala
class FileLayout(
  inputDirs: List[File],    // ì½ê¸° ì „ìš©
  outputDir: File,          // ìµœì¢… íŒŒì¼ë§Œ
  tempBaseDir: File         // ì„ì‹œ íŒŒì¼
) {
  def validateInputDirectories(): Unit
  def createTemporaryStructure(): Unit
  def ensureSufficientDiskSpace(): Unit
  def cleanupTemporaryFiles(): Unit
}
```

**í•´ê²°**:
- Additional Requirement 2 (ì…ë ¥ ë³´í˜¸)
- Additional Requirement 3 (ì¶œë ¥ ì •ë¦¬)

#### âœ… 5. ExternalSorter (Week 5)

```scala
class ExternalSorter(
  fileLayout: FileLayout,
  memoryLimit: Long = 512 * 1024 * 1024,  // 512 MB
  numThreads: Int = Runtime.getRuntime.availableProcessors()
) {
  def createSortedChunksParallel(records: Seq[Record]): Seq[File] = {
    val chunks = records.grouped(recordsPerChunk).toSeq

    // ë³‘ë ¬ ì •ë ¬
    val futures = chunks.zipWithIndex.map { case (chunk, index) =>
      Future {
        val sorted = chunk.sorted
        writeChunkToFile(sorted, chunkFile)
        chunkFile
      }
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

**í•´ê²°**: Challenge 1 (ë©”ëª¨ë¦¬ ì œí•œ)

#### âœ… 6. KWayMerger (Week 5)

```scala
class KWayMerger(sortedChunks: Seq[File]) {
  def mergeWithCallback(callback: Record => Unit): Unit = {
    val heap = mutable.PriorityQueue[HeapEntry]()

    // Min-heap ê¸°ë°˜ ë³‘í•©
    while (heap.nonEmpty) {
      val entry = heap.dequeue()
      callback(entry.record)  // ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

      readers(entry.sourceIndex).readRecord() match {
        case Some(nextRecord) =>
          heap.enqueue(HeapEntry(nextRecord, entry.sourceIndex))
        case None => // Source exhausted
      }
    }
  }
}
```

**í•´ê²°**: Challenge 1 (ë””ìŠ¤í¬ ê¸°ë°˜ ë³‘í•©)

#### âœ… 7. CheckpointManager (Week 5)

```scala
class CheckpointManager(workerId: String) {
  def saveCheckpoint(
    phase: WorkerPhase,
    state: WorkerState,
    progress: Double
  ): Future[String] = {
    val checkpoint = Checkpoint(checkpointId, workerId, phase.toString,
                                 Instant.now(), progress, state)
    // JSONìœ¼ë¡œ ì €ì¥
    writer.write(gson.toJson(checkpoint))
    checkpointId
  }

  def loadLatestCheckpoint(): Option[Checkpoint] = {
    // ê°€ì¥ ìµœê·¼ checkpoint ë¡œë“œ
    checkpointFiles.sortBy(_.lastModified()).reverse.headOption
  }
}
```

**í•´ê²°**: Challenge 3 - Layer 1 (ì§„í–‰ ìƒíƒœ ë³µêµ¬)

#### âœ… 8. ReplicationManager (Week 5)

```scala
class ReplicationManager(
  replicationFactor: Int = 2,
  workerClients: Map[Int, WorkerClient]
) {
  def replicatePartition(
    partitionData: PartitionData,
    replicas: Seq[Int],
    primaryWorker: Int
  ): Future[Unit] = {
    // ë³‘ë ¬ë¡œ replicasì— ì „ì†¡
    val replicationFutures = replicas.map { workerId =>
      sendToReplica(partitionData, workerId)
    }
    Future.sequence(replicationFutures).map(_ => ())
  }

  def recoverFromReplica(
    partitionId: Int,
    failedWorker: Int
  ): Future[PartitionData] = {
    // Replicaì—ì„œ ë°ì´í„° ë³µêµ¬ + Checksum ê²€ì¦
    workerClients(backupWorker)
      .fetchPartition(partitionId)
      .map { data =>
        if (data.checksum == metadata.checksum) data
        else throw new Exception("Checksum mismatch")
      }
  }

  def selectReplicas(partitionId: Int, excludeWorker: Int): Seq[Int] = {
    // Consistent hashing for deterministic replica selection
    val random = new Random(partitionId)
    random.shuffle(availableWorkers).take(replicationFactor - 1)
  }
}
```

**í•´ê²°**: Challenge 3 - Layer 2 (ì¤‘ê°„ ë°ì´í„° ë³µêµ¬)

#### âœ… 9. Worker with 2-Layer Fault Tolerance (Week 6)

```scala
class Worker(...) extends ShutdownAware {
  private val checkpointManager = CheckpointManager(workerId)
  private val replicationManager = ReplicationManager(replicationFactor = 2, workerClients)
  private val shutdownManager = GracefulShutdownManager(...)

  def run(): Unit = {
    // Layer 1: Checkpoint ë³µêµ¬
    val recoveredFromCheckpoint = recoverFromCheckpoint()

    if (!recoveredFromCheckpoint) {
      performSampling()
    }

    // Layer 2: ì†ì‹¤ëœ ì¤‘ê°„ íŒŒì¼ ë³µêµ¬
    if (recoveredFromCheckpoint) {
      recoverLostPartitions()  // Replicaì—ì„œ ë³µêµ¬
    }

    // Phaseë³„ checkpoint ì €ì¥
    if (currentPhase == PHASE_SORTING) {
      performLocalSort()
      savePhaseCheckpoint(PHASE_SORTING, 1.0)

      // Replication ìˆ˜í–‰
      replicateMyPartitions()

      performShuffle()
      savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)
    }

    performMerge()
    checkpointManager.deleteAllCheckpoints()
  }

  private def recoverLostPartitions(): Unit = {
    myPartitions.foreach { partitionId =>
      if (!partitionFileExists(partitionId)) {
        replicationManager.recoverFromReplica(partitionId, workerId)
      }
    }
  }

  private def replicateMyPartitions(): Unit = {
    myPartitions.foreach { partitionId =>
      val replicas = replicationManager.selectReplicas(partitionId, workerId)
      val partitionData = loadPartitionData(partitionId)
      replicationManager.replicatePartition(partitionData, replicas, workerId)
    }
  }
}
```

**í•´ê²°**: Challenge 3 (2-Layer ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í†µí•©)

### 5.3 ì½”ë“œ í’ˆì§ˆ ì§€í‘œ

**Week 4-5 ê¸°ì¤€**:
- ì´ ì½”ë“œ ë¼ì¸: ~2,000 LOC (Comments í¬í•¨ ~3,000 LOC)
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: Record 5/5 passing
- ë¬¸ì„œí™”: ëª¨ë“  public method Scaladoc ì‘ì„±
- ì½”ë“œ ìŠ¤íƒ€ì¼: Scala í‘œì¤€ ê°€ì´ë“œ ì¤€ìˆ˜

### 5.4 í•´ê²°í•œ ê¸°ìˆ ì  ë¬¸ì œ

#### ë¬¸ì œ 1: Unsigned Byte ë¹„êµ

**ì¦ìƒ**: 0xFFê°€ 0x01ë³´ë‹¤ ì‘ê²Œ ì •ë ¬ë¨ (signed ë¹„êµ)

**í•´ê²°**:
```scala
// âŒ ì˜ëª»ëœ ì½”ë“œ
this.key(0).compareTo(that.key(0))  // Signed: 0xFF = -1 < 0x01 = 1

// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
val unsigned1 = this.key(0) & 0xFF  // Unsigned: 0xFF = 255 > 0x01 = 1
```

#### ë¬¸ì œ 2: Bufferì˜ clone() ëˆ„ë½

**ì¦ìƒ**: ê°™ì€ ë ˆì½”ë“œê°€ ë°˜ë³µí•´ì„œ ì½í˜ (ì°¸ì¡° ê³µìœ )

**í•´ê²°**:
```scala
// âŒ ì˜ëª»ëœ ì½”ë“œ
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  records += record  // ê°™ì€ ë°°ì—´ ì°¸ì¡°!
}

// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  records += record.clone()  // ë³µì‚¬ë³¸ ì €ì¥
}
```

#### ë¬¸ì œ 3: ASCII Threshold ê²°ì •

**ì‹¤í—˜**:
- Threshold 0.5: Binary íŒŒì¼ì„ ASCIIë¡œ ì˜¤íŒ
- Threshold 0.95: ASCII íŒŒì¼ì„ Binaryë¡œ ì˜¤íŒ
- **Threshold 0.9**: ìµœì  (gensort ë°ì´í„°ë¡œ ê²€ì¦)

---

## 6. í–¥í›„ ê³„íš (Week 6-8)

### 6.1 Week 6: Master/Worker í†µí•© (ì§„í–‰ ì¤‘)

#### Master Node

```
ì£¼ìš” ê¸°ëŠ¥:
  â”œâ”€ Worker ë“±ë¡ ê´€ë¦¬ (CountDownLatch)
  â”œâ”€ ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°
  â”œâ”€ shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸
  â”œâ”€ Phase ë™ê¸°í™” (PhaseTracker)
  â””â”€ ìµœì¢… ê²°ê³¼ ì¶œë ¥ (stdout)

êµ¬í˜„ ìˆœì„œ:
  1. ğŸ”´ RED: MasterSpec ì‘ì„±
  2. ğŸŸ¢ GREEN: ìµœì†Œ êµ¬í˜„
  3. ğŸ”µ REFACTOR: ë¦¬íŒ©í† ë§
```

#### Worker Node

```
ì£¼ìš” ê¸°ëŠ¥:
  â”œâ”€ Masterì— ë“±ë¡
  â”œâ”€ ìƒ˜í”Œ ì¶”ì¶œ ë° ì „ì†¡
  â”œâ”€ ì •ë ¬ ë° íŒŒí‹°ì…”ë‹
  â”œâ”€ Shuffle (ì†¡ì‹ /ìˆ˜ì‹ )
  â”œâ”€ Merge
  â””â”€ ì™„ë£Œ ë³´ê³ 

ìƒíƒœ ë¨¸ì‹ :
  INITIALIZING â†’ SAMPLING â†’ SORTING â†’ SHUFFLING â†’ MERGING â†’ COMPLETED
```

### 6.2 Week 7: í†µí•© í…ŒìŠ¤íŠ¸

```bash
# Scenario 1: 3 workers, 9 partitions (Strategy B)
$ ./test_3w9p.sh

# Scenario 2: Worker crash during shuffle
$ ./test_fault_tolerance.sh

# Scenario 3: ASCII/Binary í˜¼í•© ì…ë ¥
$ ./test_mixed_format.sh
```

### 6.3 Week 8: ìµœì í™” ë° ìµœì¢… ê²€ì¦

- ë©€í‹°ìŠ¤ë ˆë“œ í™œìš©ë„ í–¥ìƒ
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ìµœì í™”
- ë””ìŠ¤í¬ I/O ë³‘ëª© ì œê±°
- valsort ê²€ì¦ í†µê³¼ í™•ì¸

---

## 7. Q&A ì¤€ë¹„

### ì˜ˆìƒ ì§ˆë¬¸

#### Q1: "TDDë¥¼ ì„ íƒí•œ ì´ìœ ëŠ”?"

**ë‹µë³€**:
- ë¶„ì‚° ì‹œìŠ¤í…œì€ ìƒíƒœ ì „ì´ì™€ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ë³µì¡
- í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ë©´ ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ ì •ì˜í•  ìˆ˜ ìˆìŒ
- ë¦¬íŒ©í† ë§ ì‹œ ì•ˆì „ì„± ë³´ì¥
- ì˜ˆ: Record í´ë˜ìŠ¤ì˜ unsigned ë¹„êµ ë²„ê·¸ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ë¨¼ì € ë°œê²¬

#### Q2: "2-Layer Fault ToleranceëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜?"

**ë‹µë³€**:
ìš°ë¦¬ëŠ” **Checkpoint + Replication** 2ë‹¨ê³„ ë³µêµ¬ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

**Layer 1: Checkpoint (ì§„í–‰ ìƒíƒœ ë³µêµ¬)**
- **ì €ì¥**: ê° Phase ì™„ë£Œ ì‹œ WorkerStateë¥¼ JSONìœ¼ë¡œ ìë™ ì €ì¥ (`/tmp/distsort/checkpoints/`)
- **ë³µêµ¬**: Worker ì¬ì‹œì‘ ì‹œ ìµœì‹  checkpoint ë¡œë“œ â†’ ë§ˆì§€ë§‰ ì™„ë£Œ Phase í™•ì¸
- **ë‚´ìš©**: processedRecords, shuffleMap, completedPartitions, phaseMetadata

**Layer 2: Replication (ì¤‘ê°„ ë°ì´í„° ë³µêµ¬)**
- **ë³µì œ**: Phase 2 (Sort & Partition) ì™„ë£Œ í›„ ê° íŒŒí‹°ì…˜ì„ ë‹¤ë¥¸ Workerì— ë³µì œ (replicationFactor=2)
- **ë³µêµ¬**: Worker crash ì‹œ replicaì—ì„œ ì†ì‹¤ëœ íŒŒí‹°ì…˜ ë°ì´í„° ë³µì‚¬
- **ê²€ì¦**: Checksumìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸

**í†µí•© ë³µêµ¬ ì˜ˆì‹œ**:
1. Worker 2ê°€ Shuffle ì¤‘ crash
2. Worker 2' ì‹œì‘ â†’ Checkpoint ë¡œë“œ (ë§ˆì§€ë§‰ Phase: PHASE_SORTING)
3. ReplicationManagerê°€ Worker 1, 3ì—ì„œ P6, P7, P8 ë³µêµ¬
4. Shuffleë¶€í„° ì¬ê°œ (Sampling/Sort ìŠ¤í‚µ)

**ì¥ì **:
- âœ… **"All intermediate data is lost" ë¬¸ì œ ì™„ì „ í•´ê²°** (ìŠ¬ë¼ì´ë“œ ìš”êµ¬ì‚¬í•­)
- âœ… ë¹ ë¥¸ ë³µêµ¬ (ì „ì²´ ì¬ì‹œì‘ ëŒ€ë¹„ ì‹œê°„ ì ˆì•½)
- âœ… ì •í™•ì„± ë³´ì¥ (Checksum + deterministic recovery)

#### Q3: "Nâ†’M ì „ëµì˜ ì¥ì ì€?"

**ë‹µë³€**:
- ë¡œë“œ ë°¸ëŸ°ì‹± ê°œì„ : íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€ë¡œ í¬ê¸° ë¶ˆê· í˜• ì™„í™”
- ë©€í‹°ì½”ì–´ í™œìš©: ê° Workerê°€ ì—¬ëŸ¬ íŒŒí‹°ì…˜ ë³‘ë ¬ merge
- ì˜ˆ: 4 workers, 12 partitions â†’ ê° Workerê°€ 3ê°œ íŒŒí‹°ì…˜ ë³‘ë ¬ merge

#### Q4: "ASCII/Binary ìë™ ê°ì§€ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜?"

**ë‹µë³€**:
- íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ì½ê¸°
- ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
- ë¹„ìœ¨ > 90% â†’ ASCII, ê·¸ ì™¸ â†’ Binary
- ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì  ê°ì§€ â†’ í˜¼í•© ì…ë ¥ ì§€ì›

#### Q5: "í˜„ì¬ ì§„í–‰ë¥ ì€?"

**ë‹µë³€**:
- Week 1-2: ì„¤ê³„ ë‹¨ê³„ 100% ì™„ë£Œ
- Week 3: Record í´ë˜ìŠ¤ 100% ì™„ë£Œ (5/5 tests passing)
- Week 4-5: Core I/O Components 100% ì™„ë£Œ
  - RecordReader, InputFormatDetector, FileLayout, RecordWriter
  - ExternalSorter, KWayMerger
  - CheckpointManager, ReplicationManager (2-Layer Fault Tolerance)
- Week 6: Master/Worker í†µí•© ì§„í–‰ ì¤‘

**ì „ì²´ ì§„í–‰ë¥ **: ì•½ 60% (ì„¤ê³„ + í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì™„ë£Œ, Master/Worker í†µí•© ì§„í–‰ ì¤‘)

#### Q6: "External Sortê°€ ë©”ëª¨ë¦¬ ì œí•œì„ ì–´ë–»ê²Œ ì¤€ìˆ˜í•˜ë‚˜?"

**ë‹µë³€**:
- `memoryLimit = 512MB` ì„¤ì •
- `recordsPerChunk = memoryLimit / 100` ê³„ì‚°
- ì˜ˆ: 512MB Ã· 100 bytes = 5,242,880 records per chunk
- 50GB ì…ë ¥ â†’ ì•½ 100 chunks ìƒì„±
- ê° chunkë¥¼ ë³‘ë ¬ë¡œ ì •ë ¬ â†’ ë””ìŠ¤í¬ì— ì €ì¥
- K-way mergeë¡œ ìµœì¢… ë³‘í•© (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)

#### Q7: "Challenge 1, 2, 3ê°€ ëª¨ë‘ í•´ê²°ë˜ì—ˆë‚˜?"

**ë‹µë³€**:
- **Challenge 1 (Input > Memory)**: âœ… ExternalSorter + KWayMergerë¡œ í•´ê²°
  - 2-Pass External Sort (512MB chunks)
  - Min-heap ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ë³‘í•©
- **Challenge 2 (Input > Disk, Distributed)**: âœ… Master-Worker + Nâ†’M Partitionìœ¼ë¡œ í•´ê²°
  - Sort â†’ Partition â†’ Shuffle â†’ Merge
  - gRPC ê¸°ë°˜ worker-to-worker í†µì‹ 
- **Challenge 3 ("All intermediate data is lost")**: âœ… **Checkpoint + Replication**ìœ¼ë¡œ ì™„ì „ í•´ê²°
  - Checkpoint: ì§„í–‰ ìƒíƒœ ë³µêµ¬
  - Replication: ì¤‘ê°„ íŒŒì¼ ë³µêµ¬ (replicationFactor=2)
  - ìŠ¬ë¼ì´ë“œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±: "fault-tolerant system"
- **Additional Requirements**: âœ… ëª¨ë‘ êµ¬í˜„ ì™„ë£Œ
  - ASCII/Binary ìë™ ê°ì§€ (90% threshold)
  - ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸ (read-only)
  - ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬ (cleanup)
  - ë™ì  í¬íŠ¸ í• ë‹¹

### ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ (Week 7 ì´í›„)

```bash
# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
$ gensort -b0 10000000 /data1/input/test.dat  # 1GB

# 2. Master ì‹œì‘ (4 workers, 12 partitions)
$ sbt "runMain distsort.Main master 4 12"
[INFO] Master started on port 30000
[INFO] Waiting for 4 workers to register...

# 3. Worker ì‹œì‘ (4ëŒ€)
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1/input -O /data1/output"
[INFO] Worker registered with ID W0

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data2/input -O /data2/output"
[INFO] Worker registered with ID W1

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data3/input -O /data3/output"
[INFO] Worker registered with ID W2

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data4/input -O /data4/output"
[INFO] Worker registered with ID W3

# 4. Master ì¶œë ¥
192.168.1.100:30000
worker0, worker1, worker2, worker3

# 5. ê²°ê³¼ ê²€ì¦
$ valsort /data1/output/partition.* /data2/output/partition.* \
          /data3/output/partition.* /data4/output/partition.*
SUCCESS
```

---

## 8. ì°¸ê³  ë¬¸í—Œ

### 8.1 í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

- Knuth, Donald E. *The Art of Computer Programming, Volume 3: Sorting and Searching*. Addison-Wesley, 1998.
  - External Sorting ì•Œê³ ë¦¬ì¦˜ (5.4ì ˆ)

- TeraSort: A Sample Hadoop Application
  - Sampling for Partitioning ê¸°ë²•

### 8.2 ì‹œìŠ¤í…œ ì„¤ê³„

- Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: Simplified Data Processing on Large Clusters." *OSDI* 2004.
  - Master-Worker ì•„í‚¤í…ì²˜, Fault Tolerance ì „ëµ

- Zaharia, Matei, et al. "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing." *NSDI* 2012.
  - Checkpoint ê¸°ë°˜ Fault Recovery

### 8.3 êµ¬í˜„ ì°¸ê³ 

- gRPC ê³µì‹ ë¬¸ì„œ: https://grpc.io/docs/languages/scala/
- Protocol Buffers: https://protobuf.dev/
- **gensort/valsort**: http://www.ordinal.com/gensort.html
  - ì •ë ¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ë° ê²€ì¦ ë„êµ¬

### 8.4 í”„ë¡œì íŠ¸ ë¬¸ì„œ

- `plan/2025-10-24_plan_ver3.md`: ì „ì²´ ì‹œìŠ¤í…œ ì„¤ê³„
- `docs/0-implementation-decisions.md`: í•µì‹¬ êµ¬í˜„ ê²°ì • ì‚¬í•­
- `docs/1-phase-coordination.md`: Phase ë™ê¸°í™” í”„ë¡œí† ì½œ
- `docs/4-error-recovery.md`: ì¥ì•  ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- `docs/6-parallelization.md`: ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬
- `docs/7-testing-strategy.md`: TDD ê°€ì´ë“œ

---

**ë°œí‘œ ì¤€ë¹„ ì™„ë£Œ**
**Team Silver** - 2025-11-16

---

## Appendix: í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project_2025/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”‚   â””â”€â”€ build.properties
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ scala/distsort/
â”‚       â”‚   â”œâ”€â”€ Main.scala
â”‚       â”‚   â”œâ”€â”€ core/
â”‚       â”‚   â”‚   â”œâ”€â”€ Record.scala               âœ… Week 4
â”‚       â”‚   â”‚   â”œâ”€â”€ RecordReader.scala         âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ BinaryRecordReader.scala   âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ AsciiRecordReader.scala    âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ InputFormatDetector.scala  âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ FileLayout.scala           âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ RecordWriter.scala         âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ ExternalSorter.scala       âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ Partitioner.scala          âœ… Week 5
â”‚       â”‚   â”‚   â””â”€â”€ KWayMerger.scala           âœ… Week 5
â”‚       â”‚   â”œâ”€â”€ checkpoint/
â”‚       â”‚   â”‚   â””â”€â”€ CheckpointManager.scala    âœ… Week 5
â”‚       â”‚   â”œâ”€â”€ shutdown/
â”‚       â”‚   â”‚   â””â”€â”€ GracefulShutdownManager.scala âœ… Week 6
â”‚       â”‚   â”œâ”€â”€ master/
â”‚       â”‚   â”‚   â”œâ”€â”€ MasterServer.scala         ğŸ“‹ Week 6
â”‚       â”‚   â”‚   â””â”€â”€ PhaseTracker.scala         ğŸ“‹ Week 6
â”‚       â”‚   â””â”€â”€ worker/
â”‚       â”‚       â”œâ”€â”€ Worker.scala               ğŸ“‹ Week 6
â”‚       â”‚       â””â”€â”€ WorkerService.scala        ğŸ“‹ Week 6
â”‚       â””â”€â”€ protobuf/
â”‚           â””â”€â”€ distsort.proto                 ğŸ“‹ Week 6
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 0-implementation-decisions.md
â”‚   â”œâ”€â”€ 1-phase-coordination.md
â”‚   â”œâ”€â”€ 4-error-recovery.md
â”‚   â”œâ”€â”€ 6-parallelization.md
â”‚   â””â”€â”€ 7-testing-strategy.md
â””â”€â”€ plan/
    â””â”€â”€ 2025-10-24_plan_ver3.md
```
