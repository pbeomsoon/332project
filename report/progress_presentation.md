# ë¶„ì‚° ì •ë ¬ ì‹œìŠ¤í…œ ì¤‘ê°„ ë°œí‘œ
**Team Silver** - ê¶Œë™ì—°, ë°•ë²”ìˆœ, ì„ì§€í›ˆ

**ë°œí‘œì¼**: 2025-11-16
**í”„ë¡œì íŠ¸**: Fault-Tolerant Distributed Sorting System

---

## ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [Challenges (ë„ì „ ê³¼ì œ)](#2-challenges-ë„ì „-ê³¼ì œ)
3. [í•´ê²° ë°©ì•ˆ](#3-í•´ê²°-ë°©ì•ˆ)
4. [ê°œë°œ ë°©ë²•ë¡ : TDD](#4-ê°œë°œ-ë°©ë²•ë¡ -tdd)
5. [í˜„ì¬ ì§„í–‰ ìƒí™© (Week 4-5)](#5-í˜„ì¬-ì§„í–‰-ìƒí™©-week-4-5)
6. [í–¥í›„ ê³„íš (Week 6-8)](#6-í–¥í›„-ê³„íš-week-6-8)
7. [Q&A ì¤€ë¹„](#7-qa-ì¤€ë¹„)
8. [ì°¸ê³  ë¬¸í—Œ](#8-ì°¸ê³ -ë¬¸í—Œ)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ

ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚° ì €ì¥ëœ ëŒ€ìš©ëŸ‰ key/value ë ˆì½”ë“œë¥¼ **ì •ë ¬**í•˜ëŠ” **ì¥ì•  í—ˆìš©ì„±** ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬í˜„

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­

| í•­ëª© | ìš”êµ¬ì‚¬í•­ |
|------|---------|
| **ì…ë ¥** | ì—¬ëŸ¬ Worker ë…¸ë“œì— ë¶„ì‚°ëœ ë¯¸ì •ë ¬ ë°ì´í„° |
| **ì¶œë ¥** | ì „ì—­ì ìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„° (partition.0, partition.1, ...) |
| **ë ˆì½”ë“œ** | 100-byte (10-byte key + 90-byte value) |
| **ì •ë ¬** | Keyë§Œ ì‚¬ìš© (unsigned byte ë¹„êµ) |
| **ê²€ì¦** | gensort/valsort ë„êµ¬ í™œìš© |

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

```
ì–¸ì–´:        Scala 2.13
ë¹Œë“œ ë„êµ¬:   SBT 1.9.7
RPC:         gRPC + Protocol Buffers
í…ŒìŠ¤íŠ¸:      ScalaTest (TDD)
ë²„ì „ ê´€ë¦¬:   Git
```

### 1.4 ë ˆì½”ë“œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Key (10B)    â”‚ Value (90B)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  0            10                                      100

íŠ¹ì§•:
- ê³ ì • ê¸¸ì´: 100 ë°”ì´íŠ¸
- Keyë§Œ ì •ë ¬ ê¸°ì¤€ (unsigned ë¹„êµ)
- ValueëŠ” Keyì™€ í•¨ê»˜ ì´ë™
```

---

## 2. Challenges (ë„ì „ ê³¼ì œ)

### Challenge 1: ì…ë ¥ì´ ë©”ëª¨ë¦¬ë³´ë‹¤ í¼

**ë¬¸ì œ**:
- ì…ë ¥ ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ë“¤ì–´ê°€ì§€ ì•ŠìŒ
- ì˜ˆ: ì…ë ¥ 50GB, ë©”ëª¨ë¦¬ 8GB

**í•´ê²°ì±…**: **Disk-based Merge Sort** (External Sort)

```
50GB ì…ë ¥
  â†“
Read â†’ 100MB chunks â†’ Sort/Write (ë³‘ë ¬)
  â†“
Merge (merging 500 files) using K-way merge
  â†“
50GB ì •ë ¬ëœ ì¶œë ¥
```

### Challenge 2: ì…ë ¥ì´ ë””ìŠ¤í¬ë³´ë‹¤ í¼ (ë¶„ì‚° í™˜ê²½)

**ë¬¸ì œ**:
- ì…ë ¥ ë°ì´í„°ê°€ ë‹¨ì¼ ë””ìŠ¤í¬ì— ë“¤ì–´ê°€ì§€ ì•ŠìŒ
- ì˜ˆ: ì…ë ¥ 10TB, ë””ìŠ¤í¬ 1TB
- ì…ë ¥/ì¶œë ¥ì´ **ì—¬ëŸ¬ ë¨¸ì‹ **ì— ë¶„ì‚° ì €ì¥

**í•´ê²°ì±…**: **Distributed Sorting** (Master-Worker)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 0   â”‚       â”‚  Worker 1   â”‚       â”‚  Worker 2   â”‚
â”‚  Input: 50GBâ”‚       â”‚  Input: 50GBâ”‚       â”‚  Input: 50GBâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                     â†“                     â†“
    Sort/Partition     Sort/Partition     Sort/Partition
       â†“                     â†“                     â†“
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Shuffle (Network) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                     Merge on each worker
                            â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â†“                â†“                â†“
       50GB #1          50GB #2          50GB #3
    (partition.0-2)  (partition.3-5)  (partition.6-8)
```

### Challenge 3: Workers may crash

**ë¬¸ì œ**:
- Workerê°€ ì‹¤í–‰ ì¤‘ crash (killed by OS)
- ëª¨ë“  ì¤‘ê°„ ë°ì´í„° ì†ì‹¤
- ê°™ì€ ë…¸ë“œì—ì„œ ìƒˆ Worker ì‹œì‘ (ê°™ì€ íŒŒë¼ë¯¸í„°)

**ìš”êµ¬ì‚¬í•­**: **Fault-tolerant**
- ìƒˆ Workerê°€ ê¸°ì¡´ Workerì™€ ë™ì¼í•œ ì¶œë ¥ ìƒì„±
- ì „ì²´ ì‹œìŠ¤í…œì´ ì •í™•í•œ ê²°ê³¼ ìƒì„±

**í•´ê²°ì±…**: **Checkpoint-based Recovery**

```
Worker crash during Shuffle:
  Last checkpoint: PHASE_SORTING (100% ì™„ë£Œ)

Worker restart:
  1. Load checkpoint
  2. Restore state (partitionBoundaries, shuffleMap, ...)
  3. Resume from PHASE_SORTING
     â­ Sampling/Sort ìŠ¤í‚µ (ë¹ ë¥¸ ë³µêµ¬)
  4. Continue: Shuffle â†’ Merge â†’ Complete
```

### Additional Requirements

#### Requirement 1: ASCII/Binary ìë™ ê°ì§€
- ASCIIì™€ Binary ì…ë ¥ì„ **ì˜µì…˜ ì—†ì´** ì²˜ë¦¬
- íŒŒì¼ í˜•ì‹ì„ ìë™ìœ¼ë¡œ ê°ì§€

#### Requirement 2: ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸
- ì…ë ¥ ë””ë ‰í† ë¦¬ëŠ” **ì½ê¸° ì „ìš©**
- ì…ë ¥ íŒŒì¼ ì‚­ì œ ê¸ˆì§€
- ì…ë ¥ ë””ë ‰í† ë¦¬ì— ìƒˆ íŒŒì¼ ìƒì„± ê¸ˆì§€

#### Requirement 3: ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬
- ì¶œë ¥ ë””ë ‰í† ë¦¬ëŠ” **ìµœì¢… íŒŒì¼ë§Œ** í¬í•¨
- ì„ì‹œ íŒŒì¼/ë””ë ‰í† ë¦¬ ìƒì„± ê°€ëŠ¥, ë‹¨ **ì‘ì—… ì™„ë£Œ í›„ ì‚­ì œ**

#### Requirement 4: í¬íŠ¸ í•˜ë“œì½”ë”© ê¸ˆì§€
- íŠ¹ì • í¬íŠ¸ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ë§ ê²ƒ
- ë‹¤ì–‘í•œ ì…ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ì—¬ëŸ¬ Worker ì‹¤í–‰ ê°€ëŠ¥

---

## 3. í•´ê²° ë°©ì•ˆ

### 3.1 ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Master Node                        â”‚
â”‚  - Worker ë“±ë¡ ê´€ë¦¬                                  â”‚
â”‚  - ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°                       â”‚
â”‚  - shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸                    â”‚
â”‚  - Phase ë™ê¸°í™” ì¡°ìœ¨                                 â”‚
â”‚  - ìµœì¢… Worker ìˆœì„œ ì¶œë ¥                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ gRPC
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚          â”‚          â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚ Worker 0 â”‚   â”‚Worker 1 â”‚ â”‚Worker 2 â”‚ â”‚Worker 3 â”‚
   â”‚          â”‚   â”‚         â”‚ â”‚         â”‚ â”‚         â”‚
   â”‚Input:    â”‚   â”‚Input:   â”‚ â”‚Input:   â”‚ â”‚Input:   â”‚
   â”‚50GB      â”‚   â”‚50GB     â”‚ â”‚50GB     â”‚ â”‚50GB     â”‚
   â”‚          â”‚   â”‚         â”‚ â”‚         â”‚ â”‚         â”‚
   â”‚Output:   â”‚   â”‚Output:  â”‚ â”‚Output:  â”‚ â”‚Output:  â”‚
   â”‚P0,P1,P2  â”‚   â”‚P3,P4,P5 â”‚ â”‚P6,P7,P8 â”‚ â”‚P9,P10,P11â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚          â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       Worker-to-Worker Shuffle (gRPC Streaming)
```

### 3.2 5-Phase ì‹¤í–‰ íë¦„

```
Phase 0: Initialization
  â”œâ”€ Master ì‹œì‘, Workerë“¤ ë“±ë¡ ëŒ€ê¸°
  â”œâ”€ ê° Worker ì‹œì‘, Masterì— ì—°ê²°
  â”œâ”€ Masterê°€ Workerì— index í• ë‹¹
  â””â”€ ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦

Phase 1: Sampling
  â”œâ”€ ê° Workerê°€ ì…ë ¥ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
  â”œâ”€ ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚° (0.01% ~ 1%)
  â”œâ”€ Masterì—ê²Œ ìƒ˜í”Œ ì „ì†¡
  â”œâ”€ Masterê°€ ì „ì²´ ìƒ˜í”Œ ì •ë ¬
  â”œâ”€ íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚° (Nê°œ or Mê°œ)
  â””â”€ shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸

Phase 2: Sort & Partition
  â”œâ”€ ê° Workerê°€ ì…ë ¥ íŒŒì¼ ì½ê¸° (ASCII/Binary ìë™ ì²˜ë¦¬)
  â”œâ”€ External Sort: Chunk ë‹¨ìœ„ ì •ë ¬ (ë³‘ë ¬)
  â”‚   â””â”€ ë©”ëª¨ë¦¬ ì œí•œ ì¤€ìˆ˜ (512MB chunks)
  â”œâ”€ íŒŒí‹°ì…˜ ê²½ê³„ì— ë”°ë¼ ë¶„í•  (Nê°œ ë˜ëŠ” Mê°œ)
  â””â”€ íŒŒí‹°ì…˜ë³„ ì„ì‹œ íŒŒì¼ ìƒì„±

Phase 3: Shuffle
  â”œâ”€ shuffleMapì— ë”°ë¼ íŒŒí‹°ì…˜ ì „ì†¡
  â”œâ”€ Worker ê°„ ë„¤íŠ¸ì›Œí¬ í†µì‹  (gRPC streaming)
  â”œâ”€ ì¬ì‹œë„ ë¡œì§ (ì§€ìˆ˜ ë°±ì˜¤í”„)
  â””â”€ ìˆ˜ì‹  í™•ì¸ ë° ì„ì‹œ ì €ì¥

Phase 4: Merge
  â”œâ”€ ê° Workerê°€ ë°›ì€ íŒŒí‹°ì…˜ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ K-way merge
  â”œâ”€ Priority Queue (min-heap) ì‚¬ìš©
  â”œâ”€ ìµœì¢… partition.n íŒŒì¼ ìƒì„±
  â””â”€ Atomic write ë³´ì¥ (temp + rename)

Phase 5: Completion
  â”œâ”€ ê° Workerê°€ Masterì—ê²Œ ì™„ë£Œ ë³´ê³ 
  â”œâ”€ Masterê°€ ì „ì²´ ì‘ì—… ì™„ë£Œ í™•ì¸
  â”œâ”€ Masterê°€ ì •ë ¬ëœ Worker ì£¼ì†Œ ì¶œë ¥ (stdout)
  â””â”€ ì„ì‹œ íŒŒì¼ ì •ë¦¬
```

### 3.3 Challenge 1 í•´ê²°: External Sort (ë©”ëª¨ë¦¬ ì œí•œ)

**ë¬¸ì œ**: ì…ë ¥ì´ ë©”ëª¨ë¦¬ë³´ë‹¤ í¼ (50GB vs 8GB)

**í•´ê²°ì±…**: 2-Pass External Sort

#### Phase 1: Chunk Sort (ë³‘ë ¬)

```scala
class ExternalSorter(
  memoryLimit: Long = 512 * 1024 * 1024  // 512MB ì œí•œ
) {
  private val recordsPerChunk = (memoryLimit / 100).toInt

  def createSortedChunksParallel(records: Seq[Record]): Seq[File] = {
    val chunks = records.grouped(recordsPerChunk).toSeq  // â­ Chunkë¡œ ë¶„í• 

    // ë³‘ë ¬ ì •ë ¬ (ë©€í‹°ì½”ì–´ í™œìš©)
    val futures = chunks.zipWithIndex.map { case (chunk, index) =>
      Future {
        val sorted = chunk.sorted              // â­ ë©”ëª¨ë¦¬ ë‚´ ì •ë ¬
        writeChunkToFile(sorted, chunkFile)    // â­ ë””ìŠ¤í¬ì— ì €ì¥
        chunkFile
      }
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

#### Phase 2: K-way Merge (Priority Queue)

```scala
class KWayMerger(sortedChunks: Seq[File]) {

  def mergeWithCallback(callback: Record => Unit): Unit = {
    val readers = sortedChunks.map(RecordReader.create)
    val heap = mutable.PriorityQueue[HeapEntry]()  // â­ Min-heap

    // ê° chunkì—ì„œ ì²« ë ˆì½”ë“œ ì½ê¸°
    readers.zipWithIndex.foreach { case (reader, index) =>
      reader.readRecord() match {
        case Some(record) => heap.enqueue(HeapEntry(record, index))
        case None => // Empty chunk
      }
    }

    // ì •ë ¬ëœ ìˆœì„œë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    while (heap.nonEmpty) {
      val entry = heap.dequeue()
      callback(entry.record)  // â­ í•˜ë‚˜ì”© ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)

      // ê°™ì€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìŒ ë ˆì½”ë“œ ì½ê¸°
      readers(entry.sourceIndex).readRecord() match {
        case Some(nextRecord) =>
          heap.enqueue(HeapEntry(nextRecord, entry.sourceIndex))
        case None => // Source exhausted
      }
    }
  }
}
```

**ì¥ì **:
- âœ… ì„ì˜ í¬ê¸° ì…ë ¥ ì²˜ë¦¬ ê°€ëŠ¥ (1KB ~ 10TB+)
- âœ… ë³‘ë ¬ ì •ë ¬ (ë©€í‹°ì½”ì–´ í™œìš©)
- âœ… ìŠ¤íŠ¸ë¦¬ë° ë³‘í•© (ë©”ëª¨ë¦¬ íš¨ìœ¨)

### 3.4 Challenge 2 í•´ê²°: Distributed System (ë¶„ì‚°)

**ë¬¸ì œ**: ì…ë ¥ì´ ë””ìŠ¤í¬ë³´ë‹¤ í¼, ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚°

**í•´ê²°ì±…**: Master-Worker ì•„í‚¤í…ì²˜ + Nâ†’M Partition Strategy

#### Nâ†’M Partition Strategy

**ê°œë…**: íŒŒí‹°ì…˜ ìˆ˜ > Worker ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ M = 3N)

**ì˜ˆì‹œ: 4 Workers, 12 Partitions**

```
Phase 2: Sort & Partition
  Worker 0, 1, 2, 3 ê°ê° â†’ P0~P11 (12ê°œ) ìƒì„±

Phase 3: Shuffle
  P0, P1, P2   â†’ Worker 0
  P3, P4, P5   â†’ Worker 1
  P6, P7, P8   â†’ Worker 2
  P9, P10, P11 â†’ Worker 3

Phase 4: Merge
  Worker 0:
    - 4ê°œ P0 ì¡°ê° K-way merge â†’ partition.0
    - 4ê°œ P1 ì¡°ê° K-way merge â†’ partition.1
    - 4ê°œ P2 ì¡°ê° K-way merge â†’ partition.2

ìµœì¢… ì¶œë ¥:
  /worker0/output/partition.0  â† ê°€ì¥ ì‘ì€ key
  /worker0/output/partition.1
  /worker0/output/partition.2
  /worker1/output/partition.3
  ...
  /worker3/output/partition.11 â† ê°€ì¥ í° key

ì½ê¸° ìˆœì„œ: partition.0 â†’ 1 â†’ 2 â†’ ... â†’ 11 = ì „ì—­ ì •ë ¬ë¨
```

**shuffleMap ìƒì„±**:
```scala
def createShuffleMap(numWorkers: Int, numPartitions: Int): Map[Int, Int] = {
  val partitionsPerWorker = numPartitions / numWorkers

  (0 until numPartitions).map { partitionID =>
    val workerID = partitionID / partitionsPerWorker
    val finalWorkerID = if (workerID >= numWorkers) numWorkers - 1 else workerID
    partitionID -> finalWorkerID
  }.toMap
}

// ì˜ˆì‹œ: createShuffleMap(4, 12)
//   â†’ {0â†’0, 1â†’0, 2â†’0, 3â†’1, 4â†’1, 5â†’1, 6â†’2, 7â†’2, 8â†’2, 9â†’3, 10â†’3, 11â†’3}
```

**ì¥ì **:
- âœ… ë¡œë“œ ë°¸ëŸ°ì‹± ê°œì„  (íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€)
- âœ… ë©€í‹°ì½”ì–´ í™œìš© ì¦ê°€ (ë³‘ë ¬ merge)
- âœ… íŒŒí‹°ì…˜ í¬ê¸° ë¶ˆê· í˜• ì™„í™”

### 3.5 Challenge 3 í•´ê²°: Checkpoint-based Recovery

**ë¬¸ì œ**: Workerê°€ ì‹¤í–‰ ì¤‘ crash â†’ ëª¨ë“  ì¤‘ê°„ ë°ì´í„° ì†ì‹¤

**í•´ê²°ì±…**: Phaseë³„ Checkpoint + Graceful Shutdown

#### Checkpoint ì €ì¥

```scala
case class WorkerState(
  processedRecords: Long,                  // ì²˜ë¦¬í•œ ë ˆì½”ë“œ ìˆ˜
  partitionBoundaries: List[Array[Byte]], // íŒŒí‹°ì…˜ ê²½ê³„
  shuffleMap: Map[Int, Int],               // íŒŒí‹°ì…˜ â†’ Worker ë§¤í•‘
  completedPartitions: Set[Int],           // ì™„ë£Œí•œ íŒŒí‹°ì…˜ë“¤
  currentFiles: List[String],              // í˜„ì¬ íŒŒì¼ë“¤
  phaseMetadata: Map[String, String]       // Phase ë©”íƒ€ë°ì´í„°
)

class CheckpointManager(workerId: String) {
  def saveCheckpoint(
    phase: WorkerPhase,
    state: WorkerState,
    progress: Double
  ): Future[String] = Future {
    val checkpointId = s"checkpoint_${System.currentTimeMillis()}_${phase}"
    val checkpoint = Checkpoint(checkpointId, workerId, phase.toString,
                                 Instant.now(), progress, state)

    // JSONìœ¼ë¡œ ì €ì¥: /tmp/distsort/checkpoints/{workerId}/{checkpointId}.json
    val file = checkpointPath.resolve(s"$checkpointId.json").toFile
    val writer = new PrintWriter(file)
    writer.write(gson.toJson(checkpoint))
    writer.close()

    checkpointId
  }
}
```

**ì €ì¥ ì‹œì **:
```scala
performSampling()
  â†’ savePhaseCheckpoint(PHASE_SAMPLING, 1.0)       // âœ…

getPartitionConfiguration()
  â†’ savePhaseCheckpoint(PHASE_WAITING_FOR_PARTITIONS, 1.0)  // âœ…

performLocalSort()
  â†’ savePhaseCheckpoint(PHASE_SORTING, 1.0)        // âœ…

performShuffle()
  â†’ savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)      // âœ…

performMerge()
  â†’ savePhaseCheckpoint(PHASE_MERGING, 1.0)        // âœ…
  â†’ checkpointManager.deleteAllCheckpoints()       // ì„±ê³µ ì‹œ ì‚­ì œ
```

#### ë³µêµ¬ ë¡œì§

```scala
class Worker(...) {
  def run(): Unit = {
    // 1. Checkpointì—ì„œ ë³µêµ¬ ì‹œë„
    val recoveredFromCheckpoint = recoverFromCheckpoint()

    if (!recoveredFromCheckpoint || currentPhase.get() == PHASE_INITIALIZING) {
      performSampling()
    }

    // ë³µêµ¬ëœ Phaseì— ë”°ë¼ ì ì ˆí•œ ë‹¨ê³„ë¶€í„° ì¬ê°œ
    if (currentPhase.get() == PHASE_SAMPLING || ...) {
      getPartitionConfiguration()
      savePhaseCheckpoint(PHASE_WAITING_FOR_PARTITIONS, 1.0)
    }

    if (currentPhase.get() == PHASE_SORTING || ...) {
      performLocalSort()
      savePhaseCheckpoint(PHASE_SORTING, 1.0)

      performShuffle()
      savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)
    }

    performMerge()
    savePhaseCheckpoint(PHASE_MERGING, 1.0)

    checkpointManager.deleteAllCheckpoints()
  }

  private def recoverFromCheckpoint(): Boolean = {
    checkpointManager.loadLatestCheckpoint() match {
      case Some(checkpoint) =>
        // ìƒíƒœ ë³µì›
        partitionBoundaries = checkpoint.state.partitionBoundaries.toArray
        shuffleMap = checkpoint.state.shuffleMap
        completedPartitions = checkpoint.state.completedPartitions

        // Phase ë³µì›
        currentPhase.set(WorkerPhase.fromName(checkpoint.phase))
        true  // ë³µêµ¬ ì„±ê³µ

      case None =>
        false  // ì²˜ìŒë¶€í„° ì‹œì‘
    }
  }
}
```

**ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤**:
```
Worker crash during Shuffle:
  Last checkpoint: PHASE_SORTING (100% ì™„ë£Œ)

Worker restart:
  1. recoverFromCheckpoint() ì„±ê³µ
  2. currentPhase = PHASE_SORTING (ë³µì›)
  3. performShuffle() ì¬ì‹œì‘
     â­ Sampling/SortëŠ” ìŠ¤í‚µ (ì‹œê°„ ëŒ€í­ ì ˆì•½)
  4. Merge â†’ ì™„ë£Œ

Result:
  âœ… ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ
  âœ… ë¹ ë¥¸ ë³µêµ¬ (ì „ì²´ ì¬ì‹œì‘ ëŒ€ë¹„)
  âœ… ì •í™•ì„± ë³´ì¥
```

#### Graceful Shutdown

```scala
class Worker(...) extends ShutdownAware {
  private val shutdownManager = GracefulShutdownManager(
    ShutdownConfig(
      gracePeriod = 30.seconds,       // â­ 30ì´ˆ ëŒ€ê¸°
      saveCheckpoint = true,           // â­ Checkpoint ì €ì¥
      waitForCurrentPhase = true       // â­ í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸°
    )
  )

  override def gracefulShutdown(): Future[Unit] = {
    // 1. í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
    // 2. Checkpoint ì €ì¥
    val currentState = getCurrentState()
    checkpointManager.saveCheckpoint(currentPhase.get(), currentState, 0.5)

    // 3. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cleanupResources()
  }
}
```

**ì¥ì **:
- âœ… ë¹ ë¥¸ ë³µêµ¬ (ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ)
- âœ… ì •í™•ì„± ë³´ì¥ (Phaseë³„ ì™„ë£Œ checkpoint)
- âœ… Graceful Shutdown (ì•ˆì „í•œ ì¢…ë£Œ)
- âœ… ê°„ë‹¨í•œ êµ¬í˜„ (JSON ì§ë ¬í™” + ìµœê·¼ 3ê°œ ìœ ì§€)

### 3.6 Additional Requirements í•´ê²°

#### Requirement 1: ASCII/Binary ìë™ ê°ì§€

```scala
object InputFormatDetector {
  private val SAMPLE_SIZE = 1000      // ì²« 1000 ë°”ì´íŠ¸ ë¶„ì„
  private val ASCII_THRESHOLD = 0.9   // 90% ì´ìƒ ASCII

  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](SAMPLE_SIZE)
    val inputStream = new FileInputStream(file)
    val bytesRead = inputStream.read(buffer)

    // ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
    val asciiCount = buffer.take(bytesRead).count { b =>
      (b >= 32 && b <= 126) || b == '\n' || b == '\r'
    }

    val asciiRatio = asciiCount.toDouble / bytesRead

    if (asciiRatio > ASCII_THRESHOLD) DataFormat.Ascii
    else DataFormat.Binary
  }
}

// RecordReader Factory Pattern
object RecordReader {
  def create(file: File): RecordReader = {
    val format = InputFormatDetector.detectFormat(file)  // â­ ìë™ ê°ì§€
    format match {
      case DataFormat.Binary => new BinaryRecordReader()
      case DataFormat.Ascii  => new AsciiRecordReader()
    }
  }
}
```

**í˜¼í•© ì…ë ¥ ì²˜ë¦¬**:
- ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í˜•ì‹ ê°ì§€
- ASCIIì™€ Binary íŒŒì¼ í˜¼ì¬ ê°€ëŠ¥

#### Requirement 2 & 3: ì…ë ¥ ë³´í˜¸ + ì¶œë ¥ ì •ë¦¬

```scala
class FileLayout(
  inputDirs: List[File],    // ì½ê¸° ì „ìš©
  outputDir: File,          // ìµœì¢… partition.* íŒŒì¼ë§Œ
  tempBaseDir: File         // ì„ì‹œ íŒŒì¼ (ìë™ ì‚­ì œ)
) {
  /**
   * ì…ë ¥ ë””ë ‰í† ë¦¬ ê²€ì¦ (ì½ê¸° ì „ìš©)
   */
  def validateInputDirectories(): Unit = {
    inputDirs.foreach { dir =>
      require(dir.exists() && dir.canRead, s"Cannot read: $dir")
      // â­ ìˆ˜ì • ê¸ˆì§€ (ì½ê¸°ë§Œ)
    }
  }

  /**
   * ì„ì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
   */
  def createTemporaryStructure(): Unit = {
    val workDir = new File(tempBaseDir, s"sort_work_$workerId")
    val subdirs = List("samples", "sorted_chunks", "partitions", "received")

    subdirs.foreach { subdir =>
      val dir = new File(workDir, subdir)
      dir.mkdirs()  // â­ /tmp/distsort/ í•˜ìœ„ì— ìƒì„±
    }
  }

  /**
   * ì„ì‹œ íŒŒì¼ ì •ë¦¬
   */
  def cleanupTemporaryFiles(): Unit = {
    val workDir = new File(tempBaseDir, s"sort_work_$workerId")
    if (workDir.exists()) {
      deleteRecursively(workDir)  // â­ ì„ì‹œ íŒŒì¼ ëª¨ë‘ ì‚­ì œ
    }
  }
}
```

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
Input:    /data1/input/         (ì½ê¸° ì „ìš©, ìˆ˜ì • ê¸ˆì§€)
Output:   /home/gla/data/       (ìµœì¢… partition.* íŒŒì¼ë§Œ)
Temp:     /tmp/sort_work_W0/    (ì„ì‹œ íŒŒì¼, ìë™ ì‚­ì œ)
          â”œâ”€â”€ samples/
          â”œâ”€â”€ sorted_chunks/
          â”œâ”€â”€ partitions/
          â””â”€â”€ received/
```

#### Requirement 4: í¬íŠ¸ í•˜ë“œì½”ë”© ê¸ˆì§€

```scala
class Worker(
  workerId: String,
  masterHost: String,
  masterPort: Int,
  workerPort: Int = 0  // â­ 0 = ìë™ í• ë‹¹
) {
  def start(): Unit = {
    server = ServerBuilder
      .forPort(workerPort)  // â­ 0ì´ë©´ ìë™ í• ë‹¹
      .addService(...)
      .build()

    server.start()
    actualPort = server.getPort  // â­ ì‹¤ì œ í• ë‹¹ëœ í¬íŠ¸

    logger.info(s"Worker $workerId started on port $actualPort")
  }
}
```

**ì¥ì **:
- âœ… ì—¬ëŸ¬ Workerë¥¼ ê°™ì€ ë¨¸ì‹ ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- âœ… í¬íŠ¸ ì¶©ëŒ ë°©ì§€

### 3.7 gRPC ê¸°ë°˜ í†µì‹ 

```protobuf
service MasterService {
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);
  rpc SendSample(SampleData) returns (Ack);
  rpc NotifyPhaseComplete(PhaseCompleteRequest) returns (Ack);
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

service WorkerService {
  rpc SetPartitionBoundaries(PartitionConfig) returns (Ack);
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);
  rpc StartShuffle(ShuffleSignal) returns (Ack);
  rpc StartMerge(MergeSignal) returns (Ack);
  rpc GetStatus(StatusRequest) returns (WorkerStatus);
}

message PartitionConfig {
  repeated bytes boundaries = 1;       // N-1 or M-1 ê°œì˜ ê²½ê³„
  int32 num_partitions = 2;            // N or M
  map<int32, int32> shuffle_map = 3;   // partitionID â†’ workerID
  repeated WorkerInfo all_workers = 4;
}
```

**Shuffle ì¬ì‹œë„ ë¡œì§**:
```scala
def sendPartitionWithRetry(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo,
    maxRetries: Int = 3
): Unit = {
  var attempt = 0
  var success = false

  while (attempt < maxRetries && !success) {
    try {
      sendPartition(partitionFile, partitionId, targetWorker)
      success = true
    } catch {
      case e: StatusRuntimeException if isRetryable(e) =>
        attempt += 1
        val backoffMs = math.pow(2, attempt).toLong * 1000  // ì§€ìˆ˜ ë°±ì˜¤í”„
        Thread.sleep(backoffMs)
      case e: Exception =>
        throw e
    }
  }
}
```

---

## 4. ê°œë°œ ë°©ë²•ë¡ : TDD

### 4.1 TDDë€?

**Test-Driven Development**: êµ¬í˜„ ì „ì— í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ëŠ” ê°œë°œ ë°©ì‹

#### Red-Green-Refactor Cycle

```
1. ğŸ”´ RED     : ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±
                (ì›í•˜ëŠ” ë™ì‘ ì •ì˜)

2. ğŸŸ¢ GREEN   : ìµœì†Œí•œì˜ ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸ í†µê³¼
                (ì¼ë‹¨ ë™ì‘í•˜ê²Œ ë§Œë“¤ê¸°)

3. ğŸ”µ REFACTOR: ì½”ë“œ ê°œì„  (í…ŒìŠ¤íŠ¸ëŠ” ê³„ì† í†µê³¼)
                (ê¹”ë”í•˜ê²Œ ë§Œë“¤ê¸°)
```

### 4.2 ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ TDDì˜ ì¤‘ìš”ì„±

| ë¬¸ì œ | TDDë¥¼ í†µí•œ í•´ê²° |
|------|----------------|
| ë³µì¡í•œ ìƒíƒœ ì „ì´ | í…ŒìŠ¤íŠ¸ë¡œ ëª…í™•í•˜ê²Œ ê²€ì¦ |
| gRPC í†µì‹  ì˜¤ë¥˜ | ì‹œë‚˜ë¦¬ì˜¤ ì‚¬ì „ ì •ì˜ |
| Race condition | ì¡°ê¸° ë°œê²¬ ê°€ëŠ¥ |
| Fault tolerance | ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦ |
| ë¦¬íŒ©í† ë§ | ì•ˆì „ì„± ë³´ì¥ |

### 4.3 TDD ì‹¤ì „ ì˜ˆì‹œ - Record í´ë˜ìŠ¤

#### Step 1: ğŸ”´ RED - ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±

```scala
class RecordSpec extends AnyFlatSpec with Matchers {
  "Record" should "store 10-byte key and 90-byte value" in {
    val key = Array.fill[Byte](10)(1)
    val value = Array.fill[Byte](90)(2)

    val record = Record(key, value)

    record.key.length shouldBe 10
    record.value.length shouldBe 90
  }

  it should "compare records by key only (unsigned)" in {
    // â­ ì¤‘ìš”: 0xFF (255) > 0x01 (1) in unsigned comparison
    val rec1 = Record(Array[Byte](0xFF.toByte) ++ Array.fill[Byte](9)(0),
                      Array.fill[Byte](90)(0))
    val rec2 = Record(Array[Byte](0x01) ++ Array.fill[Byte](9)(0),
                      Array.fill[Byte](90)(0))

    rec1.compare(rec2) should be > 0  // â­ Unsigned ë¹„êµ
  }
}
```

**ì‹¤í–‰ ê²°ê³¼**: âŒ ì»´íŒŒì¼ ì—ëŸ¬ (Record í´ë˜ìŠ¤ ë¯¸ì¡´ì¬)

#### Step 2: ğŸŸ¢ GREEN - ìµœì†Œ êµ¬í˜„

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes")
  require(value.length == 90, s"Value must be 90 bytes")

  override def compare(that: Record): Int = {
    // âš ï¸ ì¤‘ìš”: Unsigned ë¹„êµ í•„ìˆ˜!
    var i = 0
    while (i < 10) {
      val byte1 = this.key(i) & 0xFF  // Signed â†’ Unsigned ë³€í™˜
      val byte2 = that.key(i) & 0xFF
      if (byte1 != byte2) return byte1 - byte2
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value
}
```

**ì‹¤í–‰ ê²°ê³¼**: âœ… í…ŒìŠ¤íŠ¸ í†µê³¼!

#### Step 3: ğŸ”µ REFACTOR - ì½”ë“œ ê°œì„ 

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes, got ${key.length}")
  require(value.length == 90, s"Value must be 90 bytes, got ${value.length}")

  override def compare(that: Record): Int = {
    var i = 0
    while (i < 10) {
      val unsigned1 = this.key(i) & 0xFF
      val unsigned2 = that.key(i) & 0xFF
      val diff = unsigned1 - unsigned2
      if (diff != 0) return diff
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value

  override def toString: String =
    s"Record(key=${key.take(3).map("%02X".format(_)).mkString("")}..., value=${value.length}B)"
}
```

**ì‹¤í–‰ ê²°ê³¼**: âœ… í…ŒìŠ¤íŠ¸ ì—¬ì „íˆ í†µê³¼ + ê°€ë…ì„± í–¥ìƒ

### 4.4 Testing Pyramid

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ E2E Tests   â”‚  ~ 10%
            â”‚ (ëŠë¦¼)      â”‚  (ì „ì²´ ì‹œìŠ¤í…œ)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Integration Testsâ”‚  ~ 20%
         â”‚ (ë³´í†µ)           â”‚  (ì»´í¬ë„ŒíŠ¸ ê°„)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Unit Tests            â”‚  ~ 70%
    â”‚   (ë¹ ë¦„)                â”‚  (í•¨ìˆ˜/í´ë˜ìŠ¤)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í…ŒìŠ¤íŠ¸ ì‘ì„± í˜„í™© (Week 4-5 ê¸°ì¤€)

| ì»´í¬ë„ŒíŠ¸ | í…ŒìŠ¤íŠ¸ ê°œìˆ˜ | ìƒíƒœ |
|---------|-----------|------|
| Record | 5 tests | âœ… ì™„ë£Œ (Week 4) |
| RecordReader (Binary) | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| RecordReader (ASCII) | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| InputFormatDetector | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| FileLayout | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| ExternalSorter | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |
| KWayMerger | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • (Week 6) |

---

## 5. í˜„ì¬ ì§„í–‰ ìƒí™© (Week 4-5)

### 5.1 ì „ì²´ ì§„í–‰ë¥ 

```
Week 1-2: ì„¤ê³„ ë‹¨ê³„ (100% ì™„ë£Œ)
  â”œâ”€ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì •ì˜
  â”œâ”€ Protocol Buffers ì„¤ê³„
  â”œâ”€ Fault Tolerance ì „ëµ ê²°ì • (Checkpoint)
  â””â”€ 7ê°œ ì„¤ê³„ ë¬¸ì„œ ì‘ì„±

Week 3: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° Record (100% ì™„ë£Œ)
  â”œâ”€ SBT í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
  â”œâ”€ gRPC stub ìƒì„±
  â””â”€ Record í´ë˜ìŠ¤ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ (5/5 passing)

Week 4-5: Core I/O Components (100% ì™„ë£Œ)
  â”œâ”€ RecordReader ì¶”ìƒí™” (Factory Pattern)
  â”œâ”€ BinaryRecordReader (100-byte ê³ ì • ê¸¸ì´)
  â”œâ”€ AsciiRecordReader (102-byte: key+space+value+newline)
  â”œâ”€ InputFormatDetector (ìë™ í˜•ì‹ ê°ì§€)
  â”œâ”€ FileLayout (íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬)
  â””â”€ RecordWriter (Binary/ASCII ì¶œë ¥)

Week 5: Algorithms (100% ì™„ë£Œ) â­ ì‹¤ì œë¡œëŠ” ì™„ë£Œë¨
  â”œâ”€ ExternalSorter (2-Pass External Sort)
  â”œâ”€ Partitioner (Range-based partitioning)
  â””â”€ KWayMerger (Priority Queue ê¸°ë°˜ ë³‘í•©)

Week 6: Master/Worker êµ¬í˜„ (ì§„í–‰ ì¤‘)
  ğŸ“‹ Master êµ¬í˜„
  ğŸ“‹ Worker êµ¬í˜„
  ğŸ“‹ Phase ë™ê¸°í™”
  ğŸ“‹ Checkpoint í†µí•©

Week 7-8: í†µí•© í…ŒìŠ¤íŠ¸ (ì˜ˆì •)
  ğŸ“‹ ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
  ğŸ“‹ Fault Tolerance ê²€ì¦
  ğŸ“‹ ìµœì í™”
```

### 5.2 êµ¬í˜„ ì™„ë£Œ í•­ëª© ìƒì„¸

#### âœ… 1. Record í´ë˜ìŠ¤ (Week 4)

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10)
  require(value.length == 90)

  override def compare(that: Record): Int = {
    var i = 0
    while (i < 10) {
      val unsigned1 = this.key(i) & 0xFF  // â­ Unsigned ë³€í™˜
      val unsigned2 = that.key(i) & 0xFF
      val diff = unsigned1 - unsigned2
      if (diff != 0) return diff
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value
}
```

**í…ŒìŠ¤íŠ¸**: 5/5 passing

#### âœ… 2. InputFormatDetector (Week 5)

```scala
object InputFormatDetector {
  private val SAMPLE_SIZE = 1000
  private val ASCII_THRESHOLD = 0.9

  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](SAMPLE_SIZE)
    // ì²« 1000 bytes ì½ê¸°
    val bytesRead = inputStream.read(buffer)

    // ASCII ë¹„ìœ¨ ê³„ì‚°
    val asciiCount = buffer.take(bytesRead).count(isAsciiPrintable)
    val asciiRatio = asciiCount.toDouble / bytesRead

    if (asciiRatio > ASCII_THRESHOLD) DataFormat.Ascii
    else DataFormat.Binary
  }
}
```

**í•´ê²°**: Additional Requirement 1 (ASCII/Binary ìë™ ê°ì§€)

#### âœ… 3. RecordReader ì¶”ìƒí™” (Week 5)

```scala
trait RecordReader {
  def readRecord(input: InputStream): Option[Array[Byte]]
  def close(): Unit
}

object RecordReader {
  def create(file: File): RecordReader = {
    val format = InputFormatDetector.detectFormat(file)
    format match {
      case DataFormat.Binary => new BinaryRecordReader()
      case DataFormat.Ascii  => new AsciiRecordReader()
    }
  }
}
```

**BinaryRecordReader**: 100-byte ê³ ì • ê¸¸ì´
**AsciiRecordReader**: 102-byte (key + space + value + newline)

#### âœ… 4. FileLayout (Week 5)

```scala
class FileLayout(
  inputDirs: List[File],    // ì½ê¸° ì „ìš©
  outputDir: File,          // ìµœì¢… íŒŒì¼ë§Œ
  tempBaseDir: File         // ì„ì‹œ íŒŒì¼
) {
  def validateInputDirectories(): Unit
  def createTemporaryStructure(): Unit
  def ensureSufficientDiskSpace(): Unit
  def cleanupTemporaryFiles(): Unit
}
```

**í•´ê²°**:
- Additional Requirement 2 (ì…ë ¥ ë³´í˜¸)
- Additional Requirement 3 (ì¶œë ¥ ì •ë¦¬)

#### âœ… 5. ExternalSorter (Week 5)

```scala
class ExternalSorter(
  fileLayout: FileLayout,
  memoryLimit: Long = 512 * 1024 * 1024,  // 512 MB
  numThreads: Int = Runtime.getRuntime.availableProcessors()
) {
  def createSortedChunksParallel(records: Seq[Record]): Seq[File] = {
    val chunks = records.grouped(recordsPerChunk).toSeq

    // ë³‘ë ¬ ì •ë ¬
    val futures = chunks.zipWithIndex.map { case (chunk, index) =>
      Future {
        val sorted = chunk.sorted
        writeChunkToFile(sorted, chunkFile)
        chunkFile
      }
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

**í•´ê²°**: Challenge 1 (ë©”ëª¨ë¦¬ ì œí•œ)

#### âœ… 6. KWayMerger (Week 5)

```scala
class KWayMerger(sortedChunks: Seq[File]) {
  def mergeWithCallback(callback: Record => Unit): Unit = {
    val heap = mutable.PriorityQueue[HeapEntry]()

    // Min-heap ê¸°ë°˜ ë³‘í•©
    while (heap.nonEmpty) {
      val entry = heap.dequeue()
      callback(entry.record)  // ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

      readers(entry.sourceIndex).readRecord() match {
        case Some(nextRecord) =>
          heap.enqueue(HeapEntry(nextRecord, entry.sourceIndex))
        case None => // Source exhausted
      }
    }
  }
}
```

**í•´ê²°**: Challenge 1 (ë””ìŠ¤í¬ ê¸°ë°˜ ë³‘í•©)

#### âœ… 7. CheckpointManager (Week 5)

```scala
class CheckpointManager(workerId: String) {
  def saveCheckpoint(
    phase: WorkerPhase,
    state: WorkerState,
    progress: Double
  ): Future[String] = {
    val checkpoint = Checkpoint(checkpointId, workerId, phase.toString,
                                 Instant.now(), progress, state)
    // JSONìœ¼ë¡œ ì €ì¥
    writer.write(gson.toJson(checkpoint))
    checkpointId
  }

  def loadLatestCheckpoint(): Option[Checkpoint] = {
    // ê°€ì¥ ìµœê·¼ checkpoint ë¡œë“œ
    checkpointFiles.sortBy(_.lastModified()).reverse.headOption
  }
}
```

**í•´ê²°**: Challenge 3 (Fault Tolerance)

#### âœ… 8. Worker with Checkpoint Integration (Week 6)

```scala
class Worker(...) extends ShutdownAware {
  private val checkpointManager = CheckpointManager(workerId)
  private val shutdownManager = GracefulShutdownManager(...)

  def run(): Unit = {
    val recoveredFromCheckpoint = recoverFromCheckpoint()

    if (!recoveredFromCheckpoint) {
      performSampling()
    }

    // Phaseë³„ checkpoint ì €ì¥
    if (currentPhase == PHASE_SORTING) {
      performLocalSort()
      savePhaseCheckpoint(PHASE_SORTING, 1.0)

      performShuffle()
      savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)
    }

    performMerge()
    checkpointManager.deleteAllCheckpoints()
  }
}
```

**í•´ê²°**: Challenge 3 (ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜)

### 5.3 ì½”ë“œ í’ˆì§ˆ ì§€í‘œ

**Week 4-5 ê¸°ì¤€**:
- ì´ ì½”ë“œ ë¼ì¸: ~2,000 LOC (Comments í¬í•¨ ~3,000 LOC)
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: Record 5/5 passing
- ë¬¸ì„œí™”: ëª¨ë“  public method Scaladoc ì‘ì„±
- ì½”ë“œ ìŠ¤íƒ€ì¼: Scala í‘œì¤€ ê°€ì´ë“œ ì¤€ìˆ˜

### 5.4 í•´ê²°í•œ ê¸°ìˆ ì  ë¬¸ì œ

#### ë¬¸ì œ 1: Unsigned Byte ë¹„êµ

**ì¦ìƒ**: 0xFFê°€ 0x01ë³´ë‹¤ ì‘ê²Œ ì •ë ¬ë¨ (signed ë¹„êµ)

**í•´ê²°**:
```scala
// âŒ ì˜ëª»ëœ ì½”ë“œ
this.key(0).compareTo(that.key(0))  // Signed: 0xFF = -1 < 0x01 = 1

// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
val unsigned1 = this.key(0) & 0xFF  // Unsigned: 0xFF = 255 > 0x01 = 1
```

#### ë¬¸ì œ 2: Bufferì˜ clone() ëˆ„ë½

**ì¦ìƒ**: ê°™ì€ ë ˆì½”ë“œê°€ ë°˜ë³µí•´ì„œ ì½í˜ (ì°¸ì¡° ê³µìœ )

**í•´ê²°**:
```scala
// âŒ ì˜ëª»ëœ ì½”ë“œ
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  records += record  // ê°™ì€ ë°°ì—´ ì°¸ì¡°!
}

// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  records += record.clone()  // ë³µì‚¬ë³¸ ì €ì¥
}
```

#### ë¬¸ì œ 3: ASCII Threshold ê²°ì •

**ì‹¤í—˜**:
- Threshold 0.5: Binary íŒŒì¼ì„ ASCIIë¡œ ì˜¤íŒ
- Threshold 0.95: ASCII íŒŒì¼ì„ Binaryë¡œ ì˜¤íŒ
- **Threshold 0.9**: ìµœì  (gensort ë°ì´í„°ë¡œ ê²€ì¦)

---

## 6. í–¥í›„ ê³„íš (Week 6-8)

### 6.1 Week 6: Master/Worker í†µí•© (ì§„í–‰ ì¤‘)

#### Master Node

```
ì£¼ìš” ê¸°ëŠ¥:
  â”œâ”€ Worker ë“±ë¡ ê´€ë¦¬ (CountDownLatch)
  â”œâ”€ ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°
  â”œâ”€ shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸
  â”œâ”€ Phase ë™ê¸°í™” (PhaseTracker)
  â””â”€ ìµœì¢… ê²°ê³¼ ì¶œë ¥ (stdout)

êµ¬í˜„ ìˆœì„œ:
  1. ğŸ”´ RED: MasterSpec ì‘ì„±
  2. ğŸŸ¢ GREEN: ìµœì†Œ êµ¬í˜„
  3. ğŸ”µ REFACTOR: ë¦¬íŒ©í† ë§
```

#### Worker Node

```
ì£¼ìš” ê¸°ëŠ¥:
  â”œâ”€ Masterì— ë“±ë¡
  â”œâ”€ ìƒ˜í”Œ ì¶”ì¶œ ë° ì „ì†¡
  â”œâ”€ ì •ë ¬ ë° íŒŒí‹°ì…”ë‹
  â”œâ”€ Shuffle (ì†¡ì‹ /ìˆ˜ì‹ )
  â”œâ”€ Merge
  â””â”€ ì™„ë£Œ ë³´ê³ 

ìƒíƒœ ë¨¸ì‹ :
  INITIALIZING â†’ SAMPLING â†’ SORTING â†’ SHUFFLING â†’ MERGING â†’ COMPLETED
```

### 6.2 Week 7: í†µí•© í…ŒìŠ¤íŠ¸

```bash
# Scenario 1: 3 workers, 9 partitions (Strategy B)
$ ./test_3w9p.sh

# Scenario 2: Worker crash during shuffle
$ ./test_fault_tolerance.sh

# Scenario 3: ASCII/Binary í˜¼í•© ì…ë ¥
$ ./test_mixed_format.sh
```

### 6.3 Week 8: ìµœì í™” ë° ìµœì¢… ê²€ì¦

- ë©€í‹°ìŠ¤ë ˆë“œ í™œìš©ë„ í–¥ìƒ
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ìµœì í™”
- ë””ìŠ¤í¬ I/O ë³‘ëª© ì œê±°
- valsort ê²€ì¦ í†µê³¼ í™•ì¸

---

## 7. Q&A ì¤€ë¹„

### ì˜ˆìƒ ì§ˆë¬¸

#### Q1: "TDDë¥¼ ì„ íƒí•œ ì´ìœ ëŠ”?"

**ë‹µë³€**:
- ë¶„ì‚° ì‹œìŠ¤í…œì€ ìƒíƒœ ì „ì´ì™€ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ë³µì¡
- í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ë©´ ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ ì •ì˜í•  ìˆ˜ ìˆìŒ
- ë¦¬íŒ©í† ë§ ì‹œ ì•ˆì „ì„± ë³´ì¥
- ì˜ˆ: Record í´ë˜ìŠ¤ì˜ unsigned ë¹„êµ ë²„ê·¸ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ë¨¼ì € ë°œê²¬

#### Q2: "Checkpoint ê¸°ë°˜ ë³µêµ¬ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜?"

**ë‹µë³€**:
- **ì €ì¥**: ê° Phase ì™„ë£Œ ì‹œ WorkerStateë¥¼ JSONìœ¼ë¡œ ìë™ ì €ì¥ (`/tmp/distsort/checkpoints/`)
- **ë³µêµ¬**: Worker ì¬ì‹œì‘ ì‹œ ìµœì‹  checkpoint ë¡œë“œ â†’ ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ
- **ì˜ˆì‹œ**: Shuffle ì¤‘ crash â†’ PHASE_SORTING checkpoint ë¡œë“œ â†’ Shuffleë§Œ ì¬ì‹œë„ (Sampling/Sort ìŠ¤í‚µ)
- **ì¥ì **:
  - ë¹ ë¥¸ ë³µêµ¬ (ì „ì²´ ì¬ì‹œì‘ ëŒ€ë¹„ ì‹œê°„ ì ˆì•½)
  - ì •í™•ì„± ë³´ì¥ (Phaseë³„ ì™„ë£Œ ì‹œì  checkpoint)
  - Graceful Shutdown í†µí•© (30ì´ˆ grace period)

#### Q3: "Nâ†’M ì „ëµì˜ ì¥ì ì€?"

**ë‹µë³€**:
- ë¡œë“œ ë°¸ëŸ°ì‹± ê°œì„ : íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€ë¡œ í¬ê¸° ë¶ˆê· í˜• ì™„í™”
- ë©€í‹°ì½”ì–´ í™œìš©: ê° Workerê°€ ì—¬ëŸ¬ íŒŒí‹°ì…˜ ë³‘ë ¬ merge
- ì˜ˆ: 4 workers, 12 partitions â†’ ê° Workerê°€ 3ê°œ íŒŒí‹°ì…˜ ë³‘ë ¬ merge

#### Q4: "ASCII/Binary ìë™ ê°ì§€ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜?"

**ë‹µë³€**:
- íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ì½ê¸°
- ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
- ë¹„ìœ¨ > 90% â†’ ASCII, ê·¸ ì™¸ â†’ Binary
- ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì  ê°ì§€ â†’ í˜¼í•© ì…ë ¥ ì§€ì›

#### Q5: "í˜„ì¬ ì§„í–‰ë¥ ì€?"

**ë‹µë³€**:
- Week 1-2: ì„¤ê³„ ë‹¨ê³„ 100% ì™„ë£Œ
- Week 3: Record í´ë˜ìŠ¤ 100% ì™„ë£Œ (5/5 tests passing)
- Week 4-5: Core I/O Components 100% ì™„ë£Œ
  - RecordReader, InputFormatDetector, FileLayout, RecordWriter
  - ExternalSorter, KWayMerger, CheckpointManager
- Week 6: Master/Worker í†µí•© ì§„í–‰ ì¤‘

**ì „ì²´ ì§„í–‰ë¥ **: ì•½ 60% (ì„¤ê³„ + í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì™„ë£Œ, Master/Worker í†µí•© ì§„í–‰ ì¤‘)

#### Q6: "External Sortê°€ ë©”ëª¨ë¦¬ ì œí•œì„ ì–´ë–»ê²Œ ì¤€ìˆ˜í•˜ë‚˜?"

**ë‹µë³€**:
- `memoryLimit = 512MB` ì„¤ì •
- `recordsPerChunk = memoryLimit / 100` ê³„ì‚°
- ì˜ˆ: 512MB Ã· 100 bytes = 5,242,880 records per chunk
- 50GB ì…ë ¥ â†’ ì•½ 100 chunks ìƒì„±
- ê° chunkë¥¼ ë³‘ë ¬ë¡œ ì •ë ¬ â†’ ë””ìŠ¤í¬ì— ì €ì¥
- K-way mergeë¡œ ìµœì¢… ë³‘í•© (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)

#### Q7: "Challenge 1, 2, 3ê°€ ëª¨ë‘ í•´ê²°ë˜ì—ˆë‚˜?"

**ë‹µë³€**:
- **Challenge 1 (ë©”ëª¨ë¦¬ ì œí•œ)**: âœ… ExternalSorter + KWayMergerë¡œ í•´ê²°
- **Challenge 2 (ë¶„ì‚°)**: âœ… Master-Worker + Shuffleë¡œ í•´ê²°
- **Challenge 3 (Fault Tolerance)**: âœ… Checkpoint + Graceful Shutdownìœ¼ë¡œ í•´ê²°
- **Additional Requirements**: âœ… ëª¨ë‘ êµ¬í˜„ ì™„ë£Œ

### ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ (Week 7 ì´í›„)

```bash
# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
$ gensort -b0 10000000 /data1/input/test.dat  # 1GB

# 2. Master ì‹œì‘ (4 workers, 12 partitions)
$ sbt "runMain distsort.Main master 4 12"
[INFO] Master started on port 30000
[INFO] Waiting for 4 workers to register...

# 3. Worker ì‹œì‘ (4ëŒ€)
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1/input -O /data1/output"
[INFO] Worker registered with ID W0

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data2/input -O /data2/output"
[INFO] Worker registered with ID W1

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data3/input -O /data3/output"
[INFO] Worker registered with ID W2

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data4/input -O /data4/output"
[INFO] Worker registered with ID W3

# 4. Master ì¶œë ¥
192.168.1.100:30000
worker0, worker1, worker2, worker3

# 5. ê²°ê³¼ ê²€ì¦
$ valsort /data1/output/partition.* /data2/output/partition.* \
          /data3/output/partition.* /data4/output/partition.*
SUCCESS
```

---

## 8. ì°¸ê³  ë¬¸í—Œ

### 8.1 í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

- Knuth, Donald E. *The Art of Computer Programming, Volume 3: Sorting and Searching*. Addison-Wesley, 1998.
  - External Sorting ì•Œê³ ë¦¬ì¦˜ (5.4ì ˆ)

- TeraSort: A Sample Hadoop Application
  - Sampling for Partitioning ê¸°ë²•

### 8.2 ì‹œìŠ¤í…œ ì„¤ê³„

- Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: Simplified Data Processing on Large Clusters." *OSDI* 2004.
  - Master-Worker ì•„í‚¤í…ì²˜, Fault Tolerance ì „ëµ

- Zaharia, Matei, et al. "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing." *NSDI* 2012.
  - Checkpoint ê¸°ë°˜ Fault Recovery

### 8.3 êµ¬í˜„ ì°¸ê³ 

- gRPC ê³µì‹ ë¬¸ì„œ: https://grpc.io/docs/languages/scala/
- Protocol Buffers: https://protobuf.dev/
- **gensort/valsort**: http://www.ordinal.com/gensort.html
  - ì •ë ¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ë° ê²€ì¦ ë„êµ¬

### 8.4 í”„ë¡œì íŠ¸ ë¬¸ì„œ

- `plan/2025-10-24_plan_ver3.md`: ì „ì²´ ì‹œìŠ¤í…œ ì„¤ê³„
- `docs/0-implementation-decisions.md`: í•µì‹¬ êµ¬í˜„ ê²°ì • ì‚¬í•­
- `docs/1-phase-coordination.md`: Phase ë™ê¸°í™” í”„ë¡œí† ì½œ
- `docs/4-error-recovery.md`: ì¥ì•  ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- `docs/6-parallelization.md`: ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬
- `docs/7-testing-strategy.md`: TDD ê°€ì´ë“œ

---

**ë°œí‘œ ì¤€ë¹„ ì™„ë£Œ**
**Team Silver** - 2025-11-16

---

## Appendix: í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project_2025/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”‚   â””â”€â”€ build.properties
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ scala/distsort/
â”‚       â”‚   â”œâ”€â”€ Main.scala
â”‚       â”‚   â”œâ”€â”€ core/
â”‚       â”‚   â”‚   â”œâ”€â”€ Record.scala               âœ… Week 4
â”‚       â”‚   â”‚   â”œâ”€â”€ RecordReader.scala         âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ BinaryRecordReader.scala   âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ AsciiRecordReader.scala    âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ InputFormatDetector.scala  âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ FileLayout.scala           âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ RecordWriter.scala         âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ ExternalSorter.scala       âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ Partitioner.scala          âœ… Week 5
â”‚       â”‚   â”‚   â””â”€â”€ KWayMerger.scala           âœ… Week 5
â”‚       â”‚   â”œâ”€â”€ checkpoint/
â”‚       â”‚   â”‚   â””â”€â”€ CheckpointManager.scala    âœ… Week 5
â”‚       â”‚   â”œâ”€â”€ shutdown/
â”‚       â”‚   â”‚   â””â”€â”€ GracefulShutdownManager.scala âœ… Week 6
â”‚       â”‚   â”œâ”€â”€ master/
â”‚       â”‚   â”‚   â”œâ”€â”€ MasterServer.scala         ğŸ“‹ Week 6
â”‚       â”‚   â”‚   â””â”€â”€ PhaseTracker.scala         ğŸ“‹ Week 6
â”‚       â”‚   â””â”€â”€ worker/
â”‚       â”‚       â”œâ”€â”€ Worker.scala               ğŸ“‹ Week 6
â”‚       â”‚       â””â”€â”€ WorkerService.scala        ğŸ“‹ Week 6
â”‚       â””â”€â”€ protobuf/
â”‚           â””â”€â”€ distsort.proto                 ğŸ“‹ Week 6
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 0-implementation-decisions.md
â”‚   â”œâ”€â”€ 1-phase-coordination.md
â”‚   â”œâ”€â”€ 4-error-recovery.md
â”‚   â”œâ”€â”€ 6-parallelization.md
â”‚   â””â”€â”€ 7-testing-strategy.md
â””â”€â”€ plan/
    â””â”€â”€ 2025-10-24_plan_ver3.md
```
