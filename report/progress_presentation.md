# ë¶„ì‚° ì •ë ¬ ì‹œìŠ¤í…œ ì¤‘ê°„ ë°œí‘œ
**Team Silver** - ê¶Œë™ì—°, ë°•ë²”ìˆœ, ì„ì§€í›ˆ

**ë°œí‘œì¼**: 2025-11-16
**í”„ë¡œì íŠ¸**: Fault-Tolerant Distributed Sorting System

---

## ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [TDD ê¸°ë°˜ ê°œë°œ ë°©ë²•ë¡ ](#2-tdd-ê¸°ë°˜-ê°œë°œ-ë°©ë²•ë¡ )
3. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#3-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
4. [í•µì‹¬ ì„¤ê³„ ê²°ì • ì‚¬í•­](#4-í•µì‹¬-ì„¤ê³„-ê²°ì •-ì‚¬í•­)
5. [í˜„ì¬ ì§„í–‰ ìƒí™© (Week 4-5)](#5-í˜„ì¬-ì§„í–‰-ìƒí™©-week-4-5)
6. [êµ¬í˜„ ìƒì„¸ (Week 4-5)](#6-êµ¬í˜„-ìƒì„¸-week-4-5)
7. [í–¥í›„ ê³„íš (Week 6-8)](#7-í–¥í›„-ê³„íš-week-6-8)
8. [Q&A ì¤€ë¹„](#8-qa-ì¤€ë¹„)
9. [ì°¸ê³  ë¬¸í—Œ](#9-ì°¸ê³ -ë¬¸í—Œ)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ

ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚° ì €ì¥ëœ ëŒ€ìš©ëŸ‰ key/value ë ˆì½”ë“œë¥¼ ì •ë ¬í•˜ëŠ” **ì¥ì•  í—ˆìš©ì„± ë¶„ì‚° ì‹œìŠ¤í…œ** êµ¬í˜„

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­

| í•­ëª© | ìš”êµ¬ì‚¬í•­ | êµ¬í˜„ ì „ëµ |
|------|---------|----------|
| **ì…ë ¥** | ì—¬ëŸ¬ Worker ë…¸ë“œì— ë¶„ì‚°ëœ ë¯¸ì •ë ¬ ë°ì´í„° | ASCII/Binary ìë™ ê°ì§€ |
| **ì¶œë ¥** | ì „ì—­ì ìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„° | Range-based Partitioning |
| **ì¥ì•  í—ˆìš©** | Worker crash í›„ ì¬ì‹œì‘ ì‹œ ì •ìƒ ë™ì‘ | Worker Re-registration |
| **í™•ì¥ì„±** | ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬ | ThreadPool ê¸°ë°˜ |
| **ê²€ì¦** | gensort/valsort ë„êµ¬ í™œìš© | TDD ê¸°ë°˜ ê°œë°œ |

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

```
ì–¸ì–´:        Scala 2.13
ë¹Œë“œ ë„êµ¬:   SBT 1.9.7
RPC:         gRPC + Protocol Buffers
í…ŒìŠ¤íŠ¸:      ScalaTest
ë²„ì „ ê´€ë¦¬:   Git
```

---

## 2. TDD ê¸°ë°˜ ê°œë°œ ë°©ë²•ë¡ 

### 2.1 TDDë€?

**Test-Driven Development**: êµ¬í˜„ ì „ì— í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ëŠ” ê°œë°œ ë°©ì‹

#### Red-Green-Refactor Cycle

```
1. ğŸ”´ RED     : ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±
                (ì›í•˜ëŠ” ë™ì‘ ì •ì˜)

2. ğŸŸ¢ GREEN   : ìµœì†Œí•œì˜ ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸ í†µê³¼
                (ì¼ë‹¨ ë™ì‘í•˜ê²Œ ë§Œë“¤ê¸°)

3. ğŸ”µ REFACTOR: ì½”ë“œ ê°œì„  (í…ŒìŠ¤íŠ¸ëŠ” ê³„ì† í†µê³¼)
                (ê¹”ë”í•˜ê²Œ ë§Œë“¤ê¸°)
```

### 2.2 ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ TDDì˜ ì¤‘ìš”ì„±

| ë¬¸ì œ | TDDë¥¼ í†µí•œ í•´ê²° |
|------|----------------|
| ë³µì¡í•œ ìƒíƒœ ì „ì´ | í…ŒìŠ¤íŠ¸ë¡œ ëª…í™•í•˜ê²Œ ê²€ì¦ |
| gRPC í†µì‹  ì˜¤ë¥˜ | ì‹œë‚˜ë¦¬ì˜¤ ì‚¬ì „ ì •ì˜ |
| Race condition | ì¡°ê¸° ë°œê²¬ ê°€ëŠ¥ |
| Fault tolerance | ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦ |
| ë¦¬íŒ©í† ë§ | ì•ˆì „ì„± ë³´ì¥ |

### 2.3 TDD ì‹¤ì „ ì˜ˆì‹œ - Record í´ë˜ìŠ¤

#### Step 1: ğŸ”´ RED - ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±

```scala
class RecordSpec extends AnyFlatSpec with Matchers {
  "Record" should "store 10-byte key and 90-byte value" in {
    val key = Array.fill[Byte](10)(1)
    val value = Array.fill[Byte](90)(2)

    val record = Record(key, value)

    record.key.length shouldBe 10
    record.value.length shouldBe 90
  }

  it should "compare records by key only (unsigned)" in {
    // 0xFF (255) > 0x01 (1) in unsigned comparison
    val rec1 = Record(Array[Byte](0xFF.toByte) ++ Array.fill[Byte](9)(0),
                      Array.fill[Byte](90)(0))
    val rec2 = Record(Array[Byte](0x01) ++ Array.fill[Byte](9)(0),
                      Array.fill[Byte](90)(0))

    rec1.compare(rec2) should be > 0
  }
}
```

**ì‹¤í–‰ ê²°ê³¼**: âŒ ì»´íŒŒì¼ ì—ëŸ¬ (Record í´ë˜ìŠ¤ ë¯¸ì¡´ì¬)

#### Step 2: ğŸŸ¢ GREEN - ìµœì†Œ êµ¬í˜„

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes")
  require(value.length == 90, s"Value must be 90 bytes")

  override def compare(that: Record): Int = {
    // âš ï¸ ì¤‘ìš”: Unsigned ë¹„êµ í•„ìˆ˜!
    var i = 0
    while (i < 10) {
      val byte1 = this.key(i) & 0xFF  // Signed â†’ Unsigned ë³€í™˜
      val byte2 = that.key(i) & 0xFF
      if (byte1 != byte2) return byte1 - byte2
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value
}
```

**ì‹¤í–‰ ê²°ê³¼**: âœ… í…ŒìŠ¤íŠ¸ í†µê³¼!

#### Step 3: ğŸ”µ REFACTOR - ì½”ë“œ ê°œì„ 

```scala
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes, got ${key.length}")
  require(value.length == 90, s"Value must be 90 bytes, got ${value.length}")

  override def compare(that: Record): Int = {
    var i = 0
    while (i < 10) {
      val unsigned1 = this.key(i) & 0xFF
      val unsigned2 = that.key(i) & 0xFF
      val diff = unsigned1 - unsigned2
      if (diff != 0) return diff
      i += 1
    }
    0
  }

  def toBytes: Array[Byte] = key ++ value

  override def toString: String =
    s"Record(key=${key.take(3).map("%02X".format(_)).mkString("")}..., value=${value.length}B)"
}
```

**ì‹¤í–‰ ê²°ê³¼**: âœ… í…ŒìŠ¤íŠ¸ ì—¬ì „íˆ í†µê³¼ + ê°€ë…ì„± í–¥ìƒ

### 2.4 ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ TDD ì ìš©

#### Testing Pyramid

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ E2E Tests   â”‚  ~ 10%
            â”‚ (ëŠë¦¼)      â”‚  (ì „ì²´ ì‹œìŠ¤í…œ)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Integration Testsâ”‚  ~ 20%
         â”‚ (ë³´í†µ)           â”‚  (ì»´í¬ë„ŒíŠ¸ ê°„)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Unit Tests            â”‚  ~ 70%
    â”‚   (ë¹ ë¦„)                â”‚  (í•¨ìˆ˜/í´ë˜ìŠ¤)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í…ŒìŠ¤íŠ¸ ì‘ì„± í˜„í™© (Week 4-5 ê¸°ì¤€)

| ì»´í¬ë„ŒíŠ¸ | í…ŒìŠ¤íŠ¸ ê°œìˆ˜ | ìƒíƒœ |
|---------|-----------|------|
| Record | 5 tests | âœ… ì™„ë£Œ |
| RecordReader (Binary) | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • |
| RecordReader (ASCII) | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • |
| InputFormatDetector | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • |
| FileLayout | ğŸ“‹ ì„¤ê³„ | ì˜ˆì • |

---

## 3. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 3.1 ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Master Node                        â”‚
â”‚  - Worker ë“±ë¡ ê´€ë¦¬                                  â”‚
â”‚  - ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°                       â”‚
â”‚  - shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸                    â”‚
â”‚  - Phase ë™ê¸°í™” ì¡°ìœ¨                                 â”‚
â”‚  - ìµœì¢… Worker ìˆœì„œ ì¶œë ¥                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚          â”‚          â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚ Worker 0 â”‚   â”‚Worker 1 â”‚ â”‚Worker 2 â”‚
   â”‚          â”‚   â”‚         â”‚ â”‚         â”‚
   â”‚Input:    â”‚   â”‚Input:   â”‚ â”‚Input:   â”‚
   â”‚50GB      â”‚   â”‚50GB     â”‚ â”‚50GB     â”‚
   â”‚          â”‚   â”‚         â”‚ â”‚         â”‚
   â”‚Output:   â”‚   â”‚Output:  â”‚ â”‚Output:  â”‚
   â”‚P0,P1,P2  â”‚   â”‚P3,P4,P5 â”‚ â”‚P6,P7,P8 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       Worker-to-Worker Shuffle
       (gRPC Streaming)
```

### 3.2 5-Phase ì‹¤í–‰ íë¦„

```
Phase 0: Initialization
  â”œâ”€ Master ì‹œì‘, Workerë“¤ ë“±ë¡ ëŒ€ê¸°
  â”œâ”€ ê° Worker ì‹œì‘, Masterì— ì—°ê²°
  â”œâ”€ Masterê°€ Workerì— index í• ë‹¹
  â””â”€ ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦

Phase 1: Sampling
  â”œâ”€ ê° Workerê°€ ì…ë ¥ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
  â”œâ”€ ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚° (0.01% ~ 1%)
  â”œâ”€ Masterì—ê²Œ ìƒ˜í”Œ ì „ì†¡
  â”œâ”€ Masterê°€ ì „ì²´ ìƒ˜í”Œ ì •ë ¬
  â”œâ”€ íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚° (Nê°œ or Mê°œ)
  â””â”€ shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸

Phase 2: Sort & Partition
  â”œâ”€ ê° Workerê°€ ì…ë ¥ íŒŒì¼ ì½ê¸° (ASCII/Binary ìë™ ì²˜ë¦¬)
  â”œâ”€ Chunk ë‹¨ìœ„ ë©”ëª¨ë¦¬ ë‚´ ì •ë ¬ (ë³‘ë ¬)
  â”œâ”€ íŒŒí‹°ì…˜ ê²½ê³„ì— ë”°ë¼ ë¶„í•  (Nê°œ ë˜ëŠ” Mê°œ)
  â””â”€ íŒŒí‹°ì…˜ë³„ ì„ì‹œ íŒŒì¼ ìƒì„±

Phase 3: Shuffle
  â”œâ”€ shuffleMapì— ë”°ë¼ íŒŒí‹°ì…˜ ì „ì†¡
  â”œâ”€ Worker ê°„ ë„¤íŠ¸ì›Œí¬ í†µì‹  (gRPC streaming)
  â”œâ”€ ì¬ì‹œë„ ë¡œì§ (ì§€ìˆ˜ ë°±ì˜¤í”„)
  â””â”€ ìˆ˜ì‹  í™•ì¸ ë° ì„ì‹œ ì €ì¥

Phase 4: Merge
  â”œâ”€ ê° Workerê°€ ë°›ì€ íŒŒí‹°ì…˜ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ K-way merge
  â”œâ”€ Priority Queue (min-heap) ì‚¬ìš©
  â”œâ”€ ìµœì¢… partition.n íŒŒì¼ ìƒì„±
  â””â”€ Atomic write ë³´ì¥ (temp + rename)

Phase 5: Completion
  â”œâ”€ ê° Workerê°€ Masterì—ê²Œ ì™„ë£Œ ë³´ê³ 
  â”œâ”€ Masterê°€ ì „ì²´ ì‘ì—… ì™„ë£Œ í™•ì¸
  â”œâ”€ Masterê°€ ì •ë ¬ëœ Worker ì£¼ì†Œ ì¶œë ¥ (stdout)
  â””â”€ ì„ì‹œ íŒŒì¼ ì •ë¦¬
```

### 3.3 ë ˆì½”ë“œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Key (10B)    â”‚ Value (90B)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  0            10                                      100

íŠ¹ì§•:
- ê³ ì • ê¸¸ì´: 100 ë°”ì´íŠ¸
- Keyë§Œ ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© (unsigned ë¹„êµ)
- ValueëŠ” ì •ë ¬ê³¼ ë¬´ê´€í•˜ê²Œ Keyì™€ í•¨ê»˜ ì´ë™
```

### 3.4 íŒŒí‹°ì…˜ ì „ëµ (Nâ†’M Strategy)

**PDF ìš”êµ¬ì‚¬í•­**:
> "numPartitions - íŒŒí‹°ì…˜ ê°œìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ ì›Œì»¤ ìˆ˜ì™€ ë™ì¼ **ë˜ëŠ” ë°°ìˆ˜**)"

#### Strategy B: Advanced (N Workers â†’ M Partitions, M > N)

**ê°œë…**:
- íŒŒí‹°ì…˜ ìˆ˜ > Worker ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ M = 3N)
- ê° WorkerëŠ” **ì—¬ëŸ¬ íŒŒí‹°ì…˜**ì„ ë‹´ë‹¹
- Partition iëŠ” Worker (i / partitionsPerWorker)ê°€ ë‹´ë‹¹

**ì˜ˆì‹œ: 3 Workers, 9 Partitions**

```
Phase 2: Sort & Partition
  Worker 0, 1, 2 ê°ê° â†’ P0~P8 (9ê°œ) ìƒì„±

Phase 3: Shuffle
  P0, P1, P2 â†’ Worker 0
  P3, P4, P5 â†’ Worker 1
  P6, P7, P8 â†’ Worker 2

Phase 4: Merge
  Worker 0:
    - 3ê°œ P0 ì¡°ê° merge â†’ partition.0
    - 3ê°œ P1 ì¡°ê° merge â†’ partition.1
    - 3ê°œ P2 ì¡°ê° merge â†’ partition.2

ìµœì¢… ì¶œë ¥:
  /worker0/output/partition.0  â† ê°€ì¥ ì‘ì€ key
  /worker0/output/partition.1
  /worker0/output/partition.2
  /worker1/output/partition.3
  /worker1/output/partition.4
  /worker1/output/partition.5
  /worker2/output/partition.6
  /worker2/output/partition.7
  /worker2/output/partition.8  â† ê°€ì¥ í° key

ì½ê¸° ìˆœì„œ: partition.0 â†’ 1 â†’ 2 â†’ ... â†’ 8 = ì „ì—­ ì •ë ¬ë¨
```

**ì¥ì **:
- âœ… ë¡œë“œ ë°¸ëŸ°ì‹± ê°œì„  (íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€)
- âœ… ë©€í‹°ì½”ì–´ í™œìš© ì¦ê°€ (ë³‘ë ¬ merge)
- âœ… íŒŒí‹°ì…˜ í¬ê¸° ë¶ˆê· í˜• ì™„í™”
- âœ… PDF ìš”êµ¬ì‚¬í•­ ì¶©ì¡±

---

## 4. í•µì‹¬ ì„¤ê³„ ê²°ì • ì‚¬í•­

### 4.1 Fault Tolerance: Checkpoint-based Recovery

**PDF ìš”êµ¬ì‚¬í•­**:
> "The system must be fault-tolerant, which means that if a worker crashes and restarts, the overall computation should still produce correct results."

#### ì‹¤ì œ êµ¬í˜„ ì „ëµ: Checkpoint-based Recovery

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fault Tolerance Strategy (ì‹¤ì œ êµ¬í˜„)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Checkpoint ì €ì¥:                                â”‚
â”‚   âœ… ê° Phase ì™„ë£Œ ì‹œ ìë™ ì €ì¥                  â”‚
â”‚   âœ… WorkerStateë¥¼ JSONìœ¼ë¡œ ì˜ì†í™”               â”‚
â”‚   âœ… ìœ„ì¹˜: /tmp/distsort/checkpoints/           â”‚
â”‚   âœ… ìµœê·¼ 3ê°œ checkpoint ìœ ì§€                    â”‚
â”‚                                                 â”‚
â”‚ Worker Crash & Restart:                         â”‚
â”‚   âœ… ì‹œì‘ ì‹œ ìµœì‹  checkpoint ë¡œë“œ                â”‚
â”‚   âœ… ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ                  â”‚
â”‚   âœ… Sampling/Sort ì¬ìˆ˜í–‰ ë¶ˆí•„ìš”                 â”‚
â”‚                                                 â”‚
â”‚ Graceful Shutdown:                              â”‚
â”‚   âœ… 30ì´ˆ grace period                          â”‚
â”‚   âœ… í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸°                        â”‚
â”‚   âœ… Checkpoint ì €ì¥ í›„ ì¢…ë£Œ                     â”‚
â”‚                                                 â”‚
â”‚ Data Integrity:                                 â”‚
â”‚   âœ… Atomic writes (temp + rename)              â”‚
â”‚   âœ… State-based cleanup on failure             â”‚
â”‚   âœ… Idempotent operations                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CheckpointManager êµ¬í˜„

**ì €ì¥ë˜ëŠ” ìƒíƒœ (WorkerState)**:
```scala
case class WorkerState(
  processedRecords: Long,              // ì²˜ë¦¬í•œ ë ˆì½”ë“œ ìˆ˜
  partitionBoundaries: List[Array[Byte]],  // íŒŒí‹°ì…˜ ê²½ê³„
  shuffleMap: Map[Int, Int],           // íŒŒí‹°ì…˜ â†’ Worker ë§¤í•‘
  completedPartitions: Set[Int],       // ì™„ë£Œí•œ íŒŒí‹°ì…˜ë“¤
  currentFiles: List[String],          // í˜„ì¬ íŒŒì¼ë“¤
  phaseMetadata: Map[String, String]   // Phase ë©”íƒ€ë°ì´í„°
)
```

**Checkpoint ì €ì¥ ì‹œì **:
```scala
performSampling()
  â†’ savePhaseCheckpoint(PHASE_SAMPLING, 1.0)       // âœ…

getPartitionConfiguration()
  â†’ savePhaseCheckpoint(PHASE_WAITING_FOR_PARTITIONS, 1.0)  // âœ…

performLocalSort()
  â†’ savePhaseCheckpoint(PHASE_SORTING, 1.0)        // âœ…

performShuffle()
  â†’ savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)      // âœ…

performMerge()
  â†’ savePhaseCheckpoint(PHASE_MERGING, 1.0)        // âœ…
  â†’ checkpointManager.deleteAllCheckpoints()       // ì„±ê³µ ì‹œ ì‚­ì œ
```

#### ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ

```
Worker crash during Shuffle:
  Last checkpoint: PHASE_SORTING (100% ì™„ë£Œ)

Worker restart:
  1. recoverFromCheckpoint() ì„±ê³µ
  2. currentPhase = PHASE_SORTING (ë³µì›)
  3. performShuffle() ì¬ì‹œì‘
     â­ Sampling/SortëŠ” ìŠ¤í‚µ (ì‹œê°„ ëŒ€í­ ì ˆì•½)
  4. Merge â†’ ì™„ë£Œ

Result:
  âœ… ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ
  âœ… ë¹ ë¥¸ ë³µêµ¬ (ì „ì²´ ì¬ì‹œì‘ ëŒ€ë¹„)
  âœ… ì •í™•ì„± ë³´ì¥
```

#### ì •ë‹¹í™” (Justification)

| ê·¼ê±° | ì„¤ëª… |
|------|------|
| **ë¹ ë¥¸ ë³µêµ¬** | ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ â†’ ì „ì²´ ì¬ì‹œì‘ë³´ë‹¤ íš¨ìœ¨ì  |
| **ì •í™•ì„± ë³´ì¥** | Phaseë³„ ì™„ë£Œ checkpoint â†’ ë¶€ë¶„ ê²°ê³¼ ìœ ì‹¤ ì—†ìŒ |
| **ì‹¤ì œ ì‹œìŠ¤í…œ** | Spark (RDD lineage + checkpoint), Flink (checkpoint ê¸°ë°˜ exactly-once), MapReduce (Task-level restart) |
| **êµ¬í˜„ ê°€ëŠ¥ì„±** | CheckpointManager + JSON ì§ë ¬í™” + Graceful Shutdown í†µí•© |
| **PDF í•´ì„** | "produce correct results" â† Checkpointê°€ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ëª¨ë‘ ì¶©ì¡± |

### 4.2 Nâ†’M Partition Strategy

**ëª©ì **: Merge ë‹¨ê³„ì—ì„œ ë©€í‹°ì½”ì–´ í™œìš©

#### shuffleMap ìƒì„± ë¡œì§

```scala
def createShuffleMap(numWorkers: Int, numPartitions: Int): Map[Int, Int] = {
  val shuffleMap = mutable.Map[Int, Int]()
  val partitionsPerWorker = numPartitions / numWorkers

  for (partitionID <- 0 until numPartitions) {
    // íŒŒí‹°ì…˜ IDë¥¼ Worker IDë¡œ ë§¤í•‘
    val workerID = partitionID / partitionsPerWorker
    val finalWorkerID = if (workerID >= numWorkers) numWorkers - 1 else workerID
    shuffleMap(partitionID) = finalWorkerID
  }

  shuffleMap.toMap
}

// ì˜ˆì‹œ
// createShuffleMap(3, 9)
//   â†’ {0â†’0, 1â†’0, 2â†’0, 3â†’1, 4â†’1, 5â†’1, 6â†’2, 7â†’2, 8â†’2}
//
// ê²°ê³¼: Worker 0 = [0,1,2], Worker 1 = [3,4,5], Worker 2 = [6,7,8]
//       ê° WorkerëŠ” ì—°ì†ëœ partition ë²ˆí˜¸ ë‹´ë‹¹
```

#### íŒŒí‹°ì…˜ í• ë‹¹ ê³µì‹ (Range-based)

```scala
def assignWorker(partitionID: Int, numWorkers: Int, numPartitions: Int): Int = {
  val partitionsPerWorker = numPartitions / numWorkers
  val workerID = partitionID / partitionsPerWorker
  if (workerID >= numWorkers) numWorkers - 1 else workerID
}
```

### 4.3 ASCII/Binary ìë™ ê°ì§€

**PDF ìš”êµ¬ì‚¬í•­**:
> "Should work on both ASCII and binary input **without requiring an option**"

#### InputFormatDetector ì•Œê³ ë¦¬ì¦˜

```scala
object InputFormatDetector {
  private val SAMPLE_SIZE = 1000      // íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ë¶„ì„
  private val ASCII_THRESHOLD = 0.9   // 90% ì´ìƒ ASCII â†’ ASCII í˜•ì‹

  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](SAMPLE_SIZE)
    val inputStream = new FileInputStream(file)

    try {
      val bytesRead = inputStream.read(buffer)

      if (bytesRead <= 0) {
        logger.warn(s"Empty file: ${file.getName}, defaulting to Binary")
        return DataFormat.Binary
      }

      // ASCII printable: 0x20-0x7E, plus \n (0x0A), \r (0x0D)
      val asciiLikeCount = buffer.take(bytesRead).count { b =>
        (b >= 32 && b <= 126) || b == '\n' || b == '\r'
      }

      val asciiRatio = asciiLikeCount.toDouble / bytesRead

      if (asciiRatio > ASCII_THRESHOLD) {
        DataFormat.Ascii
      } else {
        DataFormat.Binary
      }
    } finally {
      inputStream.close()
    }
  }
}
```

**í˜¼í•© ì…ë ¥ ì²˜ë¦¬**:
- ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í˜•ì‹ ê°ì§€
- ASCIIì™€ Binary íŒŒì¼ í˜¼ì¬ ê°€ëŠ¥
- RecordReader Factory Patternìœ¼ë¡œ ë™ì  ìƒì„±

### 4.4 External Sort (2-Pass Algorithm)

**ëª©ì **: ë©”ëª¨ë¦¬ë³´ë‹¤ í° ë°ì´í„° ì •ë ¬

#### Phase 1: Chunk Sort (ë³‘ë ¬)

```scala
class ExternalSorter(numThreads: Int = Runtime.getRuntime.availableProcessors()) {
  private val executor = Executors.newFixedThreadPool(numThreads)

  def sortInParallel(inputFile: File, chunkSize: Long): List[File] = {
    val chunks = splitIntoChunks(inputFile, chunkSize)

    val futures = chunks.map { chunk =>
      Future {
        // ë©”ëª¨ë¦¬ ë‚´ ì •ë ¬
        val records = readChunk(chunk).toArray
        records.sortInPlace()
        writeSortedChunk(records)
      }(ExecutionContext.fromExecutor(executor))
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

#### Phase 2: K-way Merge (Priority Queue)

```scala
def kWayMerge(sortedFiles: List[File], output: File): Unit = {
  // Min-heap for K-way merge
  val heap = mutable.PriorityQueue[RecordWithSource]()(
    Ordering.by[RecordWithSource, Array[Byte]](_.record.key).reverse
  )

  // ê° íŒŒì¼ì—ì„œ ì²« ë ˆì½”ë“œ ì½ê¸°
  readers.zipWithIndex.foreach { case (reader, idx) =>
    reader.readRecord().foreach { record =>
      heap.enqueue(RecordWithSource(record, idx))
    }
  }

  // Merge
  while (heap.nonEmpty) {
    val min = heap.dequeue()
    outputWriter.write(min.record)

    // ê°™ì€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìŒ ë ˆì½”ë“œ ì½ê¸°
    readers(min.sourceId).readRecord().foreach { record =>
      heap.enqueue(RecordWithSource(record, min.sourceId))
    }
  }
}
```

### 4.5 gRPC ê¸°ë°˜ í†µì‹ 

#### Protocol Buffers ì •ì˜ (í•µì‹¬ ë¶€ë¶„)

```protobuf
syntax = "proto3";

service MasterService {
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);
  rpc SendSample(SampleData) returns (Ack);
  rpc NotifyPhaseComplete(PhaseCompleteRequest) returns (Ack);
}

service WorkerService {
  rpc SetPartitionBoundaries(PartitionConfig) returns (Ack);
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);
  rpc StartShuffle(ShuffleSignal) returns (Ack);
  rpc StartMerge(MergeSignal) returns (Ack);
}

message PartitionConfig {
  repeated bytes boundaries = 1;       // N-1 or M-1 ê°œì˜ ê²½ê³„
  int32 num_partitions = 2;            // N or M
  map<int32, int32> shuffle_map = 3;   // partitionID â†’ workerID
  repeated WorkerInfo all_workers = 4;
}

message ShuffleDataChunk {
  int32 partition_id = 1;
  bytes data = 2;               // 1MB ì²­í¬ ë‹¨ìœ„
  int64 chunk_offset = 3;
  bool is_last = 4;
}
```

#### Shuffle ì¬ì‹œë„ ë¡œì§

```scala
def sendPartitionWithRetry(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo,
    maxRetries: Int = 3): Unit = {

  var attempt = 0
  var success = false

  while (attempt < maxRetries && !success) {
    try {
      sendPartition(partitionFile, partitionId, targetWorker)
      success = true
    } catch {
      case e: StatusRuntimeException if isRetryable(e) =>
        attempt += 1
        val backoffMs = math.pow(2, attempt).toLong * 1000  // ì§€ìˆ˜ ë°±ì˜¤í”„
        logger.warn(s"Send failed (attempt $attempt/$maxRetries), " +
                   s"retrying in ${backoffMs}ms")
        Thread.sleep(backoffMs)

      case e: Exception =>
        logger.error(s"Non-retryable error: ${e.getMessage}")
        throw e
    }
  }
}
```

---

## 5. í˜„ì¬ ì§„í–‰ ìƒí™© (Week 4-5)

### 5.1 ì „ì²´ ì§„í–‰ë¥ 

```
Week 1-2: ì„¤ê³„ ë‹¨ê³„ (100% ì™„ë£Œ)
  â”œâ”€ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì •ì˜
  â”œâ”€ Protocol Buffers ì„¤ê³„
  â”œâ”€ Fault Tolerance ì „ëµ ê²°ì •
  â””â”€ 7ê°œ ì„¤ê³„ ë¬¸ì„œ ì‘ì„±

Week 3: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° Record (100% ì™„ë£Œ)
  â”œâ”€ SBT í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
  â”œâ”€ gRPC stub ìƒì„±
  â””â”€ Record í´ë˜ìŠ¤ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸

Week 4-5: Core I/O Components (100% ì™„ë£Œ)
  â”œâ”€ RecordReader ì¶”ìƒí™”
  â”œâ”€ BinaryRecordReader êµ¬í˜„
  â”œâ”€ AsciiRecordReader êµ¬í˜„
  â”œâ”€ InputFormatDetector êµ¬í˜„
  â”œâ”€ FileLayout í´ë˜ìŠ¤ êµ¬í˜„
  â””â”€ RecordWriter êµ¬í˜„

Week 6: Algorithms (ì˜ˆì •)
  ğŸ“‹ ExternalSorter
  ğŸ“‹ Partitioner
  ğŸ“‹ KWayMerger

Week 7-8: Master/Worker Integration (ì˜ˆì •)
  ğŸ“‹ Master êµ¬í˜„
  ğŸ“‹ Worker êµ¬í˜„
  ğŸ“‹ Phase ë™ê¸°í™”
  ğŸ“‹ Fault Tolerance ê²€ì¦
```

### 5.2 êµ¬í˜„ ì™„ë£Œ í•­ëª© (Week 4-5)

#### âœ… 1. RecordReader ì¶”ìƒí™”

**ëª©ì **: ASCII/Binary í˜•ì‹ì„ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì²˜ë¦¬

```scala
// ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
trait RecordReader {
  def readRecord(input: InputStream): Option[Array[Byte]]
}

// Factory Pattern
object RecordReader {
  def create(format: DataFormat): RecordReader = format match {
    case DataFormat.Binary => new BinaryRecordReader()
    case DataFormat.Ascii  => new AsciiRecordReader()
  }
}

sealed trait DataFormat
object DataFormat {
  case object Binary extends DataFormat
  case object Ascii extends DataFormat
}
```

#### âœ… 2. BinaryRecordReader êµ¬í˜„

**íŠ¹ì§•**:
- 100-byte ê³ ì • ê¸¸ì´ ì½ê¸°
- BufferedInputStream ì‚¬ìš© (1MB ë²„í¼)
- EOF ì²˜ë¦¬

```scala
class BinaryRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val record = new Array[Byte](100)
    val bytesRead = input.read(record)

    if (bytesRead == 100) {
      Some(record)
    } else if (bytesRead == -1) {
      None  // EOF
    } else {
      throw new IOException(s"Incomplete record: $bytesRead bytes")
    }
  }
}
```

#### âœ… 3. AsciiRecordReader êµ¬í˜„

**ASCII í˜•ì‹**:
```
key(10 chars) + space(1) + value(90 chars) + newline(1) = 102 bytes
```

```scala
class AsciiRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val line = new Array[Byte](102)
    val bytesRead = input.read(line)

    if (bytesRead == -1) return None
    if (bytesRead != 102)
      throw new IOException(s"Invalid ASCII record: $bytesRead bytes")
    if (line(10) != ' '.toByte)
      throw new IOException("Expected space at position 10")
    if (line(101) != '\n'.toByte)
      throw new IOException("Expected newline at position 101")

    // 100 bytesë¡œ ë³€í™˜ (spaceì™€ newline ì œê±°)
    val record = new Array[Byte](100)
    System.arraycopy(line, 0, record, 0, 10)     // key
    System.arraycopy(line, 11, record, 10, 90)   // value

    Some(record)
  }
}
```

#### âœ… 4. InputFormatDetector êµ¬í˜„

**ë™ì‘ ì›ë¦¬**:
1. íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ì½ê¸°
2. ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
3. ë¹„ìœ¨ > 90% â†’ ASCII, ê·¸ ì™¸ â†’ Binary

**ì¥ì **:
- âœ… PDF ìš”êµ¬ì‚¬í•­ ì¶©ì¡±: "without requiring an option"
- âœ… ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì  ê°ì§€ (í˜¼í•© ì…ë ¥ ì§€ì›)
- âœ… ë¹ ë¥¸ íŒë³„ (1000 bytesë§Œ ì½ìŒ)

#### âœ… 5. FileLayout í´ë˜ìŠ¤

**ì—­í• **: íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬ ë° ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦

```scala
class FileLayout(
  inputDirs: List[File],
  outputDir: File,
  tempBaseDir: File,
  workerId: String
) {
  // ì…ë ¥ ë””ë ‰í† ë¦¬ ê²€ì¦ (ì½ê¸° ì „ìš©)
  def validateInputDirectories(): Unit = {
    inputDirs.foreach { dir =>
      require(dir.exists(), s"Input directory not found: $dir")
      require(dir.isDirectory, s"Not a directory: $dir")
      require(dir.canRead, s"Cannot read directory: $dir")
    }
  }

  // ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
  def createTemporaryStructure(): Unit = {
    val workDir = new File(tempBaseDir, s"sort_work_$workerId")
    val subdirs = List("samples", "sorted_chunks", "partitions", "received")

    subdirs.foreach { subdir =>
      val dir = new File(workDir, subdir)
      if (!dir.exists()) dir.mkdirs()
    }
  }

  // ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (í•„ìš” ê³µê°„ = inputSize * 2)
  def ensureSufficientDiskSpace(): Unit = {
    val totalInputSize = inputDirs.map(calculateDirSize).sum
    val requiredTemp = totalInputSize * 2
    val availableTemp = tempBaseDir.getUsableSpace

    require(availableTemp > requiredTemp * 1.5,
      s"Insufficient temp space: need ${requiredTemp * 1.5 / 1e9}GB")
  }

  // ì„ì‹œ íŒŒì¼ ì •ë¦¬
  def cleanupTemporaryFiles(): Unit = {
    val workDir = new File(tempBaseDir, s"sort_work_$workerId")
    if (workDir.exists()) {
      FileUtils.deleteRecursively(workDir)
    }
  }
}
```

**PDF ìš”êµ¬ì‚¬í•­ ì¶©ì¡±**:
- âœ… ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸ (ì½ê¸° ì „ìš©, ìˆ˜ì • ê¸ˆì§€)
- âœ… ì„ì‹œ íŒŒì¼ê³¼ ì¶œë ¥ íŒŒì¼ ë¶„ë¦¬
- âœ… ì‘ì—… ì™„ë£Œ í›„ ìë™ ì •ë¦¬

#### âœ… 6. RecordWriter êµ¬í˜„

**íŠ¹ì§•**:
- Binary/ASCII í˜•ì‹ ì¶œë ¥ ì§€ì›
- ë²„í¼ë§ (4MB) for I/O ìµœì í™”
- Atomic write (temp + rename)

```scala
class RecordWriter(outputFile: File, format: DataFormat) {
  private val tempFile = new File(outputFile.getParent,
                                  s".${outputFile.getName}.tmp")
  private val output = new BufferedOutputStream(
    new FileOutputStream(tempFile),
    4 * 1024 * 1024  // 4MB ë²„í¼
  )

  def writeRecord(record: Array[Byte]): Unit = {
    require(record.length == 100, s"Invalid record length: ${record.length}")

    format match {
      case DataFormat.Binary =>
        output.write(record)

      case DataFormat.Ascii =>
        // key(10) + space + value(90) + newline
        output.write(record, 0, 10)
        output.write(' '.toByte)
        output.write(record, 10, 90)
        output.write('\n'.toByte)
    }
  }

  def close(): Unit = {
    output.close()

    // Atomic rename
    if (!tempFile.renameTo(outputFile)) {
      throw new IOException(s"Failed to rename $tempFile to $outputFile")
    }
  }
}
```

---

## 6. êµ¬í˜„ ìƒì„¸ (Week 4-5)

### 6.1 ì‹¤ì œ êµ¬í˜„ ì½”ë“œ ë¶„ì„

#### Record í´ë˜ìŠ¤ (Week 4 ì™„ë£Œ)

**íŒŒì¼**: `distsort/core/Record.scala`

```scala
package distsort.core

/**
 * Represents a 100-byte record with 10-byte key and 90-byte value.
 *
 * Key comparison uses unsigned byte ordering, which is critical for
 * correct sorting behavior (e.g., 0xFF > 0x01).
 */
case class Record(key: Array[Byte], value: Array[Byte]) extends Ordered[Record] {
  require(key.length == 10, s"Key must be 10 bytes, got ${key.length}")
  require(value.length == 90, s"Value must be 90 bytes, got ${value.length}")

  /**
   * Compare this record to another by key only (unsigned).
   *
   * @return negative if this < that, 0 if equal, positive if this > that
   */
  override def compare(that: Record): Int = {
    var i = 0
    while (i < 10) {
      val unsigned1 = this.key(i) & 0xFF  // Convert signed to unsigned
      val unsigned2 = that.key(i) & 0xFF
      val diff = unsigned1 - unsigned2
      if (diff != 0) return diff
      i += 1
    }
    0
  }

  /** Serialize to 100 bytes */
  def toBytes: Array[Byte] = key ++ value

  /** Create a copy with the same data */
  def copy(): Record = Record(key.clone(), value.clone())

  override def toString: String = {
    val keyHex = key.take(3).map("%02X".format(_)).mkString("")
    s"Record(key=$keyHex..., value=${value.length}B)"
  }

  override def equals(obj: Any): Boolean = obj match {
    case that: Record =>
      java.util.Arrays.equals(this.key, that.key) &&
      java.util.Arrays.equals(this.value, that.value)
    case _ => false
  }

  override def hashCode(): Int = {
    java.util.Arrays.hashCode(key)
  }
}
```

**ì£¼ìš” í¬ì¸íŠ¸**:
1. **Unsigned ë¹„êµ í•„ìˆ˜**: `& 0xFF`ë¡œ signed â†’ unsigned ë³€í™˜
2. **ë¶ˆë³€ì„±**: case classë¡œ immutability ë³´ì¥
3. **ë¬¸ì„œí™”**: Scaladocìœ¼ë¡œ ëª¨ë“  public method ì„¤ëª…

#### InputFormatDetector (Week 5 ì™„ë£Œ)

**íŒŒì¼**: `distsort/core/InputFormatDetector.scala`

```scala
package distsort.core

import java.io.{File, FileInputStream}
import org.slf4j.LoggerFactory

/**
 * Automatically detects whether a file is ASCII or Binary format.
 *
 * Algorithm:
 *   1. Read first 1000 bytes of the file
 *   2. Count ASCII printable characters
 *   3. If ratio > 90%, classify as ASCII; otherwise Binary
 *
 * This satisfies the PDF requirement:
 * "Should work on both ASCII and binary input without requiring an option"
 */
object InputFormatDetector {
  private val logger = LoggerFactory.getLogger(getClass)

  private val SAMPLE_SIZE = 1000
  private val ASCII_THRESHOLD = 0.9

  /**
   * Detect the format of a file.
   *
   * @param file The file to analyze
   * @return DataFormat.Ascii or DataFormat.Binary
   */
  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](SAMPLE_SIZE)
    val inputStream = new FileInputStream(file)

    try {
      val bytesRead = inputStream.read(buffer)

      if (bytesRead <= 0) {
        logger.warn(s"Empty file: ${file.getName}, defaulting to Binary")
        return DataFormat.Binary
      }

      val asciiCount = buffer.take(bytesRead).count(isAsciiPrintable)
      val asciiRatio = asciiCount.toDouble / bytesRead

      logger.debug(s"File: ${file.getName}, ASCII ratio: $asciiRatio")

      if (asciiRatio > ASCII_THRESHOLD) {
        logger.info(s"Detected ASCII format for file: ${file.getName}")
        DataFormat.Ascii
      } else {
        logger.info(s"Detected Binary format for file: ${file.getName}")
        DataFormat.Binary
      }
    } finally {
      inputStream.close()
    }
  }

  /**
   * Check if a byte is ASCII printable or whitespace.
   *
   * @param b The byte to check
   * @return true if printable or whitespace
   */
  private def isAsciiPrintable(b: Byte): Boolean = {
    (b >= 32 && b <= 126) ||  // Printable ASCII (space to ~)
    b == '\n' ||               // Newline
    b == '\r'                  // Carriage return
  }
}
```

**ì„¤ê³„ íŒ¨í„´**: Singleton Object (Utility)

#### BinaryRecordReader (Week 5 ì™„ë£Œ)

**íŒŒì¼**: `distsort/core/BinaryRecordReader.scala`

```scala
package distsort.core

import java.io.{BufferedInputStream, File, FileInputStream, InputStream, IOException}
import org.slf4j.LoggerFactory

/**
 * Reads 100-byte binary records from a file.
 *
 * Each record consists of:
 *   - 10 bytes: key
 *   - 90 bytes: value
 *
 * Uses BufferedInputStream with 1MB buffer for efficient I/O.
 */
class BinaryRecordReader extends RecordReader {
  private val logger = LoggerFactory.getLogger(getClass)
  private var inputStream: Option[BufferedInputStream] = None

  /**
   * Open a file for reading.
   *
   * @param file The file to read
   */
  def open(file: File): Unit = {
    close()  // Close previous stream if any

    inputStream = Some(new BufferedInputStream(
      new FileInputStream(file),
      1024 * 1024  // 1MB buffer
    ))

    logger.debug(s"Opened binary file: ${file.getName}")
  }

  /**
   * Read a single 100-byte record.
   *
   * @param input The input stream (typically BufferedInputStream)
   * @return Some(record) if successful, None if EOF
   * @throws IOException if incomplete record is encountered
   */
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val record = new Array[Byte](100)
    val bytesRead = input.read(record)

    if (bytesRead == 100) {
      Some(record)
    } else if (bytesRead == -1) {
      None  // EOF
    } else {
      throw new IOException(
        s"Incomplete binary record: expected 100 bytes, got $bytesRead"
      )
    }
  }

  /**
   * Close the input stream.
   */
  def close(): Unit = {
    inputStream.foreach { stream =>
      stream.close()
      logger.debug("Closed binary reader")
    }
    inputStream = None
  }
}
```

**ìµœì í™”**:
- BufferedInputStream: I/O í˜¸ì¶œ íšŸìˆ˜ ëŒ€í­ ê°ì†Œ
- Buffer í¬ê¸° 1MB: ê²½í—˜ì ìœ¼ë¡œ ìµœì 

### 6.2 FileLayout ìƒì„¸ êµ¬í˜„

**íŒŒì¼**: `distsort/core/FileLayout.scala` (ì¼ë¶€)

```scala
/**
 * Manages file system layout for distributed sorting.
 *
 * Directory structure:
 *   Input:  /data1/input/       (read-only, never modified)
 *   Output: /home/gla/data/     (final partition.* files only)
 *   Temp:   /tmp/sort_work_W0/  (intermediate files, auto-deleted)
 *
 * This class ensures:
 *   1. Input directories are never modified (PDF requirement)
 *   2. Temp and output directories are separate
 *   3. Sufficient disk space before starting
 *   4. Automatic cleanup on completion
 */
class FileLayout(
    inputDirs: List[File],
    outputDir: File,
    tempBaseDir: File,
    workerId: String
) {

  private val logger = LoggerFactory.getLogger(getClass)
  private val workTempDir = new File(tempBaseDir, s"sort_work_$workerId")

  // Subdirectories
  private val samplesDir = new File(workTempDir, "samples")
  private val sortedChunksDir = new File(workTempDir, "sorted_chunks")
  private val partitionsDir = new File(workTempDir, "partitions")
  private val receivedDir = new File(workTempDir, "received")

  /**
   * Validate input directories (read-only access).
   *
   * CRITICAL: This method must NOT modify input directories!
   */
  def validateInputDirectories(): Unit = {
    logger.info(s"Validating ${inputDirs.length} input directories")

    inputDirs.foreach { dir =>
      require(dir.exists(), s"Input directory not found: $dir")
      require(dir.isDirectory, s"Not a directory: $dir")
      require(dir.canRead, s"Cannot read directory: $dir")

      // Count files
      val fileCount = listFilesRecursively(dir).length
      logger.info(s"  $dir: $fileCount files")
    }
  }

  /**
   * Create temporary directory structure.
   */
  def createTemporaryStructure(): Unit = {
    logger.info(s"Creating temporary structure: $workTempDir")

    List(samplesDir, sortedChunksDir, partitionsDir, receivedDir).foreach { dir =>
      if (!dir.exists()) {
        val created = dir.mkdirs()
        require(created, s"Failed to create directory: $dir")
        logger.debug(s"  Created: $dir")
      }
    }
  }

  /**
   * Ensure sufficient disk space for sorting.
   *
   * Required space:
   *   - Temp: 2x input size (intermediate files)
   *   - Output: 1x input size (final partitions)
   *
   * Safety margin: 50% extra for temp
   */
  def ensureSufficientDiskSpace(): Unit = {
    val totalInputSize = inputDirs.map(calculateDirSize).sum
    val inputSizeGB = totalInputSize / 1e9

    logger.info(f"Total input size: $inputSizeGB%.2f GB")

    // Check temp space
    val requiredTemp = totalInputSize * 2
    val availableTemp = tempBaseDir.getUsableSpace
    val requiredTempWithMargin = requiredTemp * 1.5

    require(availableTemp > requiredTempWithMargin,
      f"Insufficient temp space: available ${availableTemp / 1e9}%.2f GB, " +
      f"required ${requiredTempWithMargin / 1e9}%.2f GB"
    )

    // Check output space
    val availableOutput = outputDir.getUsableSpace
    val requiredOutput = totalInputSize * 1.2

    require(availableOutput > requiredOutput,
      f"Insufficient output space: available ${availableOutput / 1e9}%.2f GB, " +
      f"required ${requiredOutput / 1e9}%.2f GB"
    )

    logger.info("Disk space validation passed")
  }

  /**
   * Calculate total size of a directory recursively.
   */
  private def calculateDirSize(dir: File): Long = {
    if (!dir.exists()) return 0L

    val files = listFilesRecursively(dir)
    files.map(_.length()).sum
  }

  /**
   * List all files in a directory recursively.
   */
  private def listFilesRecursively(dir: File): List[File] = {
    if (!dir.exists() || !dir.isDirectory) return List.empty

    val (files, subdirs) = dir.listFiles().toList.partition(_.isFile)
    files ++ subdirs.flatMap(listFilesRecursively)
  }

  /**
   * Clean up temporary files.
   *
   * This is called:
   *   1. On worker startup (idempotent operation)
   *   2. After successful completion
   *   3. On error/crash recovery
   */
  def cleanupTemporaryFiles(): Unit = {
    if (workTempDir.exists()) {
      logger.info(s"Cleaning up temporary files: $workTempDir")
      deleteRecursively(workTempDir)
    }
  }

  /**
   * Recursively delete a directory.
   */
  private def deleteRecursively(file: File): Unit = {
    if (file.isDirectory) {
      file.listFiles().foreach(deleteRecursively)
    }
    file.delete()
  }

  // Getters for subdirectories
  def getSamplesDir: File = samplesDir
  def getSortedChunksDir: File = sortedChunksDir
  def getPartitionsDir: File = partitionsDir
  def getReceivedDir: File = receivedDir
  def getOutputDir: File = outputDir
}
```

### 6.3 ì½”ë“œ í’ˆì§ˆ ì§€í‘œ

**Week 4-5 ê¸°ì¤€**:
- ì´ ì½”ë“œ ë¼ì¸: ~800 LOC (Comments í¬í•¨ ~1200 LOC)
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: Record 5/5 passing
- ë¬¸ì„œí™”: ëª¨ë“  public method Scaladoc ì‘ì„±
- ì½”ë“œ ìŠ¤íƒ€ì¼: Scala í‘œì¤€ ê°€ì´ë“œ ì¤€ìˆ˜

### 6.4 í•´ê²°í•œ ê¸°ìˆ ì  ë¬¸ì œ

#### ë¬¸ì œ 1: Unsigned Byte ë¹„êµ

**ì¦ìƒ**: 0xFFê°€ 0x01ë³´ë‹¤ ì‘ê²Œ ì •ë ¬ë¨ (signed ë¹„êµ)

**í•´ê²°**:
```scala
// âŒ ì˜ëª»ëœ ì½”ë“œ
def compare(that: Record): Int = {
  this.key(0).compareTo(that.key(0))  // Signed comparison
}
// ê²°ê³¼: 0xFF (-1 in signed) < 0x01 (1 in signed)

// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
def compare(that: Record): Int = {
  val unsigned1 = this.key(0) & 0xFF  // 0xFF â†’ 255
  val unsigned2 = that.key(0) & 0xFF  // 0x01 â†’ 1
  unsigned1 - unsigned2                // 255 - 1 = 254 > 0
}
```

#### ë¬¸ì œ 2: Bufferì˜ clone() ëˆ„ë½

**ì¦ìƒ**: ê°™ì€ ë ˆì½”ë“œê°€ ë°˜ë³µí•´ì„œ ì½í˜ (ì°¸ì¡° ê³µìœ )

**ì›ì¸**:
```scala
// âŒ ì˜ëª»ëœ ì½”ë“œ
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  records += record  // ê°™ì€ ë°°ì—´ ì°¸ì¡°!
}
// ê²°ê³¼: recordsì— ê°™ì€ ë°°ì—´ì´ ì—¬ëŸ¬ ë²ˆ ë“¤ì–´ê°
```

**í•´ê²°**:
```scala
// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  records += record.clone()  // ë³µì‚¬ë³¸ ì €ì¥
}
```

#### ë¬¸ì œ 3: ASCII Threshold ê²°ì •

**ì‹¤í—˜**:
- Threshold 0.5: Binary íŒŒì¼ì„ ASCIIë¡œ ì˜¤íŒ
- Threshold 0.95: ASCII íŒŒì¼ì„ Binaryë¡œ ì˜¤íŒ
- **Threshold 0.9**: ìµœì  (gensort ë°ì´í„°ë¡œ ê²€ì¦)

**ìµœì¢… ê²°ì •**: 0.9 (90%)

#### ë¬¸ì œ 4: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ëŒ€ë¹„

**ì „ëµ**:
1. ì‘ì—… ì‹œì‘ ì „ ê³µê°„ ê²€ì¦
2. í•„ìš” ê³µê°„ = inputSize * 2 (ì„ì‹œ íŒŒì¼ ê³ ë ¤)
3. ì•ˆì „ ì—¬ìœ  = 50% ì¶”ê°€
4. ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

### 6.5 êµ¬í˜„ í’ˆì§ˆ ê´€ë¦¬

#### ë²„í¼ í¬ê¸° ê²°ì • ê·¼ê±°

**BinaryRecordReader: 1MB ë²„í¼**
- ëª©ì : I/O í˜¸ì¶œ íšŸìˆ˜ ìµœì†Œí™”
- ê·¼ê±°: ì¼ë°˜ì ì¸ íŒŒì¼ ì‹œìŠ¤í…œ ë¸”ë¡ í¬ê¸°ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê· í˜•

**RecordWriter: 4MB ë²„í¼**
- ëª©ì : ëŒ€ìš©ëŸ‰ ì¶œë ¥ ì‹œ I/O íš¨ìœ¨ í–¥ìƒ
- ê·¼ê±°: ì¶œë ¥ íŒŒì¼ì´ ë” í¬ë¯€ë¡œ ë²„í¼ ì¦ê°€, 8MB ì´ìƒì€ ë©”ëª¨ë¦¬ ë‚­ë¹„ ìš°ë ¤

---

## 7. í–¥í›„ ê³„íš (Week 6-8)

### 7.1 Week 6: Algorithms êµ¬í˜„

#### ëª©í‘œ

**ExternalSorter**, **Partitioner**, **KWayMerger** êµ¬í˜„

#### ì„¸ë¶€ ê³„íš

##### ExternalSorter

```scala
class ExternalSorter(
    chunkSize: Long = 100 * 1024 * 1024,  // 100MB
    numThreads: Int = Runtime.getRuntime.availableProcessors()
) {

  /**
   * Sort input file using 2-pass external sort.
   *
   * Phase 1: Split into chunks, sort each in parallel
   * Phase 2: K-way merge all sorted chunks
   */
  def sort(inputFile: File, outputFile: File): Unit = {
    // Phase 1: Chunk sort (parallel)
    val sortedChunks = sortChunksInParallel(inputFile)

    // Phase 2: K-way merge
    kWayMerge(sortedChunks, outputFile)

    // Cleanup
    sortedChunks.foreach(_.delete())
  }

  private def sortChunksInParallel(inputFile: File): List[File] = {
    // TDD: Test first!
    ???
  }
}
```

##### Partitioner

```scala
class Partitioner(
    boundaries: Array[Array[Byte]],
    numPartitions: Int
) {

  /**
   * Partition sorted chunks by range.
   *
   * Uses binary search to find partition ID for each record.
   */
  def partition(sortedChunk: File, outputDir: File): Map[Int, File] = {
    // TDD: Test first!
    ???
  }

  private def findPartitionBinarySearch(key: Array[Byte]): Int = {
    // TDD: Test first!
    ???
  }
}
```

##### KWayMerger

```scala
class KWayMerger {

  /**
   * Merge K sorted files into one using priority queue.
   */
  def merge(inputFiles: List[File], outputFile: File): Unit = {
    // Min-heap
    val heap = mutable.PriorityQueue[RecordWithSource]()(
      Ordering.by[RecordWithSource, Array[Byte]](_.record.key).reverse
    )

    // TDD: Test first!
    ???
  }
}
```

#### ê²€ì¦ ê¸°ì¤€

```bash
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
gensort -b0 1000000 test_input.dat  # 100MB

# ë¡œì»¬ ì •ë ¬ ì‹¤í–‰ (Worker ì—†ì´)
sbt "runMain distsort.LocalSortTest test_input.dat test_output.dat"

# ê²€ì¦
valsort test_output.dat
# ì˜ˆìƒ: SUCCESS
```

### 7.2 Week 7: Master/Worker êµ¬í˜„

#### Master Node

```
ì£¼ìš” ê¸°ëŠ¥:
  â”œâ”€ Worker ë“±ë¡ ê´€ë¦¬ (CountDownLatch)
  â”œâ”€ ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°
  â”œâ”€ shuffleMap ìƒì„± ë° ë¸Œë¡œë“œìºìŠ¤íŠ¸
  â”œâ”€ Phase ë™ê¸°í™” (PhaseTracker)
  â””â”€ ìµœì¢… ê²°ê³¼ ì¶œë ¥ (stdout)

êµ¬í˜„ ìˆœì„œ:
  1. ğŸ”´ RED: MasterSpec ì‘ì„±
  2. ğŸŸ¢ GREEN: ìµœì†Œ êµ¬í˜„
  3. ğŸ”µ REFACTOR: ë¦¬íŒ©í† ë§
```

#### Worker Node

```
ì£¼ìš” ê¸°ëŠ¥:
  â”œâ”€ Masterì— ë“±ë¡
  â”œâ”€ ìƒ˜í”Œ ì¶”ì¶œ ë° ì „ì†¡
  â”œâ”€ ì •ë ¬ ë° íŒŒí‹°ì…”ë‹
  â”œâ”€ Shuffle (ì†¡ì‹ /ìˆ˜ì‹ )
  â”œâ”€ Merge
  â””â”€ ì™„ë£Œ ë³´ê³ 

ìƒíƒœ ë¨¸ì‹ :
  INITIALIZING â†’ SAMPLING â†’ SORTING â†’ SHUFFLING â†’ MERGING â†’ COMPLETED
```

### 7.3 Week 8: í†µí•© í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

#### í†µí•© í…ŒìŠ¤íŠ¸

```bash
# Scenario 1: 3 workers, 3 partitions (Strategy A)
$ ./test_3w3p.sh

# Scenario 2: 3 workers, 9 partitions (Strategy B)
$ ./test_3w9p.sh

# Scenario 3: Worker crash during shuffle
$ ./test_fault_tolerance.sh
```

#### ìµœì í™”

- ë©€í‹°ìŠ¤ë ˆë“œ í™œìš©ë„ í–¥ìƒ
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ìµœì í™”
- ë””ìŠ¤í¬ I/O ë³‘ëª© ì œê±°

---

## 8. Q&A ì¤€ë¹„

### 8.1 ì˜ˆìƒ ì§ˆë¬¸

#### Q1: "TDDë¥¼ ì„ íƒí•œ ì´ìœ ëŠ”?"

**ë‹µë³€**:
- ë¶„ì‚° ì‹œìŠ¤í…œì€ ìƒíƒœ ì „ì´ì™€ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ë³µì¡
- í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ë©´ ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ ì •ì˜í•  ìˆ˜ ìˆìŒ
- ë¦¬íŒ©í† ë§ ì‹œ ì•ˆì „ì„± ë³´ì¥
- ì˜ˆ: Record í´ë˜ìŠ¤ì˜ unsigned ë¹„êµ ë²„ê·¸ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ë¨¼ì € ë°œê²¬

#### Q2: "Checkpoint ê¸°ë°˜ ë³µêµ¬ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜?"

**ë‹µë³€**:
- **ì €ì¥**: ê° Phase ì™„ë£Œ ì‹œ WorkerStateë¥¼ JSONìœ¼ë¡œ ìë™ ì €ì¥ (`/tmp/distsort/checkpoints/`)
- **ë³µêµ¬**: Worker ì¬ì‹œì‘ ì‹œ ìµœì‹  checkpoint ë¡œë“œ â†’ ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ
- **ì˜ˆì‹œ**: Shuffle ì¤‘ crash â†’ PHASE_SORTING checkpoint ë¡œë“œ â†’ Shuffleë§Œ ì¬ì‹œë„ (Sampling/Sort ìŠ¤í‚µ)
- **ì¥ì **:
  - ë¹ ë¥¸ ë³µêµ¬ (ì „ì²´ ì¬ì‹œì‘ ëŒ€ë¹„ ì‹œê°„ ì ˆì•½)
  - ì •í™•ì„± ë³´ì¥ (Phaseë³„ ì™„ë£Œ ì‹œì  checkpoint)
  - Graceful Shutdown í†µí•© (30ì´ˆ grace period)
- **êµ¬í˜„**: CheckpointManager + JSON ì§ë ¬í™” + ìµœê·¼ 3ê°œ ìœ ì§€
- **PDF ìš”êµ¬ì‚¬í•­**: "produce correct results" â† ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ëª¨ë‘ ì¶©ì¡±

#### Q3: "Nâ†’M ì „ëµì˜ ì¥ì ì€?"

**ë‹µë³€**:
- ë¡œë“œ ë°¸ëŸ°ì‹± ê°œì„ : íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€ë¡œ í¬ê¸° ë¶ˆê· í˜• ì™„í™”
- ë©€í‹°ì½”ì–´ í™œìš©: ê° Workerê°€ ì—¬ëŸ¬ íŒŒí‹°ì…˜ ë³‘ë ¬ merge
- PDF ìš”êµ¬ì‚¬í•­ ì¶©ì¡±: "numPartitionsëŠ” ì›Œì»¤ ìˆ˜ì™€ ë™ì¼ ë˜ëŠ” ë°°ìˆ˜"
- ì˜ˆ: 3 workers, 9 partitions â†’ ê° Workerê°€ 3ê°œ íŒŒí‹°ì…˜ ë³‘ë ¬ merge

#### Q4: "ASCII/Binary ìë™ ê°ì§€ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜?"

**ë‹µë³€**:
- íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ì½ê¸°
- ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
- ë¹„ìœ¨ > 90% â†’ ASCII, ê·¸ ì™¸ â†’ Binary
- ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì  ê°ì§€ â†’ í˜¼í•© ì…ë ¥ ì§€ì›
- PDF ìš”êµ¬ì‚¬í•­: "without requiring an option"

#### Q5: "í˜„ì¬ ì§„í–‰ë¥ ì€?"

**ë‹µë³€**:
- Week 1-2: ì„¤ê³„ ë‹¨ê³„ 100% ì™„ë£Œ
- Week 3: Record í´ë˜ìŠ¤ 100% ì™„ë£Œ
- Week 4-5: Core I/O Components 100% ì™„ë£Œ
  - RecordReader, InputFormatDetector, FileLayout, RecordWriter
- Week 6: Algorithms êµ¬í˜„ ì˜ˆì •
  - ExternalSorter, Partitioner, KWayMerger
- Week 7-8: Master/Worker í†µí•© ì˜ˆì •

ì „ì²´ ì§„í–‰ë¥ : ì•½ 40% (ì„¤ê³„ + ê¸°ì´ˆ ì»´í¬ë„ŒíŠ¸ ì™„ë£Œ)

#### Q6: "ê°€ì¥ ì–´ë ¤ì› ë˜ ë¶€ë¶„ì€?"

**ë‹µë³€**:
1. **Unsigned ë¹„êµ**: Scalaì˜ Byteê°€ signed â†’ `& 0xFF`ë¡œ ë³€í™˜ í•„ìš”
2. **íŒŒí‹°ì…˜ ì „ëµ**: Nâ†’N vs Nâ†’M ì„ íƒ, shuffleMap ë¡œì§ ì´í•´
3. **Fault Tolerance**: ì–´ë””ê¹Œì§€ ë³µêµ¬ vs ì¬ì‹œì‘? â†’ PDF í•´ì„ + êµ¬í˜„ ë³µì¡ë„ ê³ ë ¤

#### Q7: "í…ŒìŠ¤íŠ¸ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜?"

**ë‹µë³€**:
- **Unit Tests**: ê°œë³„ í•¨ìˆ˜/í´ë˜ìŠ¤ ê²€ì¦ (ScalaTest)
  - ì˜ˆ: RecordSpec, RecordReaderSpec
- **Integration Tests**: ì»´í¬ë„ŒíŠ¸ ê°„ í†µì‹  ê²€ì¦
  - ì˜ˆ: gRPC í†µì‹ , Shuffle ì¬ì‹œë„
- **E2E Tests**: ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦
  - ì˜ˆ: 3 workersë¡œ 100MB ë°ì´í„° ì •ë ¬, valsortë¡œ ê²€ì¦
- **Fault Tolerance Tests**: Worker crash ì‹œë‚˜ë¦¬ì˜¤
  - ì˜ˆ: Shuffle ì¤‘ kill -9, ì¬ì‹œì‘ í›„ ë³µêµ¬

### 8.2 ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ (Week 6 ì´í›„)

```bash
# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
$ gensort -b0 10000000 /data1/input/test.dat  # 1GB

# 2. Master ì‹œì‘ (3 workers, 9 partitions)
$ sbt "runMain distsort.Main master 3 9"
[INFO] Master started on port 30000
[INFO] Waiting for 3 workers to register...

# 3. Worker ì‹œì‘ (3ëŒ€)
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1/input -O /data1/output"
[INFO] Worker registered with ID W0

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data2/input -O /data2/output"
[INFO] Worker registered with ID W1

$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data3/input -O /data3/output"
[INFO] Worker registered with ID W2

# 4. Master ì¶œë ¥
192.168.1.100:30000
worker0, worker1, worker2

# 5. ê²°ê³¼ ê²€ì¦
$ valsort /data1/output/partition.* /data2/output/partition.* /data3/output/partition.*
SUCCESS
```

---

## 9. ì°¸ê³  ë¬¸í—Œ

### 9.1 í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

- Knuth, Donald E. *The Art of Computer Programming, Volume 3: Sorting and Searching*. Addison-Wesley, 1998.
  - External Sorting ì•Œê³ ë¦¬ì¦˜ (5.4ì ˆ)

- TeraSort: A Sample Hadoop Application
  - Sampling for Partitioning ê¸°ë²•

### 9.2 ì‹œìŠ¤í…œ ì„¤ê³„

- Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: Simplified Data Processing on Large Clusters." *OSDI* 2004.
  - Master-Worker ì•„í‚¤í…ì²˜, Fault Tolerance ì „ëµ

- Zaharia, Matei, et al. "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing." *NSDI* 2012.
  - Lineage-based Fault Recovery

### 9.3 êµ¬í˜„ ì°¸ê³ 

- gRPC ê³µì‹ ë¬¸ì„œ: https://grpc.io/docs/languages/scala/
- Protocol Buffers: https://protobuf.dev/
- **gensort/valsort**: http://www.ordinal.com/gensort.html
  - ì •ë ¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ë° ê²€ì¦ ë„êµ¬

### 9.4 í”„ë¡œì íŠ¸ ë¬¸ì„œ

- `plan/2025-10-24_plan_ver3.md`: ì „ì²´ ì‹œìŠ¤í…œ ì„¤ê³„
- `docs/0-implementation-decisions.md`: í•µì‹¬ êµ¬í˜„ ê²°ì • ì‚¬í•­
- `docs/1-phase-coordination.md`: Phase ë™ê¸°í™” í”„ë¡œí† ì½œ
- `docs/4-error-recovery.md`: ì¥ì•  ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- `docs/6-parallelization.md`: ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬
- `docs/7-testing-strategy.md`: TDD ê°€ì´ë“œ

---

**ë°œí‘œ ì¤€ë¹„ ì™„ë£Œ**
**Team Silver** - 2025-11-16

---

## Appendix: ì¶”ê°€ ìë£Œ

### A.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project_2025/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”‚   â””â”€â”€ build.properties
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ scala/distsort/
â”‚       â”‚   â”œâ”€â”€ Main.scala
â”‚       â”‚   â”œâ”€â”€ core/
â”‚       â”‚   â”‚   â”œâ”€â”€ Record.scala               âœ… Week 4
â”‚       â”‚   â”‚   â”œâ”€â”€ RecordReader.scala         âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ BinaryRecordReader.scala   âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ AsciiRecordReader.scala    âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ InputFormatDetector.scala  âœ… Week 5
â”‚       â”‚   â”‚   â”œâ”€â”€ FileLayout.scala           âœ… Week 5
â”‚       â”‚   â”‚   â””â”€â”€ RecordWriter.scala         âœ… Week 5
â”‚       â”‚   â”œâ”€â”€ algorithms/
â”‚       â”‚   â”‚   â”œâ”€â”€ ExternalSorter.scala       ğŸ“‹ Week 6
â”‚       â”‚   â”‚   â”œâ”€â”€ Partitioner.scala          ğŸ“‹ Week 6
â”‚       â”‚   â”‚   â””â”€â”€ KWayMerger.scala           ğŸ“‹ Week 6
â”‚       â”‚   â”œâ”€â”€ master/
â”‚       â”‚   â”‚   â”œâ”€â”€ MasterServer.scala         ğŸ“‹ Week 7
â”‚       â”‚   â”‚   â””â”€â”€ PhaseTracker.scala         ğŸ“‹ Week 7
â”‚       â”‚   â””â”€â”€ worker/
â”‚       â”‚       â”œâ”€â”€ WorkerNode.scala           ğŸ“‹ Week 7
â”‚       â”‚       â””â”€â”€ WorkerStateMachine.scala   ğŸ“‹ Week 7
â”‚       â””â”€â”€ protobuf/
â”‚           â””â”€â”€ distsort.proto                 ğŸ“‹ Week 7
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 0-implementation-decisions.md
â”‚   â”œâ”€â”€ 1-phase-coordination.md
â”‚   â”œâ”€â”€ 4-error-recovery.md
â”‚   â”œâ”€â”€ 6-parallelization.md
â”‚   â””â”€â”€ 7-testing-strategy.md
â””â”€â”€ plan/
    â””â”€â”€ 2025-10-24_plan_ver3.md
```

### A.2 ê°œë°œ ë„êµ¬

| ë„êµ¬ | ë²„ì „ | ìš©ë„ |
|------|------|------|
| Scala | 2.13.12 | êµ¬í˜„ ì–¸ì–´ |
| SBT | 1.9.7 | ë¹Œë“œ ë„êµ¬ |
| gRPC | 1.59.1 | RPC í”„ë ˆì„ì›Œí¬ |
| ScalaTest | 3.2.17 | í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ |
| gensort | 1.5 | í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± |
| valsort | 1.5 | ì •ë ¬ ê²°ê³¼ ê²€ì¦ |

### A.3 Git Commit ì „ëµ

```
Commit Message í˜•ì‹:
  <type>: <subject>

  <body>

  Co-Authored-By: ê¶Œë™ì—° <yeon903@github>
  Co-Authored-By: ë°•ë²”ìˆœ <pbs7818@github>
  Co-Authored-By: ì„ì§€í›ˆ <Jih00nLim@github>

Type:
  feat: ìƒˆë¡œìš´ ê¸°ëŠ¥
  fix: ë²„ê·¸ ìˆ˜ì •
  docs: ë¬¸ì„œ ì—…ë°ì´íŠ¸
  test: í…ŒìŠ¤íŠ¸ ì¶”ê°€/ìˆ˜ì •
  refactor: ë¦¬íŒ©í† ë§

ì˜ˆì‹œ:
  feat: Implement BinaryRecordReader with 1MB buffering

  - Add BinaryRecordReader class
  - Use BufferedInputStream for efficient I/O
  - Handle EOF and incomplete records
  - Add comprehensive error messages

  Co-Authored-By: ...
```
