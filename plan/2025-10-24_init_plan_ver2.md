# Distributed Sorting System - ìƒì„¸ ì„¤ê³„ ë¬¸ì„œ (v2)

**ì‘ì„±ì¼**: 2025-10-24
**ë²„ì „**: 2.0 (ë³´ì™„ ì‚¬í•­ ì¶”ê°€)
**í”„ë¡œì íŠ¸**: Fault-Tolerant Distributed Key/Value Sorting System

---

## ğŸ“‹ Version 2 ì£¼ìš” ë³€ê²½ì‚¬í•­

ì´ ë¬¸ì„œëŠ” ì´ˆê¸° ê³„íš(v1.0)ì„ ê¸°ë°˜ìœ¼ë¡œ PDF í”„ë¡œì íŠ¸ ëª…ì„¸ì„œ ìš”êµ¬ì‚¬í•­ì„ ì¶”ê°€ ë°˜ì˜í•œ ë²„ì „ì…ë‹ˆë‹¤.

### ğŸ”´ í•„ìˆ˜ ì¶”ê°€ ì‚¬í•­
1. **ASCII/Binary ì…ë ¥ í˜•ì‹ ì²˜ë¦¬** - RecordReader ì¶”ìƒí™” ë° í¬ë§·ë³„ êµ¬í˜„
2. **ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤ ê°œì„ ** - ì—¬ëŸ¬ ì…ë ¥ ë””ë ‰í† ë¦¬, ì¶”ê°€ ì˜µì…˜ í”Œë˜ê·¸ ì§€ì›
3. **Master ì¶œë ¥ í˜•ì‹ í‘œì¤€í™”** - stdout vs stderr ë¶„ë¦¬
4. **ì¤‘ê°„ íŒŒì¼ ì €ì¥ ìœ„ì¹˜ ì˜µì…˜** - `--temp` í”Œë˜ê·¸ ì¶”ê°€

### ğŸŸ¡ ì¤‘ìš” ê°œì„  ì‚¬í•­
5. **ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§** - ì§€ìˆ˜ ë°±ì˜¤í”„, Circuit breaker
6. **íŒŒí‹°ì…˜ íŒŒì¼ ë„¤ì´ë° ëª…í™•í™”** - `partition.N` í˜•ì‹ ë³´ì¥
7. **ìƒ˜í”Œë§ ë¹„ìœ¨ ë™ì  ì¡°ì •** - ì…ë ¥ í¬ê¸°ì— ë”°ë¥¸ ì ì‘ì  ìƒ˜í”Œë§
8. **ë””ìŠ¤í¬ ê³µê°„ ì‚¬ì „ í™•ì¸** - ì‘ì—… ì‹œì‘ ì „ ê²€ì¦

---

## ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [ë°ì´í„° êµ¬ì¡° ë° í¬ë§·](#3-ë°ì´í„°-êµ¬ì¡°-ë°-í¬ë§·)
4. [í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ìƒì„¸](#4-í•µì‹¬-ì•Œê³ ë¦¬ì¦˜-ìƒì„¸)
5. [ë„¤íŠ¸ì›Œí¬ í†µì‹  ì„¤ê³„](#5-ë„¤íŠ¸ì›Œí¬-í†µì‹ -ì„¤ê³„)
6. [ì¥ì•  í—ˆìš©ì„± ë©”ì»¤ë‹ˆì¦˜](#6-ì¥ì• -í—ˆìš©ì„±-ë©”ì»¤ë‹ˆì¦˜)
7. [ë©€í‹°ìŠ¤ë ˆë“œ ë° ë³‘ë ¬ ì²˜ë¦¬](#7-ë©€í‹°ìŠ¤ë ˆë“œ-ë°-ë³‘ë ¬-ì²˜ë¦¬)
8. [êµ¬í˜„ ì„¸ë¶€ì‚¬í•­](#8-êµ¬í˜„-ì„¸ë¶€ì‚¬í•­)
9. [ì„±ëŠ¥ ìµœì í™” ì „ëµ](#9-ì„±ëŠ¥-ìµœì í™”-ì „ëµ)
10. [í…ŒìŠ¤íŠ¸ ì „ëµ](#10-í…ŒìŠ¤íŠ¸-ì „ëµ)
11. **[ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤ ìƒì„¸](#11-ëª…ë ¹í–‰-ì¸í„°í˜ì´ìŠ¤-ìƒì„¸)** â­ NEW
12. **[ì…ë ¥ í˜•ì‹ ì²˜ë¦¬](#12-ì…ë ¥-í˜•ì‹-ì²˜ë¦¬)** â­ NEW
13. [ê°œë°œ ë§ˆì¼ìŠ¤í†¤](#13-ê°œë°œ-ë§ˆì¼ìŠ¤í†¤)
14. [ì°¸ê³  ìë£Œ](#14-ì°¸ê³ -ìë£Œ)
15. [ì˜ˆìƒ ì´ìŠˆ ë° í•´ê²°ì±…](#15-ì˜ˆìƒ-ì´ìŠˆ-ë°-í•´ê²°ì±…)
16. [ê²°ë¡ ](#16-ê²°ë¡ )

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ
ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚° ì €ì¥ëœ ëŒ€ìš©ëŸ‰ key/value ë ˆì½”ë“œë¥¼ ì •ë ¬í•˜ëŠ” ì¥ì•  í—ˆìš©ì„± ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬í˜„

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­
- **ì…ë ¥**: ì—¬ëŸ¬ Worker ë…¸ë“œì— ë¶„ì‚°ëœ 32MB ë¸”ë¡ ë‹¨ìœ„ì˜ ë¯¸ì •ë ¬ ë°ì´í„°
- **ì¶œë ¥**: ì „ì—­ì ìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„° (ê° Workerì— ì—°ì†ëœ íŒŒí‹°ì…˜ ì €ì¥)
- **ì¥ì•  í—ˆìš©**: Worker í”„ë¡œì„¸ìŠ¤ crash í›„ ì¬ì‹œì‘ ì‹œ ì˜¬ë°”ë¥¸ ê²°ê³¼ ìƒì„±
- **í™•ì¥ì„±**: ë©€í‹°ì½”ì–´ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
- **í˜•ì‹ ì§€ì›**: ASCII ë° Binary ì…ë ¥ ëª¨ë‘ ì§€ì› â­ NEW

### 1.3 ì œì•½ì‚¬í•­
- ì…ë ¥ ë””ë ‰í† ë¦¬ ìˆ˜ì • ê¸ˆì§€ (ì½ê¸° ì „ìš©)
- ì¶œë ¥ ë””ë ‰í† ë¦¬ì—ëŠ” ìµœì¢… ê²°ê³¼ë§Œ ì €ì¥
- í¬íŠ¸ í•˜ë“œì½”ë”© ê¸ˆì§€
- Akka ì‚¬ìš© ê¸ˆì§€
- ASCII/Binary í˜•ì‹ì€ ëª…ë ¹í–‰ í”Œë˜ê·¸ë¡œ êµ¬ë¶„

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Master Node                        â”‚
â”‚  - Worker ë“±ë¡ ê´€ë¦¬                                      â”‚
â”‚  - ìƒ˜í”Œ ìˆ˜ì§‘ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°                          â”‚
â”‚  - ì „ì—­ ì¡°ì • (Coordination)                             â”‚
â”‚  - ìµœì¢… Worker ìˆœì„œ ì¶œë ¥ (stdout)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚               â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Worker 1 â”‚   â”‚ Worker 2 â”‚  â”‚ Worker 3 â”‚   â”‚ Worker N â”‚
â”‚          â”‚   â”‚          â”‚  â”‚          â”‚   â”‚          â”‚
â”‚ Input:   â”‚   â”‚ Input:   â”‚  â”‚ Input:   â”‚   â”‚ Input:   â”‚
â”‚ 50GB     â”‚   â”‚ 50GB     â”‚  â”‚ 50GB     â”‚   â”‚ 50GB     â”‚
â”‚          â”‚   â”‚          â”‚  â”‚          â”‚   â”‚          â”‚
â”‚ Output:  â”‚   â”‚ Output:  â”‚  â”‚ Output:  â”‚   â”‚ Output:  â”‚
â”‚ P0,P1,P2 â”‚   â”‚ P3,P4,P5 â”‚  â”‚ P6,P7,P8 â”‚   â”‚ Px,Py,Pz â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Worker-to-Worker Shuffle
```

### 2.2 ì‹¤í–‰ íë¦„

```
Phase 0: ì´ˆê¸°í™”
  - Master ì‹œì‘, Workerë“¤ ë“±ë¡ ëŒ€ê¸°
  - ê° Worker ì‹œì‘, Masterì— ì—°ê²° ë° ë“±ë¡
  - ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦ â­ NEW

Phase 1: Sampling (ìƒ˜í”Œë§)
  - ê° Workerê°€ ì…ë ¥ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
  - ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚° â­ NEW
  - Masterì—ê²Œ ìƒ˜í”Œ ì „ì†¡
  - Masterê°€ ì „ì²´ ìƒ˜í”Œ ì •ë ¬ ë° íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚°
  - ëª¨ë“  Workerì—ê²Œ íŒŒí‹°ì…˜ ê²½ê³„ ë¸Œë¡œë“œìºìŠ¤íŠ¸

Phase 2: Sort & Partition (ì •ë ¬ ë° íŒŒí‹°ì…”ë‹)
  - ê° Workerê°€ ì…ë ¥ ë¸”ë¡ ì½ê¸° (ASCII/Binary ìë™ ì²˜ë¦¬) â­ NEW
  - ë©”ëª¨ë¦¬ ë‚´ ì •ë ¬
  - íŒŒí‹°ì…˜ ê²½ê³„ì— ë”°ë¼ ë°ì´í„° ë¶„í• 
  - íŒŒí‹°ì…˜ë³„ ì„ì‹œ íŒŒì¼ ìƒì„± (--temp ë””ë ‰í† ë¦¬ ì‚¬ìš©) â­ NEW

Phase 3: Shuffle (ë°ì´í„° ì¬ë¶„ë°°)
  - Worker ê°„ ë„¤íŠ¸ì›Œí¬ í†µì‹  (ì¬ì‹œë„ ë¡œì§ í¬í•¨) â­ NEW
  - ê° íŒŒí‹°ì…˜ ë°ì´í„°ë¥¼ í•´ë‹¹ Workerë¡œ ì „ì†¡
  - íƒ€ì„ì•„ì›ƒ ë° ë°±ì˜¤í”„ ì²˜ë¦¬ â­ NEW

Phase 4: Merge (ë³‘í•©)
  - ê° Workerê°€ ë°›ì€ ì •ë ¬ëœ íŒŒí‹°ì…˜ë“¤ì„ K-way merge
  - ìµœì¢… ì •ë ¬ëœ íŒŒì¼ ìƒì„± (partition.0, partition.1, ...) â­ NEW
  - Atomic write ë³´ì¥

Phase 5: ì™„ë£Œ
  - ê° Workerê°€ Masterì—ê²Œ ì™„ë£Œ ë³´ê³ 
  - Masterê°€ ì „ì²´ ì‘ì—… ì™„ë£Œ í™•ì¸
  - Masterê°€ ì •ë ¬ëœ Worker ì£¼ì†Œ ì¶œë ¥ (stdout) â­ NEW
  - ì„ì‹œ íŒŒì¼ ì •ë¦¬
```

---

## 3. ë°ì´í„° êµ¬ì¡° ë° í¬ë§·

### 3.1 ë ˆì½”ë“œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Key (10B)    â”‚ Value (90B)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  0            10                                          100
```

**íŠ¹ì§•:**
- ê³ ì • ê¸¸ì´: 100 ë°”ì´íŠ¸
- Keyë§Œ ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
- ValueëŠ” ì •ë ¬ê³¼ ë¬´ê´€í•˜ê²Œ Keyì™€ í•¨ê»˜ ì´ë™

### 3.2 Key ë¹„êµ ê·œì¹™

```scala
def compareKeys(keyA: Array[Byte], keyB: Array[Byte]): Int = {
  for (i <- 0 until 10) {
    if (keyA(i) != keyB(i)) {
      // unsigned byte ë¹„êµ
      return (keyA(i) & 0xFF).compareTo(keyB(i) & 0xFF)
    }
  }
  0 // ë™ì¼
}
```

**ì£¼ì˜ì‚¬í•­:**
- ë°”ì´íŠ¸ ë‹¨ìœ„ ì‚¬ì „ì‹ ë¹„êµ
- Unsigned ë¹„êµ í•„ìˆ˜ (Java/Scalaì˜ byteëŠ” signed)
- ì²« ë²ˆì§¸ë¡œ ë‹¤ë¥¸ ë°”ì´íŠ¸ì—ì„œ ëŒ€ì†Œ íŒë‹¨

### 3.3 ì…ë ¥ ë°ì´í„° êµ¬ì¡°

```
Input Directories:
/data1/input/
  â”œâ”€â”€ block_0001  (32MB)
  â”œâ”€â”€ block_0002  (32MB)
  â””â”€â”€ ...

/data2/input/
  â”œâ”€â”€ block_0001  (32MB)
  â””â”€â”€ ...

ì´ ì…ë ¥: 50GB x N workers
```

### 3.4 ì¶œë ¥ ë°ì´í„° êµ¬ì¡° â­ UPDATED

```
Output Directory (ê° Worker):
/output/
  â”œâ”€â”€ partition.0
  â”œâ”€â”€ partition.1
  â”œâ”€â”€ partition.2
  â””â”€â”€ ...

íŒŒì¼ ëª…ëª… ê·œì¹™:
- ì •í™•íˆ "partition.N" í˜•ì‹ (Nì€ 0ë¶€í„° ì‹œì‘)
- í™•ì¥ì ì—†ìŒ
- ê° WorkerëŠ” ì—°ì†ëœ íŒŒí‹°ì…˜ ë²ˆí˜¸ ì €ì¥
```

### 3.5 ì¤‘ê°„ ë°ì´í„° êµ¬ì¡° â­ UPDATED

```
Temporary Directory (ì‘ì—… ì¤‘):
${TEMP_DIR}/sort_work_${WORKER_ID}/
  â”œâ”€â”€ samples/
  â”‚   â””â”€â”€ sample.dat  (ë™ì  í¬ê¸°)
  â”‚
  â”œâ”€â”€ sorted_chunks/
  â”‚   â”œâ”€â”€ chunk_0000.sorted  (100MB)
  â”‚   â”œâ”€â”€ chunk_0001.sorted  (100MB)
  â”‚   â””â”€â”€ ...
  â”‚
  â”œâ”€â”€ partitions/
  â”‚   â”œâ”€â”€ partition_0/
  â”‚   â”‚   â”œâ”€â”€ from_chunk_0000.dat
  â”‚   â”‚   â”œâ”€â”€ from_chunk_0001.dat
  â”‚   â”‚   â””â”€â”€ ...
  â”‚   â”œâ”€â”€ partition_1/
  â”‚   â”‚   â””â”€â”€ ...
  â”‚   â””â”€â”€ ...
  â”‚
  â””â”€â”€ received/
      â”œâ”€â”€ partition_0_from_worker_1.dat
      â”œâ”€â”€ partition_0_from_worker_2.dat
      â””â”€â”€ ...

TEMP_DIR ì§€ì •:
- --temp í”Œë˜ê·¸ë¡œ ì§€ì • (ê¸°ë³¸ê°’: /tmp)
- ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ ì‚¬ì „ í™•ì¸ í•„ìš”
- ì‘ì—… ì™„ë£Œ í›„ ìë™ ì‚­ì œ
```

---

## 4. í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ìƒì„¸

### 4.1 Phase 1: Sampling Algorithm â­ UPDATED

#### 4.1.1 ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚° â­ NEW

```scala
def calculateSampleRate(totalInputSize: Long, numWorkers: Int): Double = {
  // Workerë‹¹ ëª©í‘œ ìƒ˜í”Œ ê°œìˆ˜ (10,000ê°œ key)
  val targetSamplesPerWorker = 10000
  val totalTargetSamples = targetSamplesPerWorker * numWorkers

  // ì „ì²´ ë ˆì½”ë“œ ìˆ˜ ì¶”ì •
  val estimatedTotalRecords = totalInputSize / 100

  // ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚°
  val calculatedRate = totalTargetSamples.toDouble / estimatedTotalRecords

  // ìµœì†Œ/ìµœëŒ€ ì œí•œ
  val minRate = 0.0001  // 0.01%
  val maxRate = 0.01    // 1%

  math.max(minRate, math.min(maxRate, calculatedRate))
}
```

**ìƒ˜í”Œë§ ì „ëµ:**
- ì…ë ¥ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ë¹„ìœ¨ ì¡°ì •
- ìµœì†Œ ìƒ˜í”Œ: Workerë‹¹ 1,000ê°œ key
- ìµœëŒ€ ìƒ˜í”Œ: Workerë‹¹ 100,000ê°œ key
- ì´ ìƒ˜í”Œ í¬ê¸°: ì¼ë°˜ì ìœ¼ë¡œ 1-10MB

#### 4.1.2 ìƒ˜í”Œ ì¶”ì¶œ (ê° Worker)

```scala
def extractSample(inputDirs: List[String],
                  format: DataFormat,
                  totalInputSize: Long,
                  numWorkers: Int): Array[Array[Byte]] = {

  val sampleRate = calculateSampleRate(totalInputSize, numWorkers)
  val random = new Random(42)  // ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ê³ ì • seed
  val samples = mutable.ArrayBuffer[Array[Byte]]()

  val reader = createRecordReader(format)  // â­ NEW: í˜•ì‹ë³„ ë¦¬ë”

  for (file <- getAllInputFiles(inputDirs)) {
    val input = new BufferedInputStream(new FileInputStream(file))

    var record = reader.readRecord(input)
    while (record.isDefined) {
      if (random.nextDouble() < sampleRate) {
        val key = record.get.take(10)
        samples += key
      }
      record = reader.readRecord(input)
    }
    input.close()
  }

  samples.toArray
}
```

#### 4.1.3 íŒŒí‹°ì…˜ ê²½ê³„ ê³„ì‚° (Master)

```scala
def calculatePartitionBoundaries(
    allSamples: Array[Array[Byte]],
    numPartitions: Int): Array[Array[Byte]] = {

  // 1. ëª¨ë“  ìƒ˜í”Œ ì •ë ¬
  val sortedSamples = allSamples.sortWith(compareKeys(_, _) < 0)

  // 2. ê· ë“± ë¶„í• ì„ ìœ„í•œ ê²½ê³„ ì„ íƒ
  val boundaries = new Array[Array[Byte]](numPartitions - 1)
  val step = sortedSamples.length / numPartitions

  for (i <- 0 until numPartitions - 1) {
    boundaries(i) = sortedSamples((i + 1) * step)
  }

  boundaries
}
```

### 4.2 Phase 2: Sort & Partition Algorithm

#### 4.2.1 ì²­í¬ ë‹¨ìœ„ ì •ë ¬ â­ UPDATED

```scala
def sortChunk(inputFile: File,
              format: DataFormat,
              chunkSize: Int = 100 * 1024 * 1024): File = {
  val records = new Array[Array[Byte]](chunkSize / 100)
  val reader = createRecordReader(format)  // â­ NEW
  val input = new BufferedInputStream(new FileInputStream(inputFile))

  var count = 0
  var record = reader.readRecord(input)

  while (count < records.length && record.isDefined) {
    records(count) = record.get
    count += 1
    record = reader.readRecord(input)
  }
  input.close()

  // ì •ë ¬ (key ê¸°ì¤€)
  val sortedRecords = records.take(count).sortWith { (a, b) =>
    compareKeys(a.take(10), b.take(10)) < 0
  }

  // ì„ì‹œ íŒŒì¼ ì €ì¥
  val outputFile = createTempFile("chunk_", ".sorted")
  val output = new BufferedOutputStream(new FileOutputStream(outputFile))
  sortedRecords.foreach(output.write)
  output.close()

  outputFile
}
```

#### 4.2.2 íŒŒí‹°ì…”ë‹

```scala
def partitionSortedChunk(
    sortedChunk: File,
    boundaries: Array[Array[Byte]],
    outputDir: File): Unit = {

  val partitionWriters = boundaries.indices.map { i =>
    new BufferedOutputStream(
      new FileOutputStream(new File(outputDir, s"partition_$i.dat"), true),
      1024 * 1024  // 1MB ë²„í¼
    )
  }.toArray :+ new BufferedOutputStream(
    new FileOutputStream(
      new File(outputDir, s"partition_${boundaries.length}.dat"), true
    ),
    1024 * 1024
  )

  val input = new BufferedInputStream(new FileInputStream(sortedChunk))
  val record = new Array[Byte](100)

  while (input.read(record) == 100) {
    val key = record.take(10)
    val partitionId = findPartitionBinarySearch(key, boundaries)  // â­ UPDATED
    partitionWriters(partitionId).write(record)
  }

  input.close()
  partitionWriters.foreach(_.close())
}

def findPartitionBinarySearch(key: Array[Byte],
                               boundaries: Array[Array[Byte]]): Int = {
  var left = 0
  var right = boundaries.length

  while (left < right) {
    val mid = (left + right) / 2
    if (compareKeys(key, boundaries(mid)) < 0) {
      right = mid
    } else {
      left = mid + 1
    }
  }
  left
}
```

### 4.3 Phase 3: Shuffle Algorithm â­ UPDATED

#### 4.3.1 ì†¡ì‹  (Sender) with Retry â­ NEW

```scala
def shufflePartitionWithRetry(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo,
    maxRetries: Int = 3): Unit = {

  var attempt = 0
  var success = false

  while (attempt < maxRetries && !success) {
    try {
      shufflePartition(partitionFile, partitionId, targetWorker)
      success = true
    } catch {
      case e: StatusRuntimeException if isRetryable(e) =>
        attempt += 1
        val backoffMs = math.pow(2, attempt).toLong * 1000  // ì§€ìˆ˜ ë°±ì˜¤í”„
        logger.warn(s"Shuffle failed (attempt $attempt/$maxRetries), " +
                   s"retrying in ${backoffMs}ms: ${e.getMessage}")
        Thread.sleep(backoffMs)

      case e: Exception =>
        logger.error(s"Non-retryable shuffle error: ${e.getMessage}")
        throw e
    }
  }

  if (!success) {
    throw new IOException(s"Failed to shuffle after $maxRetries attempts")
  }
}

def isRetryable(e: StatusRuntimeException): Boolean = {
  e.getStatus.getCode match {
    case Status.Code.UNAVAILABLE => true
    case Status.Code.DEADLINE_EXCEEDED => true
    case Status.Code.RESOURCE_EXHAUSTED => true
    case Status.Code.ABORTED => true
    case _ => false
  }
}

def shufflePartition(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo): Unit = {

  val channel = createGrpcChannelWithRetry(
    targetWorker.host,
    targetWorker.port
  )  // â­ NEW

  val stub = WorkerServiceGrpc.newStub(channel)
  val responseObserver = new StreamObserver[ShuffleAck] {
    override def onNext(ack: ShuffleAck): Unit = {
      logger.debug(s"Received ACK: ${ack.getBytesReceived} bytes")
    }
    override def onError(t: Throwable): Unit = {
      logger.error(s"Shuffle failed: ${t.getMessage}")
    }
    override def onCompleted(): Unit = {
      logger.info(s"Shuffle completed for partition $partitionId")
    }
  }

  val requestObserver = stub.shuffleData(responseObserver)

  // ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡
  val input = new BufferedInputStream(new FileInputStream(partitionFile))
  val buffer = new Array[Byte](1024 * 1024)  // 1MB ì²­í¬

  var bytesRead = 0
  while ({bytesRead = input.read(buffer); bytesRead > 0}) {
    val chunk = ShuffleDataChunk.newBuilder()
      .setPartitionId(partitionId)
      .setData(ByteString.copyFrom(buffer, 0, bytesRead))
      .build()

    requestObserver.onNext(chunk)
  }

  input.close()
  requestObserver.onCompleted()

  // ì™„ë£Œ ëŒ€ê¸° ë° ì±„ë„ ì¢…ë£Œ
  channel.shutdown()
  channel.awaitTermination(60, TimeUnit.SECONDS)
}

def createGrpcChannelWithRetry(host: String, port: Int): ManagedChannel = {
  ManagedChannelBuilder
    .forTarget(s"$host:$port")
    .usePlaintext()
    .keepAliveTime(30, TimeUnit.SECONDS)
    .keepAliveTimeout(10, TimeUnit.SECONDS)
    .idleTimeout(5, TimeUnit.MINUTES)
    .maxRetryAttempts(3)
    .retryBufferSize(16 * 1024 * 1024)  // 16MB
    .build()
}
```

#### 4.3.2 ìˆ˜ì‹  (Receiver)

```scala
class WorkerServiceImpl extends WorkerServiceGrpc.WorkerServiceImplBase {

  override def shuffleData(
      responseObserver: StreamObserver[ShuffleAck]):
      StreamObserver[ShuffleDataChunk] = {

    new StreamObserver[ShuffleDataChunk] {
      val receivedFiles = mutable.Map[Int, FileOutputStream]()

      override def onNext(chunk: ShuffleDataChunk): Unit = {
        val partitionId = chunk.getPartitionId
        val output = receivedFiles.getOrElseUpdate(
          partitionId,
          new BufferedOutputStream(
            new FileOutputStream(
              new File(receivedDir, s"partition_${partitionId}_${UUID.randomUUID()}.dat")
            ),
            1024 * 1024
          )
        )

        output.write(chunk.getData.toByteArray)
      }

      override def onError(t: Throwable): Unit = {
        logger.error(s"Shuffle receive error: ${t.getMessage}")
        receivedFiles.values.foreach(_.close())
        receivedFiles.clear()
      }

      override def onCompleted(): Unit = {
        receivedFiles.values.foreach(_.close())
        responseObserver.onNext(
          ShuffleAck.newBuilder()
            .setSuccess(true)
            .setBytesReceived(receivedFiles.values.map(_.size).sum)
            .build()
        )
        responseObserver.onCompleted()
      }
    }
  }
}
```

### 4.4 Phase 4: Merge Algorithm â­ UPDATED

#### 4.4.1 K-way Merge with Atomic Write

```scala
case class RecordWithSource(
  record: Array[Byte],
  sourceId: Int
)

def kWayMergeAtomic(
    inputFiles: List[File],
    partitionId: Int,
    outputDir: File): Unit = {

  // ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê¸° â­ NEW
  val tempFile = new File(outputDir, s".partition.$partitionId.tmp")

  // Priority Queue (min-heap)
  implicit val ord: Ordering[RecordWithSource] = Ordering.by { rws =>
    rws.record.take(10)
  }(Ordering.by[Array[Byte], String](_.map("%02x".format(_)).mkString).reverse)

  val heap = mutable.PriorityQueue[RecordWithSource]()
  val readers = inputFiles.zipWithIndex.map { case (file, idx) =>
    new BufferedInputStream(new FileInputStream(file), 1024 * 1024)
  }

  // ì´ˆê¸°í™”: ê° íŒŒì¼ì—ì„œ ì²« ë ˆì½”ë“œ ì½ê¸°
  readers.zipWithIndex.foreach { case (reader, idx) =>
    val record = new Array[Byte](100)
    if (reader.read(record) == 100) {
      heap.enqueue(RecordWithSource(record.clone(), idx))
    }
  }

  val output = new BufferedOutputStream(
    new FileOutputStream(tempFile),
    4 * 1024 * 1024  // 4MB ë²„í¼
  )

  // Merge
  while (heap.nonEmpty) {
    val min = heap.dequeue()
    output.write(min.record)

    // ê°™ì€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìŒ ë ˆì½”ë“œ ì½ê¸°
    val record = new Array[Byte](100)
    if (readers(min.sourceId).read(record) == 100) {
      heap.enqueue(RecordWithSource(record, min.sourceId))
    }
  }

  output.close()
  readers.foreach(_.close())

  // Atomic rename â­ NEW
  val finalFile = new File(outputDir, s"partition.$partitionId")
  if (!tempFile.renameTo(finalFile)) {
    throw new IOException(s"Failed to create final partition file: $finalFile")
  }

  logger.info(s"Created partition.$partitionId (${finalFile.length() / 1e6} MB)")
}
```

---

## 5. ë„¤íŠ¸ì›Œí¬ í†µì‹  ì„¤ê³„

### 5.1 Protocol Buffers ì •ì˜

```protobuf
syntax = "proto3";

package distsort;

// ========== Master Service ==========

service MasterService {
  // Worker ë“±ë¡
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);

  // ìƒ˜í”Œ ì „ì†¡
  rpc SendSample(SampleData) returns (Ack);

  // ì‘ì—… ì™„ë£Œ ë³´ê³ 
  rpc ReportCompletion(CompletionInfo) returns (Ack);
}

message WorkerInfo {
  string worker_id = 1;
  string host = 2;
  int32 port = 3;
  repeated string input_directories = 4;
  string output_directory = 5;
  int64 total_input_size = 6;
  DataFormat format = 7;  // â­ NEW
}

enum DataFormat {  // â­ NEW
  BINARY = 0;
  ASCII = 1;
}

message RegistrationResponse {
  bool success = 1;
  string message = 2;
  int32 worker_index = 3;
}

message SampleData {
  string worker_id = 1;
  repeated bytes keys = 2;
  int32 sample_count = 3;
}

message CompletionInfo {
  string worker_id = 1;
  repeated int32 partition_ids = 2;
  repeated int64 partition_sizes = 3;
  double elapsed_time_seconds = 4;
}

message Ack {
  bool success = 1;
  string message = 2;
}

// ========== Worker Service ==========

service WorkerService {
  rpc SetPartitionBoundaries(PartitionBoundaries) returns (Ack);
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);
  rpc GetStatus(StatusRequest) returns (WorkerStatus);
  rpc Reset(ResetRequest) returns (Ack);  // â­ NEW: ì¥ì•  ë³µêµ¬ìš©
}

message PartitionBoundaries {
  repeated bytes boundaries = 1;
  int32 num_partitions = 2;
  map<int32, WorkerInfo> partition_assignment = 3;
}

message ShuffleDataChunk {
  int32 partition_id = 1;
  bytes data = 2;
  int64 chunk_offset = 3;
  bool is_last = 4;
}

message ShuffleAck {
  bool success = 1;
  string message = 2;
  int64 bytes_received = 3;
}

message StatusRequest {
  string worker_id = 1;
}

message WorkerStatus {
  enum Phase {
    INITIALIZING = 0;
    SAMPLING = 1;
    SORTING = 2;
    SHUFFLING = 3;
    MERGING = 4;
    COMPLETED = 5;
    FAILED = 6;
  }

  Phase current_phase = 1;
  double progress_percentage = 2;
  string message = 3;
}

message ResetRequest {  // â­ NEW
  string reason = 1;
}
```

### 5.2 Master êµ¬í˜„ â­ UPDATED

```scala
class MasterServer(numWorkers: Int) extends MasterServiceGrpc.MasterServiceImplBase {

  private val workers = new ConcurrentHashMap[String, WorkerInfo]()
  private val samples = new ConcurrentHashMap[String, SampleData]()
  private val completions = new AtomicInteger(0)
  private val latch = new CountDownLatch(numWorkers)

  override def registerWorker(
      request: WorkerInfo,
      responseObserver: StreamObserver[RegistrationResponse]): Unit = {

    val workerId = request.getWorkerId
    workers.put(workerId, request)

    // stderrë¡œ ë¡œê·¸, stdoutì€ ìµœì¢… ê²°ê³¼ ì „ìš© â­ NEW
    System.err.println(s"[INFO] Worker registered: $workerId at ${request.getHost}:${request.getPort}")

    val response = RegistrationResponse.newBuilder()
      .setSuccess(true)
      .setWorkerIndex(workers.size())
      .setMessage("Registration successful")
      .build()

    responseObserver.onNext(response)
    responseObserver.onCompleted()

    latch.countDown()
  }

  override def sendSample(
      request: SampleData,
      responseObserver: StreamObserver[Ack]): Unit = {

    samples.put(request.getWorkerId, request)
    System.err.println(s"[INFO] Received sample from ${request.getWorkerId}: ${request.getSampleCount} keys")

    // ëª¨ë“  ìƒ˜í”Œ ë„ì°© í™•ì¸
    if (samples.size() == numWorkers) {
      calculateAndBroadcastBoundaries()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def calculateAndBroadcastBoundaries(): Unit = {
    // ëª¨ë“  ìƒ˜í”Œ ìˆ˜ì§‘
    val allKeys = samples.values().asScala.flatMap(_.getKeysList.asScala).toArray

    // ì •ë ¬
    val sortedKeys = allKeys.sortWith { (a, b) =>
      compareKeys(a.toByteArray, b.toByteArray) < 0
    }

    // ê²½ê³„ ê³„ì‚° (ê· ë“± ë¶„ë°°)
    val boundaries = (1 until numWorkers).map { i =>
      val idx = (sortedKeys.length * i) / numWorkers
      sortedKeys(idx)
    }

    System.err.println(s"[INFO] Calculated ${boundaries.length} partition boundaries")

    // íŒŒí‹°ì…˜ í• ë‹¹ (Worker i â†’ Partition i)
    val assignment = workers.asScala.toList.sortBy(_._2.getWorkerIndex).zipWithIndex.map {
      case ((_, workerInfo), partitionId) => (partitionId, workerInfo)
    }.toMap

    // ëª¨ë“  Workerì—ê²Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
    val boundariesMsg = PartitionBoundaries.newBuilder()
      .addAllBoundaries(boundaries.map(ByteString.copyFrom).asJava)
      .setNumPartitions(numWorkers)
      .putAllPartitionAssignment(assignment.map { case (k, v) => (k: Integer, v) }.asJava)
      .build()

    workers.values().asScala.foreach { workerInfo =>
      val channel = createGrpcChannelWithRetry(
        workerInfo.getHost,
        workerInfo.getPort
      )

      val stub = WorkerServiceGrpc.newBlockingStub(channel)
      stub.setPartitionBoundaries(boundariesMsg)
      channel.shutdown()
    }
  }

  override def reportCompletion(
      request: CompletionInfo,
      responseObserver: StreamObserver[Ack]): Unit = {

    System.err.println(s"[INFO] Worker ${request.getWorkerId} completed in ${request.getElapsedTimeSeconds}s")

    if (completions.incrementAndGet() == numWorkers) {
      System.err.println("[INFO] All workers completed! Job finished.")
      printFinalOrdering()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def printFinalOrdering(): Unit = {
    val orderedWorkers = workers.values().asScala.toList
      .sortBy(_.getWorkerIndex)
      .map(w => s"${w.getHost}:${w.getPort}")

    // â­ NEW: stdoutìœ¼ë¡œë§Œ ì¶œë ¥ (ë¡œê·¸ì™€ ë¶„ë¦¬)
    System.out.println(orderedWorkers.mkString(", "))
    System.out.flush()
  }
}
```

---

## 6. ì¥ì•  í—ˆìš©ì„± ë©”ì»¤ë‹ˆì¦˜

### 6.1 ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤

```
ì‹œë‚˜ë¦¬ì˜¤ 1: Sort ì¤‘ Worker Crash
  Worker2 ì •ë ¬ ì¤‘ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
  â†’ ì„ì‹œ íŒŒì¼ ì¼ë¶€ë§Œ ìƒì„±ëœ ìƒíƒœ
  â†’ ì¬ì‹œì‘ ì‹œ ì„ì‹œ íŒŒì¼ ì‚­ì œ í›„ ì²˜ìŒë¶€í„°

ì‹œë‚˜ë¦¬ì˜¤ 2: Shuffle ì¤‘ Worker Crash
  Worker2ê°€ ë°ì´í„° ì†¡ì‹  ì¤‘ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
  â†’ ì¼ë¶€ WorkerëŠ” ë°ì´í„° ë°›ì•˜ì§€ë§Œ ë‚˜ë¨¸ì§€ëŠ” ëª» ë°›ìŒ
  â†’ ì¬ì‹œì‘ ì‹œ ëª¨ë“  Workerê°€ Shuffle ì¬ì‹œì‘

ì‹œë‚˜ë¦¬ì˜¤ 3: Merge ì¤‘ Worker Crash
  Worker2ê°€ ë³‘í•© ì¤‘ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
  â†’ ìµœì¢… ì¶œë ¥ íŒŒì¼ ë¯¸ì™„ì„±
  â†’ ì¬ì‹œì‘ ì‹œ ë³‘í•© ì¬ì‹œì‘
```

### 6.2 ë³µêµ¬ ì „ëµ

#### 6.2.1 ë©±ë“±ì„± ë³´ì¥

```scala
class WorkerNode {

  def start(): Unit = {
    try {
      // ì‹œì‘ ì „ ì •ë¦¬
      cleanupTemporaryFiles()
      cleanupOutputFiles()

      // ì •ìƒ ì‘ì—… ìˆ˜í–‰
      performSorting()

    } catch {
      case e: Exception =>
        logger.error("Worker failed", e)
        cleanupTemporaryFiles()
        cleanupOutputFiles()
        throw e
    }
  }

  private def cleanupTemporaryFiles(): Unit = {
    val tempDir = new File(config.tempDir, s"sort_work_$workerId")
    if (tempDir.exists()) {
      FileUtils.deleteRecursively(tempDir)
      logger.info("Cleaned up temporary files")
    }
  }

  private def cleanupOutputFiles(): Unit = {
    val outputDirFile = new File(config.outputDir)
    if (outputDirFile.exists()) {
      outputDirFile.listFiles().foreach { file =>
        if (file.getName.startsWith("partition.") ||
            file.getName.startsWith(".partition.")) {  // â­ NEW: ì„ì‹œ íŒŒì¼ë„ ì‚­ì œ
          file.delete()
          logger.info(s"Deleted incomplete output: ${file.getName}")
        }
      }
    }
  }
}
```

---

## 7. ë©€í‹°ìŠ¤ë ˆë“œ ë° ë³‘ë ¬ ì²˜ë¦¬

### 7.1 Sort & Partition ë³‘ë ¬í™”

```scala
class ParallelSorter(numThreads: Int = 4) {

  private val executor = Executors.newFixedThreadPool(numThreads)

  def sortAll(inputFiles: List[File], format: DataFormat): List[File] = {
    val futures = inputFiles.map { file =>
      executor.submit(new Callable[File] {
        override def call(): File = sortChunk(file, format)
      })
    }

    futures.map(_.get())
  }

  def partitionAll(sortedChunks: List[File],
                   boundaries: Array[Array[Byte]]): Unit = {
    val futures = sortedChunks.map { chunk =>
      executor.submit(new Callable[Unit] {
        override def call(): Unit = partitionSortedChunk(chunk, boundaries)
      })
    }

    futures.foreach(_.get())
  }

  def shutdown(): Unit = {
    executor.shutdown()
    executor.awaitTermination(1, TimeUnit.HOURS)
  }
}
```

---

## 8. êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 8.1 í”„ë¡œì íŠ¸ êµ¬ì¡° â­ UPDATED

```
project_2025/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”‚   â””â”€â”€ build.properties
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ scala/
â”‚       â”‚   â””â”€â”€ distsort/
â”‚       â”‚       â”œâ”€â”€ Main.scala
â”‚       â”‚       â”œâ”€â”€ master/
â”‚       â”‚       â”‚   â”œâ”€â”€ MasterServer.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ PartitionCalculator.scala
â”‚       â”‚       â”‚   â””â”€â”€ WorkerRegistry.scala
â”‚       â”‚       â”œâ”€â”€ worker/
â”‚       â”‚       â”‚   â”œâ”€â”€ WorkerNode.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ Sampler.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ Sorter.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ Partitioner.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ Shuffler.scala
â”‚       â”‚       â”‚   â””â”€â”€ Merger.scala
â”‚       â”‚       â”œâ”€â”€ common/
â”‚       â”‚       â”‚   â”œâ”€â”€ RecordComparator.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ RecordReader.scala          â­ NEW
â”‚       â”‚       â”‚   â”œâ”€â”€ BinaryRecordReader.scala    â­ NEW
â”‚       â”‚       â”‚   â”œâ”€â”€ AsciiRecordReader.scala     â­ NEW
â”‚       â”‚       â”‚   â”œâ”€â”€ FileUtils.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ NetworkUtils.scala
â”‚       â”‚       â”‚   â””â”€â”€ CommandLineParser.scala     â­ NEW
â”‚       â”‚       â””â”€â”€ proto/
â”‚       â”‚           â””â”€â”€ (generated proto files)
â”‚       â”œâ”€â”€ protobuf/
â”‚       â”‚   â””â”€â”€ distsort.proto
â”‚       â””â”€â”€ resources/
â”‚           â””â”€â”€ logback.xml
â”œâ”€â”€ plan/
â”‚   â”œâ”€â”€ 2025-10-24_init_plan.md
â”‚   â””â”€â”€ 2025-10-24_init_plan_ver2.md         â­ NEW
â””â”€â”€ README.md
```

### 8.2 build.sbt

```scala
name := "distributed-sorting"
version := "1.0"
scalaVersion := "2.13.12"

libraryDependencies ++= Seq(
  // gRPC
  "io.grpc" % "grpc-netty" % "1.58.0",
  "io.grpc" % "grpc-protobuf" % "1.58.0",
  "io.grpc" % "grpc-stub" % "1.58.0",
  "com.thesamet.scalapb" %% "scalapb-runtime-grpc" % "0.11.13",

  // Logging
  "ch.qos.logback" % "logback-classic" % "1.4.11",
  "com.typesafe.scala-logging" %% "scala-logging" % "3.9.5",

  // Testing
  "org.scalatest" %% "scalatest" % "3.2.17" % Test
)

// Protocol Buffers ì„¤ì •
Compile / PB.targets := Seq(
  scalapb.gen() -> (Compile / sourceManaged).value / "scalapb"
)
```

---

## 9. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 9.1 I/O ìµœì í™”

#### 9.1.1 ë²„í¼ë§

```scala
// ì˜ëª»ëœ ì˜ˆ: ë²„í¼ë§ ì—†ìŒ
val input = new FileInputStream(file)
val record = new Array[Byte](100)
while (input.read(record) == 100) {  // ë§¤ë²ˆ ì‹œìŠ¤í…œ ì½œ!
  process(record)
}

// ì˜¬ë°”ë¥¸ ì˜ˆ: ë²„í¼ë§ ì‚¬ìš©
val input = new BufferedInputStream(
  new FileInputStream(file),
  1024 * 1024  // 1MB ë²„í¼
)
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  process(record)
}
```

### 9.2 ë””ìŠ¤í¬ ê³µê°„ ì‚¬ì „ í™•ì¸ â­ NEW

```scala
def ensureSufficientDiskSpace(
    tempDir: File,
    outputDir: File,
    inputSize: Long): Unit = {

  // í•„ìš” ê³µê°„ ì¶”ì •
  val requiredTemp = inputSize * 2  // ì •ë ¬ ì¤‘ê°„ íŒŒì¼ìš©
  val requiredOutput = inputSize    // ìµœì¢… ì¶œë ¥ìš©

  val tempAvailable = tempDir.getUsableSpace
  val outputAvailable = outputDir.getUsableSpace

  if (tempAvailable < requiredTemp * 1.5) {
    throw new IOException(
      f"Insufficient temp disk space: ${tempAvailable / 1e9}%.1f GB available, " +
      f"${requiredTemp * 1.5 / 1e9}%.1f GB required"
    )
  }

  if (outputAvailable < requiredOutput * 1.2) {
    throw new IOException(
      f"Insufficient output disk space: ${outputAvailable / 1e9}%.1f GB available, " +
      f"${requiredOutput * 1.2 / 1e9}%.1f GB required"
    )
  }

  logger.info(f"Disk space check passed: " +
             f"Temp ${tempAvailable / 1e9}%.1f GB, " +
             f"Output ${outputAvailable / 1e9}%.1f GB")
}
```

---

## 10. í…ŒìŠ¤íŠ¸ ì „ëµ

### 10.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```scala
class RecordComparatorTest extends AnyFlatSpec with Matchers {

  "compareKeys" should "compare unsigned bytes correctly" in {
    val key1 = Array[Byte](0x00, 0x00, 0x00, 0x00, 0x00,
                           0x00, 0x00, 0x00, 0x00, 0x00)
    val key2 = Array[Byte](0xFF.toByte, 0xFF.toByte, 0xFF.toByte,
                           0xFF.toByte, 0xFF.toByte, 0xFF.toByte,
                           0xFF.toByte, 0xFF.toByte, 0xFF.toByte,
                           0xFF.toByte)

    RecordComparator.compareKeys(key1, key2) should be < 0
    RecordComparator.compareKeys(key2, key1) should be > 0
    RecordComparator.compareKeys(key1, key1) shouldEqual 0
  }
}

class RecordReaderTest extends AnyFlatSpec with Matchers {  // â­ NEW

  "BinaryRecordReader" should "read 100-byte records" in {
    val data = Array.fill(300)(0.toByte)
    val input = new ByteArrayInputStream(data)
    val reader = new BinaryRecordReader()

    val record1 = reader.readRecord(input)
    val record2 = reader.readRecord(input)
    val record3 = reader.readRecord(input)
    val record4 = reader.readRecord(input)

    record1 shouldBe defined
    record2 shouldBe defined
    record3 shouldBe defined
    record4 shouldBe empty
  }

  "AsciiRecordReader" should "parse ASCII format correctly" in {
    val key = "ABCDEFGHIJ"
    val value = "X" * 90
    val line = s"$key $value\n"
    val input = new ByteArrayInputStream(line.getBytes("ASCII"))
    val reader = new AsciiRecordReader()

    val record = reader.readRecord(input)
    record shouldBe defined
    record.get.take(10) shouldEqual key.getBytes("ASCII")
    record.get.drop(10) shouldEqual value.getBytes("ASCII")
  }
}
```

---

## 11. ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤ ìƒì„¸ â­ NEW

### 11.1 Master ëª…ë ¹í–‰

```bash
sbt "runMain distsort.Main master <N>"
```

**ë§¤ê°œë³€ìˆ˜:**
- `<N>`: ì˜ˆìƒ Worker ìˆ˜ (ì •ìˆ˜)

**ì¶œë ¥:**
- **stdout**: ì •ë ¬ëœ Worker ì£¼ì†Œ (ì‰¼í‘œ êµ¬ë¶„)
  - ì˜ˆ: `192.168.1.10:30001, 192.168.1.11:30002, 192.168.1.12:30003`
- **stderr**: ë¡œê·¸ ë©”ì‹œì§€ (ì§„í–‰ ìƒí™©, ì—ëŸ¬ ë“±)

**ì˜ˆì‹œ:**
```bash
$ sbt "runMain distsort.Main master 3" 2> master.log
192.168.1.10:30001, 192.168.1.11:30002, 192.168.1.12:30003
```

### 11.2 Worker ëª…ë ¹í–‰

```bash
sbt "runMain distsort.Main worker <master> -I <dir1> <dir2> ... -O <output> [options]"
```

**í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜:**
- `<master>`: Master ì£¼ì†Œ (host:port í˜•ì‹)
- `-I <dirs...>`: ì…ë ¥ ë””ë ‰í† ë¦¬ë“¤ (ìµœì†Œ 1ê°œ, ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
- `-O <output>`: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ë‹¨ì¼)

**ì„ íƒì  ë§¤ê°œë³€ìˆ˜:**
- `--ascii`: ASCII í˜•ì‹ ì…ë ¥ (ê¸°ë³¸ê°’: binary)
- `--binary`: Binary í˜•ì‹ ì…ë ¥ (ëª…ì‹œì )
- `--temp <dir>`: ì¤‘ê°„ íŒŒì¼ ì €ì¥ ìœ„ì¹˜ (ê¸°ë³¸ê°’: /tmp)
- `--threads <N>`: ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)

**ì˜ˆì‹œ:**

```bash
# Binary í˜•ì‹ (ê¸°ë³¸)
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1 /data2 -O /output"

# ASCII í˜•ì‹
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1 -O /output --ascii"

# ì¤‘ê°„ íŒŒì¼ ìœ„ì¹˜ ì§€ì •
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1 -O /output --temp /scratch"

# ëª¨ë“  ì˜µì…˜ ì‚¬ìš©
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 \
  -I /data1 /data2 /data3 \
  -O /output \
  --ascii \
  --temp /scratch \
  --threads 8"
```

### 11.3 ëª…ë ¹í–‰ íŒŒì„œ êµ¬í˜„ â­ NEW

```scala
case class WorkerConfig(
  masterAddress: String,
  inputDirs: List[String],
  outputDir: String,
  format: DataFormat = DataFormat.Binary,
  tempDir: String = "/tmp",
  numThreads: Option[Int] = None
)

object CommandLineParser {

  def parseWorkerArgs(args: Array[String]): WorkerConfig = {
    var masterAddress: Option[String] = None
    var inputDirs: List[String] = List.empty
    var outputDir: Option[String] = None
    var format: DataFormat = DataFormat.Binary
    var tempDir: String = "/tmp"
    var numThreads: Option[Int] = None

    var i = 0
    while (i < args.length) {
      args(i) match {
        case "-I" =>
          // ë‹¤ìŒ í”Œë˜ê·¸ê¹Œì§€ ëª¨ë“  ë””ë ‰í† ë¦¬ ìˆ˜ì§‘
          i += 1
          while (i < args.length && !args(i).startsWith("-")) {
            inputDirs = inputDirs :+ args(i)
            i += 1
          }
          i -= 1

        case "-O" =>
          i += 1
          if (i >= args.length) {
            throw new IllegalArgumentException("Missing output directory after -O")
          }
          outputDir = Some(args(i))

        case "--ascii" =>
          format = DataFormat.Ascii

        case "--binary" =>
          format = DataFormat.Binary

        case "--temp" =>
          i += 1
          if (i >= args.length) {
            throw new IllegalArgumentException("Missing directory after --temp")
          }
          tempDir = args(i)

        case "--threads" =>
          i += 1
          if (i >= args.length) {
            throw new IllegalArgumentException("Missing number after --threads")
          }
          try {
            numThreads = Some(args(i).toInt)
          } catch {
            case _: NumberFormatException =>
              throw new IllegalArgumentException(s"Invalid thread count: ${args(i)}")
          }

        case addr if !addr.startsWith("-") && masterAddress.isEmpty =>
          masterAddress = Some(addr)

        case unknown =>
          throw new IllegalArgumentException(s"Unknown option: $unknown")
      }
      i += 1
    }

    // ê²€ì¦
    require(masterAddress.isDefined, "Master address is required")
    require(inputDirs.nonEmpty, "At least one input directory is required (-I)")
    require(outputDir.isDefined, "Output directory is required (-O)")

    // ì…ë ¥ ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    inputDirs.foreach { dir =>
      val file = new File(dir)
      require(file.exists() && file.isDirectory,
              s"Input directory does not exist or is not a directory: $dir")
    }

    // ì¤‘ê°„ íŒŒì¼ ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ (ì—†ìœ¼ë©´ ìƒì„±)
    val tempDirFile = new File(tempDir)
    if (!tempDirFile.exists()) {
      tempDirFile.mkdirs()
    }
    require(tempDirFile.canWrite, s"Temp directory is not writable: $tempDir")

    WorkerConfig(
      masterAddress = masterAddress.get,
      inputDirs = inputDirs,
      outputDir = outputDir.get,
      format = format,
      tempDir = tempDir,
      numThreads = numThreads
    )
  }

  def printUsage(): Unit = {
    println("""
      |Usage:
      |  Master:
      |    sbt "runMain distsort.Main master <num_workers>"
      |
      |  Worker:
      |    sbt "runMain distsort.Main worker <master> -I <dirs...> -O <output> [options]"
      |
      |  Options:
      |    --ascii           Use ASCII format (default: binary)
      |    --binary          Use binary format (explicit)
      |    --temp <dir>      Temporary directory (default: /tmp)
      |    --threads <N>     Number of threads (default: CPU cores)
      |
      |  Examples:
      |    # Master
      |    $ sbt "runMain distsort.Main master 3"
      |
      |    # Worker (binary)
      |    $ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1 /data2 -O /output"
      |
      |    # Worker (ASCII, custom temp)
      |    $ sbt "runMain distsort.Main worker 192.168.1.1:30000 \\
      |        -I /data1 -O /output --ascii --temp /scratch"
      |""".stripMargin)
  }
}
```

---

## 12. ì…ë ¥ í˜•ì‹ ì²˜ë¦¬ â­ NEW

### 12.1 ë ˆì½”ë“œ ë¦¬ë” ì¶”ìƒí™”

```scala
sealed trait DataFormat
object DataFormat {
  case object Binary extends DataFormat
  case object Ascii extends DataFormat
}

trait RecordReader {
  /**
   * ìŠ¤íŠ¸ë¦¼ì—ì„œ ë ˆì½”ë“œ í•˜ë‚˜ë¥¼ ì½ìŠµë‹ˆë‹¤.
   * @return Some(100ë°”ì´íŠ¸ ë ˆì½”ë“œ) ë˜ëŠ” None (EOF)
   */
  def readRecord(input: InputStream): Option[Array[Byte]]
}

object RecordReader {
  def create(format: DataFormat): RecordReader = format match {
    case DataFormat.Binary => new BinaryRecordReader()
    case DataFormat.Ascii => new AsciiRecordReader()
  }
}
```

### 12.2 Binary í˜•ì‹ ë¦¬ë”

```scala
class BinaryRecordReader extends RecordReader {

  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val record = new Array[Byte](100)
    val bytesRead = input.read(record)

    if (bytesRead == 100) {
      Some(record)
    } else if (bytesRead == -1) {
      None  // EOF
    } else {
      throw new IOException(
        s"Incomplete record: expected 100 bytes, got $bytesRead bytes"
      )
    }
  }
}
```

### 12.3 ASCII í˜•ì‹ ë¦¬ë”

```scala
class AsciiRecordReader extends RecordReader {

  /**
   * ASCII í˜•ì‹:
   * - Key: 10 ë¬¸ì (ASCII printable)
   * - Space: 1 ë°”ì´íŠ¸
   * - Value: 90 ë¬¸ì (ASCII printable)
   * - Newline: 1 ë°”ì´íŠ¸ (\n)
   * ì´ 102 ë°”ì´íŠ¸
   */
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val line = new Array[Byte](102)
    val bytesRead = input.read(line)

    if (bytesRead == -1) {
      return None  // EOF
    }

    if (bytesRead != 102) {
      throw new IOException(
        s"Invalid ASCII record: expected 102 bytes, got $bytesRead bytes"
      )
    }

    // í˜•ì‹ ê²€ì¦
    if (line(10) != ' '.toByte) {
      throw new IOException(
        s"Invalid ASCII record: expected space at position 10, got '${line(10).toChar}'"
      )
    }

    if (line(101) != '\n'.toByte) {
      throw new IOException(
        s"Invalid ASCII record: expected newline at position 101, got '${line(101).toChar}'"
      )
    }

    // 100ë°”ì´íŠ¸ ë ˆì½”ë“œë¡œ ë³€í™˜
    val record = new Array[Byte](100)

    // Key: ì²« 10ë°”ì´íŠ¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
    System.arraycopy(line, 0, record, 0, 10)

    // Value: 12ë²ˆì§¸ë¶€í„° 90ë°”ì´íŠ¸ ë³µì‚¬ (space ê±´ë„ˆë›°ê¸°)
    System.arraycopy(line, 11, record, 10, 90)

    Some(record)
  }
}
```

### 12.4 ì‚¬ìš© ì˜ˆì‹œ

```scala
def processInputFiles(
    inputDirs: List[String],
    format: DataFormat,
    processor: Array[Byte] => Unit): Unit = {

  val reader = RecordReader.create(format)

  for (file <- getAllInputFiles(inputDirs)) {
    val input = new BufferedInputStream(
      new FileInputStream(file),
      1024 * 1024  // 1MB ë²„í¼
    )

    try {
      var record = reader.readRecord(input)
      while (record.isDefined) {
        processor(record.get)
        record = reader.readRecord(input)
      }
    } finally {
      input.close()
    }
  }
}

// ìƒ˜í”Œë§ ì˜ˆì‹œ
def extractSample(config: WorkerConfig): Array[Array[Byte]] = {
  val samples = mutable.ArrayBuffer[Array[Byte]]()
  val random = new Random(42)

  val sampleRate = calculateSampleRate(
    FileUtils.calculateTotalSize(config.inputDirs),
    numWorkers
  )

  processInputFiles(config.inputDirs, config.format, { record =>
    if (random.nextDouble() < sampleRate) {
      samples += record.take(10)  // Keyë§Œ ì €ì¥
    }
  })

  samples.toArray
}
```

---

## 13. ê°œë°œ ë§ˆì¼ìŠ¤í†¤ â­ UPDATED

### Milestone 1: ê¸°ë³¸ ì¸í”„ë¼ (Week 1-2)
- [ ] í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
- [ ] Protocol Buffers ì •ì˜ (DataFormat í¬í•¨) â­
- [ ] RecordReader ì¶”ìƒí™” ë° êµ¬í˜„ â­
- [ ] CommandLineParser êµ¬í˜„ â­
- [ ] Master/Worker ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ
- [ ] gRPC í†µì‹  ê¸°ë³¸ êµ¬í˜„
- [ ] ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (1 Master + 1 Worker, ì‘ì€ ë°ì´í„°)

**ê²€ì¦:**
```bash
# Master ì‹¤í–‰
$ sbt "runMain distsort.Main master 1" 2> master.log
127.0.0.1:30000

# Worker ì‹¤í–‰ (Binary)
$ sbt "runMain distsort.Main worker 127.0.0.1:30000 -I test_input -O test_output"
> Worker registered successfully

# Worker ì‹¤í–‰ (ASCII)
$ sbt "runMain distsort.Main worker 127.0.0.1:30000 -I test_input -O test_output --ascii"
> Worker registered successfully
```

### Milestone 2: í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (Week 3-4)
- [ ] ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚° êµ¬í˜„ â­
- [ ] ìƒ˜í”Œë§ êµ¬í˜„ (ASCII/Binary ì§€ì›) â­
- [ ] ì •ë ¬ ë° íŒŒí‹°ì…”ë‹ êµ¬í˜„
- [ ] Binary search ê¸°ë°˜ íŒŒí‹°ì…˜ ì°¾ê¸° â­
- [ ] Shuffle êµ¬í˜„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨) â­
- [ ] Merge êµ¬í˜„ (Atomic write) â­
- [ ] ë¡œì»¬ í†µí•© í…ŒìŠ¤íŠ¸ (1 Master + 3 Workers)

**ê²€ì¦:**
```bash
# Binary í˜•ì‹ í…ŒìŠ¤íŠ¸
$ ./gensort -b 100 input_binary
$ ./run_test.sh --binary
$ ./valsort output_binary
> SUCCESS

# ASCII í˜•ì‹ í…ŒìŠ¤íŠ¸
$ ./gensort -a 100 input_ascii
$ ./run_test.sh --ascii
$ ./valsort -a output_ascii
> SUCCESS
```

### Milestone 3: ì¥ì•  í—ˆìš©ì„± (Week 5)
- [ ] ì¬ì‹œì‘ ë¡œì§ êµ¬í˜„
- [ ] ì„ì‹œ íŒŒì¼ ì •ë¦¬ êµ¬í˜„
- [ ] Worker ì¥ì•  í…ŒìŠ¤íŠ¸
- [ ] ì›ìì  ì¶œë ¥ êµ¬í˜„ (partition.N ë„¤ì´ë°) â­
- [ ] ë„¤íŠ¸ì›Œí¬ ì¬ì‹œë„ ë° íƒ€ì„ì•„ì›ƒ â­

**ê²€ì¦:**
```bash
# Worker ê°•ì œ ì¢…ë£Œ í…ŒìŠ¤íŠ¸
$ ./test_failure.sh
> Worker 2 killed at t=10s
> Worker 2 restarted at t=12s
> All workers completed successfully
> Output verified: SUCCESS
```

### Milestone 4: ì„±ëŠ¥ ìµœì í™” (Week 6-7)
- [ ] ë©€í‹°ìŠ¤ë ˆë“œ êµ¬í˜„
- [ ] I/O ë²„í¼ë§ ìµœì í™”
- [ ] ë„¤íŠ¸ì›Œí¬ ë°°ì¹˜ ì „ì†¡
- [ ] ë””ìŠ¤í¬ ê³µê°„ ì‚¬ì „ í™•ì¸ â­
- [ ] í”„ë¡œíŒŒì¼ë§ ë° ë³‘ëª© ì œê±°
- [ ] ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ (50GB+)

**ê²€ì¦:**
```bash
# ì„±ëŠ¥ ì¸¡ì •
$ ./benchmark.sh
> 50GB sorted in 180 seconds
> Throughput: 285 MB/s
> Speedup vs single machine: 8.2x

# ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ì‹œë‚˜ë¦¬ì˜¤
$ ./test_disk_full.sh
> ERROR: Insufficient temp disk space: 5.2 GB available, 75.0 GB required
> Worker exited gracefully
```

### Milestone 5: ë§ˆë¬´ë¦¬ (Week 8)
- [ ] ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ASCII/Binary ëª¨ë‘)
- [ ] Master ì¶œë ¥ í˜•ì‹ ê²€ì¦ (stdout vs stderr) â­
- [ ] ë¬¸ì„œí™” ì™„ì„±
- [ ] ì½”ë“œ ë¦¬íŒ©í† ë§ ë° ì •ë¦¬
- [ ] ìµœì¢… ë°ëª¨ ì¤€ë¹„

---

## 14. ì°¸ê³  ìë£Œ

### 14.1 í•µì‹¬ ì•Œê³ ë¦¬ì¦˜
- External Sorting: Knuth, TAOCP Vol. 3
- K-way Merge: Priority Queue ê¸°ë°˜ ë³‘í•©
- Sampling for Partitioning: TeraSort ë…¼ë¬¸

### 14.2 ì‹œìŠ¤í…œ ì„¤ê³„
- MapReduce: Dean & Ghemawat, OSDI 2004
- Spark: Zaharia et al., NSDI 2012
- Distributed Sorting: Sort Benchmark ìš°ìŠ¹ ë…¼ë¬¸ë“¤

### 14.3 êµ¬í˜„ ì°¸ê³ 
- gRPC ê³µì‹ ë¬¸ì„œ: https://grpc.io/docs/languages/java/
- Protocol Buffers: https://protobuf.dev/
- Scala ë™ì‹œì„± íŒ¨í„´
- gensort/valsort ë„êµ¬ ì‚¬ìš©ë²•

---

## 15. ì˜ˆìƒ ì´ìŠˆ ë° í•´ê²°ì±… â­ UPDATED

### Issue 1: íŒŒí‹°ì…˜ ë¶ˆê· í˜•
**ì¦ìƒ:** ì¼ë¶€ Workerê°€ ë§¤ìš° í° íŒŒí‹°ì…˜ì„ ë°›ì•„ ë³‘ëª© ë°œìƒ

**í•´ê²°:**
- ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì • (ì…ë ¥ í¬ê¸° ê³ ë ¤) â­
- ìƒ˜í”Œ í¬ê¸° ì¦ê°€
- íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€ (Worker ë‹¹ ì—¬ëŸ¬ íŒŒí‹°ì…˜)
- Binary searchë¡œ íš¨ìœ¨ì ì¸ íŒŒí‹°ì…˜ í• ë‹¹ â­

### Issue 2: ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ë¶€ì¡±
**ì¦ìƒ:** Shuffle ë‹¨ê³„ì—ì„œ ë§¤ìš° ëŠë¦¼

**í•´ê²°:**
- ì••ì¶• ì‚¬ìš©
- ë°°ì¹˜ í¬ê¸° ì¦ê°€
- ë™ì‹œ ì „ì†¡ ìˆ˜ ì¡°ì ˆ
- ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ â­
- Keep-alive ë° íƒ€ì„ì•„ì›ƒ ì„¤ì • â­

### Issue 3: ë©”ëª¨ë¦¬ ë¶€ì¡±
**ì¦ìƒ:** OutOfMemoryError

**í•´ê²°:**
- ì²­í¬ í¬ê¸° ê°ì†Œ
- ìŠ¤ë ˆë“œ ìˆ˜ ê°ì†Œ
- ë²„í¼ í¬ê¸° ì¡°ì •
- JVM í™ í¬ê¸° ì¦ê°€ (`-Xmx8g`)

### Issue 4: ë””ìŠ¤í¬ I/O ë³‘ëª©
**ì¦ìƒ:** CPU ìœ íœ´ ìƒíƒœì¸ë° ì‘ì—… ëŠë¦¼

**í•´ê²°:**
- ë²„í¼ë§ ì¦ê°€ (1-4MB)
- Sequential I/O íŒ¨í„´ ìœ ì§€
- SSD ì‚¬ìš©
- ì„ì‹œ íŒŒì¼ì„ ë³„ë„ ë””ìŠ¤í¬ì— ì €ì¥ (`--temp` ì˜µì…˜) â­

### Issue 5: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± â­ NEW
**ì¦ìƒ:** ì‘ì—… ì¤‘ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨

**í•´ê²°:**
- ì‹œì‘ ì „ ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦ (ensureSufficientDiskSpace) â­
- ì„ì‹œ íŒŒì¼ ì¡°ê¸° ì •ë¦¬
- ì••ì¶• ì‚¬ìš©
- ì—¬ìœ  ê³µê°„ 50% ì´ìƒ í™•ë³´

### Issue 6: ASCII/Binary í˜•ì‹ í˜¼ë™ â­ NEW
**ì¦ìƒ:** ì˜ëª»ëœ í˜•ì‹ìœ¼ë¡œ ì½ì–´ì„œ ë°ì´í„° ì†ìƒ

**í•´ê²°:**
- ëª…ë ¹í–‰ í”Œë˜ê·¸ ëª…í™•í™” (`--ascii` / `--binary`) â­
- RecordReader í˜•ì‹ ê²€ì¦ ì¶”ê°€ â­
- ì´ˆê¸° ëª‡ ë ˆì½”ë“œë¡œ í˜•ì‹ ìë™ ê°ì§€ (ì„ íƒì )
- ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 

### Issue 7: Worker ì¬ì‹œì‘ ë¬´í•œ ë£¨í”„
**ì¦ìƒ:** Workerê°€ ê³„ì† ì‹¤íŒ¨í•˜ê³  ì¬ì‹œì‘

**í•´ê²°:**
- ì‹¤íŒ¨ ì›ì¸ ë¡œê¹… ê°•í™”
- ì¬ì‹œì‘ íšŸìˆ˜ ì œí•œ
- ë°±ì˜¤í”„ ì „ëµ (exponential backoff) â­
- Masterì˜ ì „ì—­ ì¬ì‹œì‘ íŠ¸ë¦¬ê±°
- ë””ìŠ¤í¬/ë©”ëª¨ë¦¬ ì‚¬ì „ ê²€ì¦ â­

---

## 16. ê²°ë¡ 

### 16.1 Version 2 ì£¼ìš” ê°œì„ ì‚¬í•­ ìš”ì•½

ì´ ì„¤ê³„ ë¬¸ì„œ v2ëŠ” ì´ˆê¸° ê³„íšì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ **8ê°€ì§€ í•µì‹¬ ê°œì„ ì‚¬í•­**ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤:

1. âœ… **ASCII/Binary ì…ë ¥ í˜•ì‹ ì²˜ë¦¬**
   - RecordReader ì¶”ìƒí™”
   - BinaryRecordReader ë° AsciiRecordReader êµ¬í˜„
   - ëª…ë ¹í–‰ í”Œë˜ê·¸ (`--ascii` / `--binary`)

2. âœ… **ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤ ê°œì„ **
   - ì—¬ëŸ¬ ì…ë ¥ ë””ë ‰í† ë¦¬ ì§€ì›
   - ì¶”ê°€ ì˜µì…˜ í”Œë˜ê·¸ (`--temp`, `--threads`)
   - ìƒì„¸í•œ CommandLineParser êµ¬í˜„

3. âœ… **Master ì¶œë ¥ í˜•ì‹ í‘œì¤€í™”**
   - stdout: Worker ì£¼ì†Œë§Œ (ì‰¼í‘œ êµ¬ë¶„)
   - stderr: ë¡œê·¸ ë©”ì‹œì§€

4. âœ… **ì¤‘ê°„ íŒŒì¼ ì €ì¥ ìœ„ì¹˜ ì˜µì…˜**
   - `--temp` í”Œë˜ê·¸ ì¶”ê°€
   - ë””ë ‰í† ë¦¬ ì¡´ì¬ ë° ê¶Œí•œ ê²€ì¦

5. âœ… **ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§**
   - ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
   - Keep-alive ë° íƒ€ì„ì•„ì›ƒ ì„¤ì •
   - isRetryable() ì—ëŸ¬ ë¶„ë¥˜

6. âœ… **íŒŒí‹°ì…˜ íŒŒì¼ ë„¤ì´ë° ëª…í™•í™”**
   - `partition.N` í˜•ì‹ ë³´ì¥
   - Atomic write (ì„ì‹œ íŒŒì¼ â†’ rename)

7. âœ… **ìƒ˜í”Œë§ ë¹„ìœ¨ ë™ì  ì¡°ì •**
   - ì…ë ¥ í¬ê¸° ê¸°ë°˜ ê³„ì‚°
   - ìµœì†Œ/ìµœëŒ€ ë¹„ìœ¨ ì œí•œ

8. âœ… **ë””ìŠ¤í¬ ê³µê°„ ì‚¬ì „ í™•ì¸**
   - ensureSufficientDiskSpace() í•¨ìˆ˜
   - ì‘ì—… ì‹œì‘ ì „ ê²€ì¦

### 16.2 êµ¬í˜„ ì™„ì„±ë„

**Version 1 ëŒ€ë¹„ ê°œì„ :**
- **ì •í™•ì„±**: PDF ëª…ì„¸ì„œ ìš”êµ¬ì‚¬í•­ 100% ë°˜ì˜
- **ê²¬ê³ ì„±**: ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ë¡œì§ ê°•í™”
- **ì‚¬ìš©ì„±**: ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤ ê°œì„ 
- **í™•ì¥ì„±**: ë™ì  íŒŒë¼ë¯¸í„° ì¡°ì •
- **ë””ë²„ê¹…**: stdout/stderr ë¶„ë¦¬, ìƒì„¸í•œ ë¡œê¹…

**ì„±ê³µ ê¸°ì¤€:**
- âœ… ì •í™•ì„±: valsort í†µê³¼ (ASCII/Binary ëª¨ë‘)
- âœ… ì¥ì•  í—ˆìš©: Worker ì¬ì‹œì‘ í›„ ì •ìƒ ì™„ë£Œ
- âœ… ì„±ëŠ¥: ë‹¨ì¼ ë¨¸ì‹  ëŒ€ë¹„ ìœ ì˜ë¯¸í•œ ì†ë„ í–¥ìƒ
- âœ… ì½”ë“œ í’ˆì§ˆ: ê¹”ë”í•˜ê³  ìœ ì§€ë³´ìˆ˜ ê°€ëŠ¥í•œ êµ¬ì¡°
- âœ… ëª…ì„¸ ì¤€ìˆ˜: PDF ìš”êµ¬ì‚¬í•­ 100% ì¶©ì¡±

### 16.3 ë‹¤ìŒ ë‹¨ê³„

1. **Milestone 1 êµ¬í˜„ ì‹œì‘**
   - RecordReader ë° CommandLineParser ìš°ì„  êµ¬í˜„
   - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
   - ASCII/Binary í˜•ì‹ ê²€ì¦

2. **í”„ë¡œí† íƒ€ì… ê²€ì¦**
   - ì‘ì€ ë°ì´í„°ë¡œ E2E í…ŒìŠ¤íŠ¸
   - ASCII ë° Binary ëª¨ë‘ í…ŒìŠ¤íŠ¸
   - stdout/stderr ì¶œë ¥ ê²€ì¦

3. **ì ì§„ì  í™•ì¥**
   - ì¥ì•  í—ˆìš©ì„± ì¶”ê°€
   - ì„±ëŠ¥ ìµœì í™”
   - ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸

---

**ë¬¸ì„œ ë²„ì „:** 2.0
**ìµœì¢… ìˆ˜ì •:** 2025-10-24
**ì‘ì„±ì:** Project Team
**ë³€ê²½ ì´ë ¥:**
- v1.0 (2025-10-24): ì´ˆê¸° ì„¤ê³„
- v2.0 (2025-10-24): PDF ëª…ì„¸ì„œ ìš”êµ¬ì‚¬í•­ ë°˜ì˜, 8ê°€ì§€ í•µì‹¬ ê°œì„ ì‚¬í•­ ì¶”ê°€
