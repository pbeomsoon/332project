# Implementation Guide

**ì‘ì„±ì¼**: 2025-10-26
**ëª©ì **: ì‹¤ì œ êµ¬í˜„í•´ì•¼ í•  ë¶€ë¶„ë“¤ì„ ëª…í™•í•˜ê²Œ ì •ë¦¬
**ì°¸ì¡°**: project.sorting.2025.pdf, summary.md

---

## ğŸ“‹ ìš”êµ¬ì‚¬í•­ ìš”ì•½

### ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
```bash
# Master
master <N>

# Worker
worker <master_address> -I <dir1> <dir2> ... -O <output_dir>
```

### í•„ìˆ˜ ê¸°ëŠ¥
1. âœ… **ë¶„ì‚° ì •ë ¬**: Nê°œ workerì— ë¶„ì‚°ëœ ë°ì´í„°ë¥¼ ì „ì—­ ì •ë ¬
2. âœ… **ë©€í‹°ì½”ì–´ í™œìš©**: ì •ë ¬/ë³‘í•© ë‹¨ê³„ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
3. âœ… **Fault Tolerance**: Worker í¬ë˜ì‹œ í›„ ì¬ì‹œì‘ ì‹œ ì •ìƒ ë™ì‘
4. âœ… **ASCII/Binary ìë™ ê°ì§€**: ëª…ë ¹í–‰ ì˜µì…˜ ì—†ì´ ìë™ íŒë³„
5. âœ… **í¬íŠ¸ ë™ì  í• ë‹¹**: í•˜ë“œì½”ë”© ê¸ˆì§€

### ì¶œë ¥ í˜•ì‹
```
Line 1: <Master IP>:<Master Port>
Line 2: <Worker1 IP>
Line 3: <Worker2 IP>
...
Line N+1: <WorkerN IP>
```

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì¡°

### Phase 0: Initialization
```
Master:
  1. ëœë¤ í¬íŠ¸ ë°”ì¸ë”©
  2. Nê°œ Worker ë“±ë¡ ëŒ€ê¸°
  3. Worker ìˆœì„œ í™•ì •
  4. stdoutìœ¼ë¡œ Master IP:Port ì¶œë ¥

Worker:
  1. ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº” (ì¬ê·€ì )
  2. Masterì— ì—°ê²°
  3. ë“±ë¡ ë° index ìˆ˜ì‹ 
```

### Phase 1: Sampling
```
Master:
  1. ê° Workerë¡œë¶€í„° ìƒ˜í”Œ ìˆ˜ì‹ 
  2. ëª¨ë“  ìƒ˜í”Œ ì •ë ¬
  3. numPartitions-1 ê°œì˜ ê²½ê³„ ê³„ì‚°
  4. ê²½ê³„ + shuffleMapì„ ëª¨ë“  Workerì— ë¸Œë¡œë“œìºìŠ¤íŠ¸

Worker:
  1. ì…ë ¥ íŒŒì¼ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ (ë™ì  ìƒ˜í”Œë§ ë¹„ìœ¨)
  2. Masterì— ìƒ˜í”Œ ì „ì†¡
  3. ê²½ê³„ + shuffleMap ìˆ˜ì‹ 
```

### Phase 2: Sort & Partition
```
Worker:
  1. ì…ë ¥ ë¸”ë¡ë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì •ë ¬ (ë³‘ë ¬)
  2. ì •ë ¬ëœ ì²­í¬ë¥¼ íŒŒí‹°ì…˜ ê²½ê³„ì— ë”°ë¼ ë¶„í• 
  3. numPartitions ê°œì˜ íŒŒí‹°ì…˜ íŒŒì¼ ìƒì„±
```

### Phase 3: Shuffle
```
Worker:
  1. shuffleMap í™•ì¸
  2. ê° íŒŒí‹°ì…˜ì„ ë‹´ë‹¹ Workerì— ì „ì†¡ (ë³‘ë ¬)
  3. ìì‹ ì´ ë‹´ë‹¹í•œ íŒŒí‹°ì…˜ë“¤ ìˆ˜ì‹ 
  4. ì„ì‹œ íŒŒì¼ ì •ë¦¬
```

### Phase 4: Merge
```
Worker:
  1. ìˆ˜ì‹ í•œ íŒŒí‹°ì…˜ë“¤ì„ ê·¸ë£¹í™”
  2. ê° ê·¸ë£¹ì„ K-way merge (ë³‘ë ¬)
  3. ìµœì¢… ì¶œë ¥ íŒŒì¼ ìƒì„±: partition.0, partition.3, partition.6 ë“±
  4. ì„ì‹œ íŒŒì¼ ì •ë¦¬

Master:
  1. ëª¨ë“  Worker ì™„ë£Œ í™•ì¸
  2. Worker ìˆœì„œ ì¶œë ¥ (stdout)
```

---

## ğŸ“¦ êµ¬í˜„ ëª¨ë“ˆ

### 1. Main & CLI Parser

**íŒŒì¼**: `src/main/scala/distsort/Main.scala`

**ì°¸ì¡°**:
- `docs/0-implementation-decisions.md` Section 6 (ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤)

```scala
object Main {
  def main(args: Array[String]): Unit = {
    args(0) match {
      case "master" =>
        val numWorkers = args(1).toInt
        MasterMain.run(numWorkers)

      case "worker" =>
        val masterAddr = args(1)
        val (inputDirs, outputDir) = parseWorkerArgs(args.drop(2))
        WorkerMain.run(masterAddr, inputDirs, outputDir)

      case _ =>
        printUsage()
        sys.exit(1)
    }
  }

  private def parseWorkerArgs(args: Array[String]): (Seq[String], String) = {
    var inputDirs = Seq.empty[String]
    var outputDir = ""
    var i = 0

    while (i < args.length) {
      args(i) match {
        case "-I" =>
          // ë‹¤ìŒ ì¸ìë“¤ì„ inputDirsì— ì¶”ê°€ (-O ë‚˜ì˜¬ ë•Œê¹Œì§€)
          i += 1
          while (i < args.length && args(i) != "-O") {
            inputDirs = inputDirs :+ args(i)
            i += 1
          }

        case "-O" =>
          outputDir = args(i + 1)
          i += 2

        case _ =>
          i += 1
      }
    }

    (inputDirs, outputDir)
  }
}
```

**êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ëª…ë ¹í–‰ íŒŒì‹± (master/worker êµ¬ë¶„)
- [ ] Worker ì¸ì íŒŒì‹± (-I ì—¬ëŸ¬ ë””ë ‰í† ë¦¬, -O í•˜ë‚˜)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ (ì˜ëª»ëœ ì¸ì)
- [ ] Usage ë©”ì‹œì§€

---

### 2. Master Service

**íŒŒì¼**: `src/main/scala/distsort/master/MasterServer.scala`

**ì°¸ì¡°**:
- `docs/1-phase-coordination.md` (Phase ë™ê¸°í™” ë©”ì»¤ë‹ˆì¦˜)
- `docs/3-grpc-sequences.md` (Master-Worker gRPC ì‹œí€€ìŠ¤)
- `docs/4-error-recovery.md` (Worker ì¥ì•  ê°ì§€ ë° ì²˜ë¦¬)

```scala
class MasterServer(numWorkers: Int) extends MasterServiceGrpc.MasterServiceImplBase {
  private val workers = new ConcurrentHashMap[String, WorkerInfo]()
  private val samples = new ConcurrentHashMap[String, SampleData]()
  private val registrationLatch = new CountDownLatch(numWorkers)

  // Worker identity ì¶”ì  (input/output dir â†’ workerId)
  private case class WorkerIdentity(
    inputDirs: Seq[String],
    outputDir: String
  )
  private val workerIdentities = new ConcurrentHashMap[WorkerIdentity, WorkerInfo]()

  // Phase 0: Worker ë“±ë¡ (Re-registration ì§€ì›)
  override def registerWorker(
    request: WorkerInfo,
    responseObserver: StreamObserver[RegistrationResponse]
  ): Unit = synchronized {
    val identity = WorkerIdentity(
      inputDirs = request.inputDirectories.sorted,  // Canonical ordering
      outputDir = request.outputDirectory
    )

    // Check if this is a restarted worker
    val existingWorker = workerIdentities.get(identity)

    if (existingWorker != null) {
      // âœ… Worker ì¬ì‹œì‘ ê°ì§€!
      logger.info(s"Worker ${existingWorker.workerId} restarting " +
        s"(detected by matching input/output dirs)")

      // Update worker connection info (IP/port may have changed)
      val updatedWorker = existingWorker.copy(
        address = request.address,
        port = request.port
      )

      workers.put(existingWorker.workerId, updatedWorker)
      workerIdentities.put(identity, updatedWorker)

      // Return same worker ID and index
      val response = RegistrationResponse.newBuilder()
        .setSuccess(true)
        .setWorkerId(existingWorker.workerId)
        .setWorkerIndex(existingWorker.workerIndex)
        .setMessage("Worker re-registered after restart")
        .build()

      responseObserver.onNext(response)
      responseObserver.onCompleted()

      logger.info(s"Re-registered worker ${existingWorker.workerId} " +
        s"with index ${existingWorker.workerIndex}")

    } else {
      // New worker registration
      val workerIndex = workers.size()
      val workerInfo = request.copy(workerIndex = workerIndex)

      workers.put(request.workerId, workerInfo)
      workerIdentities.put(identity, workerInfo)
      registrationLatch.countDown()

      val response = RegistrationResponse.newBuilder()
        .setSuccess(true)
        .setWorkerId(request.workerId)
        .setWorkerIndex(workerIndex)
        .setMessage("Registration successful")
        .build()

      responseObserver.onNext(response)
      responseObserver.onCompleted()

      logger.info(s"New worker registered: ${request.workerId} " +
        s"with index $workerIndex (${workers.size()}/$numWorkers)")
    }
  }

  // Phase 1: ìƒ˜í”Œ ìˆ˜ì§‘
  override def sendSample(
    request: SampleData,
    responseObserver: StreamObserver[Ack]
  ): Unit = {
    samples.put(request.workerId, request)

    if (samples.size() == numWorkers) {
      calculateAndBroadcastConfig()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def calculateAndBroadcastConfig(): Unit = {
    // 1. ìƒ˜í”Œ ì •ë ¬
    val allKeys = samples.values().asScala.flatMap(_.keys).toArray
    val sorted = allKeys.sortWith(compareKeys)

    // 2. ê²½ê³„ ê³„ì‚°
    // numPartitions = numWorkers (N â†’ N mapping)
    // ê° workerê°€ ì •í™•íˆ í•˜ë‚˜ì˜ partition ë‹´ë‹¹
    val numPartitions = numWorkers
    val boundaries = (1 until numPartitions).map { i =>
      val idx = (sorted.length * i) / numPartitions
      sorted(idx)
    }

    // 3. shuffleMap ìƒì„± (1:1 mapping)
    // shuffleMap(partitionId) = workerId
    // partition.0 â†’ W0, partition.1 â†’ W1, ...
    val shuffleMap = (0 until numPartitions).map { pid =>
      (pid, pid)
    }.toMap

    // 4. ë¸Œë¡œë“œìºìŠ¤íŠ¸
    val config = PartitionConfig.newBuilder()
      .addAllBoundaries(boundaries)
      .setNumPartitions(numPartitions)
      .putAllShuffleMap(shuffleMap)
      .build()

    workers.values().foreach { worker =>
      val stub = createStub(worker)
      stub.setPartitionBoundaries(config)
    }
  }

  // Phase 4: ì™„ë£Œ ë³´ê³ 
  override def reportCompletion(
    request: CompletionInfo,
    responseObserver: StreamObserver[Ack]
  ): Unit = {
    completions.incrementAndGet()

    if (completions.get() == numWorkers) {
      printFinalOrdering()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def printFinalOrdering(): Unit = {
    val masterAddr = s"$masterHost:$masterPort"
    println(masterAddr)  // Line 1

    workers.values().sortBy(_.workerIndex).foreach { worker =>
      println(worker.host)  // Line 2~N+1
    }
  }
}
```

**êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë„¤íŠ¸ì›Œí¬ ì„œë²„ setup (ëœë¤ í¬íŠ¸)
- [ ] Worker ë“±ë¡ (CountDownLatch)
- [ ] **Worker identity ì¶”ì ** (input/output dir â†’ WorkerInfo)
- [ ] **Re-registration ê°ì§€ ë° ì²˜ë¦¬** (ë™ì¼í•œ input/output â†’ ë™ì¼ worker ID ì¬í• ë‹¹)
- [ ] ìƒ˜í”Œ ìˆ˜ì§‘ ë° ê²½ê³„ ê³„ì‚°
- [ ] shuffleMap ìƒì„± (Nâ†’N, 1:1 mapping)
- [ ] PartitionConfig ë¸Œë¡œë“œìºìŠ¤íŠ¸
- [ ] ì™„ë£Œ ë³´ê³  ì²˜ë¦¬
- [ ] stdout ì¶œë ¥ (Master IP:Port, Worker IPs)

---

### 3. Worker Node

**íŒŒì¼**: `src/main/scala/distsort/worker/WorkerNode.scala`

**ì°¸ì¡°**:
- `docs/2-worker-state-machine.md` (Worker ìƒíƒœ ë¨¸ì‹  ë° ì „í™˜ ë¡œì§)
- `docs/5-file-management.md` (íŒŒì¼ ë ˆì´ì•„ì›ƒ ë° ë””ë ‰í† ë¦¬ êµ¬ì¡°)
- `docs/6-parallelization.md` (ë³‘ë ¬ ì •ë ¬ ë° ë³‘í•© ì „ëµ)

```scala
class WorkerNode(
  masterAddress: String,
  inputDirs: Seq[String],
  outputDir: String
) {
  private val workerId = generateWorkerId()
  private var workerIndex: Int = -1

  def start(): Unit = {
    // Phase 0: ì´ˆê¸°í™”
    val inputFiles = scanInputFiles(inputDirs)
    val totalSize = inputFiles.map(_.length).sum

    registerWithMaster()

    // Phase 1: ìƒ˜í”Œë§
    val samples = extractSamples(inputFiles, totalSize)
    sendSamplesToMaster(samples)
    val config = waitForPartitionConfig()

    // Phase 2: ì •ë ¬ ë° íŒŒí‹°ì…”ë‹
    val partitionFiles = sortAndPartition(inputFiles, config)

    // Phase 3: Shuffle
    val receivedPartitions = shuffle(partitionFiles, config)

    // Phase 4: Merge
    val outputFiles = merge(receivedPartitions, config)

    // ì™„ë£Œ ë³´ê³ 
    reportCompletion(outputFiles)
  }

  private def scanInputFiles(dirs: Seq[String]): Seq[File] = {
    dirs.flatMap { dir =>
      val dirFile = new File(dir)
      scanRecursive(dirFile).filter(_.isFile)
    }
  }

  private def scanRecursive(dir: File): Seq[File] = {
    if (!dir.exists() || !dir.isDirectory) {
      Seq.empty
    } else {
      val files = dir.listFiles()
      files.filter(_.isFile) ++ files.filter(_.isDirectory).flatMap(scanRecursive)
    }
  }

  private def extractSamples(
    files: Seq[File],
    totalSize: Long
  ): Seq[Array[Byte]] = {
    val sampleRate = determineSampleRate(totalSize)  // êµ¬í˜„ì—ì„œ ê²°ì •
    val samples = mutable.ArrayBuffer[Array[Byte]]()
    // Deterministic: ê³ ì • seed ì‚¬ìš© (ì¬ì‹œì‘ ì‹œ ë™ì¼í•œ ìƒ˜í”Œ)
    val random = new Random(workerId.hashCode)

    // Deterministic: íŒŒì¼ì„ ì •ë ¬ëœ ìˆœì„œë¡œ ì²˜ë¦¬
    files.sortBy(_.getAbsolutePath).foreach { file =>
      val format = detectFormat(file)
      val reader = RecordReader.create(format)
      val input = new BufferedInputStream(new FileInputStream(file))

      var record = reader.readRecord(input)
      while (record.isDefined) {
        if (random.nextDouble() < sampleRate) {
          samples += record.get.take(10)  // keyë§Œ
        }
        record = reader.readRecord(input)
      }
      input.close()
    }

    samples.toSeq
  }

  private def sortAndPartition(
    files: Seq[File],
    config: PartitionConfig
  ): Map[Int, File] = {
    // ë³‘ë ¬ ì •ë ¬
    val executor = Executors.newFixedThreadPool(numCores)
    val sortedChunks = files.par.map { file =>
      sortChunk(file)
    }.seq.toSeq

    // íŒŒí‹°ì…”ë‹
    val partitionWriters = (0 until config.numPartitions).map { pid =>
      pid -> new BufferedOutputStream(
        new FileOutputStream(getPartitionFile(pid))
      )
    }.toMap

    sortedChunks.foreach { chunk =>
      val input = new BufferedInputStream(new FileInputStream(chunk))
      val record = new Array[Byte](100)

      while (input.read(record) == 100) {
        val key = record.take(10)
        val pid = findPartition(key, config.boundaries)
        partitionWriters(pid).write(record)
      }
      input.close()
      chunk.delete()
    }

    partitionWriters.values.foreach(_.close())
    executor.shutdown()

    partitionWriters.keys.map { pid =>
      pid -> getPartitionFile(pid)
    }.toMap
  }

  private def shuffle(
    localPartitions: Map[Int, File],
    config: PartitionConfig
  ): Map[Int, Seq[File]] = {
    val receivedPartitions = mutable.Map[Int, mutable.ArrayBuffer[File]]()
    val executor = Executors.newFixedThreadPool(numCores * 2)

    // ìì‹ ì´ ë‹´ë‹¹í•  íŒŒí‹°ì…˜ ëª©ë¡
    val myPartitions = config.shuffleMap
      .filter(_._2 == workerIndex)
      .keys
      .toSet

    // ì „ì†¡ íƒœìŠ¤í¬
    val sendFutures = localPartitions.map { case (pid, file) =>
      Future {
        val targetWorker = config.shuffleMap(pid)
        if (targetWorker == workerIndex) {
          // ë¡œì»¬ ìœ ì§€
          receivedPartitions.getOrElseUpdate(pid, mutable.ArrayBuffer()) += file
        } else {
          // ì „ì†¡
          sendPartition(file, pid, workers(targetWorker))
          file.delete()
        }
      }(ExecutionContext.fromExecutor(executor))
    }

    // ìˆ˜ì‹  ëŒ€ê¸°
    val receiveFutures = myPartitions.flatMap { pid =>
      workers.indices.filter(_ != workerIndex).map { otherWorker =>
        Future {
          val receivedFile = receivePartition(pid)
          receivedPartitions.getOrElseUpdate(pid, mutable.ArrayBuffer()) += receivedFile
        }(ExecutionContext.fromExecutor(executor))
      }
    }

    Await.result(Future.sequence(sendFutures ++ receiveFutures), Duration.Inf)
    executor.shutdown()

    receivedPartitions.map { case (k, v) => (k, v.toSeq) }.toMap
  }

  private def merge(
    partitions: Map[Int, Seq[File]],
    config: PartitionConfig
  ): Seq[File] = {
    val outputFiles = mutable.ArrayBuffer[File]()
    val executor = Executors.newFixedThreadPool(numCores)

    val futures = partitions.toSeq.sortBy(_._1).map { case (pid, files) =>
      Future {
        val outputFile = new File(outputDir, s"partition.$pid")
        kWayMerge(files, outputFile)
        files.foreach(_.delete())
        outputFile
      }(ExecutionContext.fromExecutor(executor))
    }

    val results = Await.result(Future.sequence(futures), Duration.Inf)
    executor.shutdown()

    results
  }

  private def kWayMerge(inputs: Seq[File], output: File): Unit = {
    val readers = inputs.map { f =>
      new BufferedInputStream(new FileInputStream(f))
    }

    val heap = new mutable.PriorityQueue[(Array[Byte], Int)]()(
      Ordering.by[(Array[Byte], Int), Array[Byte]](_._1)(
        Ordering.by[Array[Byte], String](_.take(10).map("%02x".format(_)).mkString).reverse
      )
    )

    // ì´ˆê¸°í™”
    readers.zipWithIndex.foreach { case (reader, idx) =>
      val record = new Array[Byte](100)
      if (reader.read(record) == 100) {
        heap.enqueue((record, idx))
      }
    }

    val writer = new BufferedOutputStream(new FileOutputStream(output))

    while (heap.nonEmpty) {
      val (record, idx) = heap.dequeue()
      writer.write(record)

      val nextRecord = new Array[Byte](100)
      if (readers(idx).read(nextRecord) == 100) {
        heap.enqueue((nextRecord, idx))
      }
    }

    writer.close()
    readers.foreach(_.close())
  }
}
```

**êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì…ë ¥ íŒŒì¼ ì¬ê·€ ìŠ¤ìº” (ì •ë ¬ëœ ìˆœì„œ)
- [ ] Master ë“±ë¡ ë° index ìˆ˜ì‹ 
- [ ] Deterministic ìƒ˜í”Œ ì¶”ì¶œ (ê³ ì • seed)
- [ ] PartitionConfig ìˆ˜ì‹ 
- [ ] ë³‘ë ¬ ì •ë ¬ (ThreadPool)
- [ ] íŒŒí‹°ì…”ë‹ (ê²½ê³„ ê¸°ë°˜)
- [ ] Shuffle ì†¡ìˆ˜ì‹  (ë³‘ë ¬)
- [ ] K-way merge (ë³‘ë ¬, PriorityQueue)
- [ ] ì¶œë ¥ íŒŒì¼ ìƒì„± (partition.n)
- [ ] ì¬ì‹œì‘ ì‹œ cleanup ë° ì²˜ìŒë¶€í„° ì‹¤í–‰

---

### 4. Record Reader

**íŒŒì¼**: `src/main/scala/distsort/common/RecordReader.scala`

**ì°¸ì¡°**:
- `docs/0-implementation-decisions.md` Section 5 (ASCII/Binary í˜•ì‹ ìë™ ê°ì§€)

```scala
sealed trait DataFormat
object DataFormat {
  case object Binary extends DataFormat
  case object Ascii extends DataFormat
}

trait RecordReader {
  def readRecord(input: InputStream): Option[Array[Byte]]
}

object RecordReader {
  def create(format: DataFormat): RecordReader = format match {
    case DataFormat.Binary => new BinaryRecordReader()
    case DataFormat.Ascii => new AsciiRecordReader()
  }
}

class BinaryRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val record = new Array[Byte](100)
    val bytesRead = input.read(record)

    if (bytesRead == 100) Some(record)
    else if (bytesRead == -1) None
    else throw new IOException(s"Incomplete record: $bytesRead bytes")
  }
}

class AsciiRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val line = new Array[Byte](102)  // 10 + 1 + 90 + 1
    val bytesRead = input.read(line)

    if (bytesRead == -1) return None
    if (bytesRead != 102) throw new IOException(s"Invalid ASCII record: $bytesRead bytes")
    if (line(10) != ' '.toByte) throw new IOException("Expected space at position 10")
    if (line(101) != '\n'.toByte) throw new IOException("Expected newline at position 101")

    val record = new Array[Byte](100)
    System.arraycopy(line, 0, record, 0, 10)   // key
    System.arraycopy(line, 11, record, 10, 90) // value

    Some(record)
  }
}

object InputFormatDetector {
  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](1000)
    val input = new FileInputStream(file)

    try {
      val bytesRead = input.read(buffer)

      if (bytesRead <= 0) {
        return DataFormat.Binary
      }

      val asciiCount = buffer.take(bytesRead).count { b =>
        (b >= 32 && b <= 126) || b == '\n' || b == '\r'
      }

      val asciiRatio = asciiCount.toDouble / bytesRead

      if (asciiRatio > 0.9) DataFormat.Ascii else DataFormat.Binary
    } finally {
      input.close()
    }
  }
}
```

**êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] BinaryRecordReader (100ë°”ì´íŠ¸)
- [ ] AsciiRecordReader (102ë°”ì´íŠ¸ â†’ 100ë°”ì´íŠ¸)
- [ ] í˜•ì‹ ìë™ ê°ì§€ (ASCII ë¹„ìœ¨ > 90%)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ (ë¶ˆì™„ì „í•œ ë ˆì½”ë“œ)

---

### 5. Protocol Buffers

**íŒŒì¼**: `src/main/protobuf/distsort.proto`

**ì°¸ì¡°**:
- `docs/3-grpc-sequences.md` (gRPC ë©”ì‹œì§€ ì •ì˜ ë° ì‹œí€€ìŠ¤)

```protobuf
syntax = "proto3";

package distsort;

// Master Service
service MasterService {
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);
  rpc SendSample(SampleData) returns (Ack);
  rpc ReportCompletion(CompletionInfo) returns (Ack);
}

message WorkerInfo {
  string worker_id = 1;
  string host = 2;
  int32 port = 3;
  repeated string input_directories = 4;
  string output_directory = 5;
  int64 total_input_size = 6;
}

message RegistrationResponse {
  bool success = 1;
  string message = 2;
  int32 worker_index = 3;
}

message SampleData {
  string worker_id = 1;
  repeated bytes keys = 2;
}

message CompletionInfo {
  string worker_id = 1;
  repeated int32 partition_ids = 2;
  repeated int64 partition_sizes = 3;
}

message Ack {
  bool success = 1;
  string message = 2;
}

// Worker Service
service WorkerService {
  rpc SetPartitionBoundaries(PartitionConfig) returns (Ack);
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);
}

message PartitionConfig {
  repeated bytes boundaries = 1;
  int32 num_partitions = 2;
  map<int32, int32> shuffle_map = 3;
  repeated WorkerInfo all_workers = 4;
}

message ShuffleDataChunk {
  int32 partition_id = 1;
  bytes data = 2;
}

message ShuffleAck {
  bool success = 1;
  int64 bytes_received = 2;
}
```

**êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸** (ì„ íƒì‚¬í•­ - gRPC ì‚¬ìš© ì‹œ):
- [ ] protobuf ì •ì˜
- [ ] sbt ë¹Œë“œ ì„¤ì • (scalapb)
- [ ] gRPC stub ìƒì„±

---

### 6. Fault Tolerance (PDF page 19 ê¸°ì¤€)

**PDF ìš”êµ¬ì‚¬í•­**: ì¤‘ê°„ ë°ì´í„° ì†ì‹¤ë˜ì–´ë„ ë™ì¼í•œ ì¶œë ¥ ìƒì„±

**êµ¬í˜„ ì „ëµ**: Worker Re-registration (í•´ì„ B)
- Worker crash â†’ ë™ì¼í•œ worker ID ì¬í• ë‹¹
- í•´ë‹¹ Workerê°€ ë‹´ë‹¹í–ˆë˜ partition ì¬ìƒì„±
- ë‹¤ë¥¸ Workerë“¤ì€ ì¤‘ë‹¨ ì—†ì´ ê³„ì† ì‘ì—…

**íŒŒì¼**: `src/main/scala/distsort/worker/FaultTolerance.scala`

**ì°¸ì¡°**:
- `docs/4-error-recovery.md` Section 3.3 Interpretation B (Worker Re-registration ìƒì„¸)

```scala
class WorkerNode(
  masterAddress: String,
  inputDirs: Seq[String],
  outputDir: String
) {
  private val workerId = generateWorkerId()

  def start(): Unit = {
    try {
      // ì¬ì‹œì‘ ì‹œ ì²˜ìŒë¶€í„° ì‹œì‘ (deterministic execution)
      cleanupTemporaryFiles()
      cleanupOutputFiles()

      // Phase 0-4 ì‹¤í–‰ (deterministic)
      val inputFiles = scanInputFiles(inputDirs).sortBy(_.getAbsolutePath)
      performSortingWithSeed(inputFiles, seed = workerId.hashCode)

    } catch {
      case e: Exception =>
        logger.error("Worker failed", e)
        cleanupTemporaryFiles()
        cleanupOutputFiles()
        throw e
    }
  }

  private def cleanupTemporaryFiles(): Unit = {
    val tempDir = new File(System.getProperty("java.io.tmpdir"), s"distsort_$workerId")
    if (tempDir.exists()) {
      FileUtils.deleteRecursively(tempDir)
    }
  }

  private def cleanupOutputFiles(): Unit = {
    val outputDirFile = new File(outputDir)
    if (outputDirFile.exists()) {
      outputDirFile.listFiles().foreach { file =>
        if (file.getName.startsWith("partition.") ||
            file.getName.startsWith(".partition.")) {
          file.delete()
        }
      }
    }
  }
}
```

**êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Deterministic worker ID ìƒì„± (input/output dir ê¸°ë°˜)
- [ ] Masterì—ì„œ Worker identity ì¶”ì 
- [ ] Worker re-registration ì‹œ ë™ì¼í•œ worker ID/index ì¬í• ë‹¹
- [ ] Deterministic sampling (ê³ ì • seed)
- [ ] ì…ë ¥ íŒŒì¼ ì •ë ¬ëœ ìˆœì„œë¡œ ì²˜ë¦¬
- [ ] ì¬ì‹œì‘ ì‹œ cleanup ë° ì²˜ìŒë¶€í„° ì‹¤í–‰
- [ ] ì‹¤íŒ¨ ì‹œ ì •ë¦¬ ë¡œì§

---

## ğŸ”„ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Milestone 1: ê¸°ë³¸ ì¸í”„ë¼ (Week 1-2)
```
Priority 1: í•„ìˆ˜ ê¸°ë°˜
  - [ ] build.sbt ì„¤ì •
  - [ ] Main ë° CLI íŒŒì„œ
  - [ ] Master/Worker ìŠ¤ì¼ˆë ˆí†¤

Priority 2: í†µì‹ 
  - [ ] ë„¤íŠ¸ì›Œí¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ (gRPC/Netty/java.net)
  - [ ] ë„¤íŠ¸ì›Œí¬ ì„œë²„ setup (Master)
  - [ ] Worker ë“±ë¡
  - [ ] ê°„ë‹¨í•œ ë©”ì‹œì§€ ì†¡ìˆ˜ì‹  í…ŒìŠ¤íŠ¸
```

### Milestone 2: ê¸°ë³¸ ì •ë ¬ (Week 3-4)
```
Priority 1: ë°ì´í„° ì²˜ë¦¬
  - [ ] RecordReader (Binary/ASCII)
  - [ ] í˜•ì‹ ìë™ ê°ì§€
  - [ ] ì…ë ¥ íŒŒì¼ ìŠ¤ìº”

Priority 2: ìƒ˜í”Œë§
  - [ ] ìƒ˜í”Œ ì¶”ì¶œ
  - [ ] ê²½ê³„ ê³„ì‚°
  - [ ] PartitionConfig ë¸Œë¡œë“œìºìŠ¤íŠ¸

Priority 3: ì •ë ¬/íŒŒí‹°ì…”ë‹
  - [ ] ì²­í¬ ì •ë ¬
  - [ ] íŒŒí‹°ì…”ë‹ (ê²½ê³„ ê¸°ë°˜)
  - [ ] shuffleMap ìƒì„± (Nâ†’M)

Priority 4: Shuffle/Merge
  - [ ] ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (íŒŒí‹°ì…˜ ì „ì†¡)
  - [ ] K-way merge
  - [ ] ì¶œë ¥ íŒŒì¼ ìƒì„±
```

### Milestone 3: ë³‘ë ¬í™” (Week 5)
```
Priority 1: ë³‘ë ¬ ì •ë ¬
  - [ ] ThreadPool (ì •ë ¬)
  - [ ] ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬

Priority 2: ë³‘ë ¬ Shuffle
  - [ ] ThreadPool (ì „ì†¡/ìˆ˜ì‹ )
  - [ ] ë™ì‹œì„± ì œì–´

Priority 3: ë³‘ë ¬ Merge
  - [ ] ThreadPool (ë³‘í•©)
  - [ ] íŒŒí‹°ì…˜ë³„ ë…ë¦½ ë³‘í•©
```

### Milestone 4: Fault Tolerance (Week 6) - Worker Re-registration
```
Priority 1: Worker Identity & Re-registration
  - [ ] Deterministic worker ID ìƒì„± (input/output dir ê¸°ë°˜)
  - [ ] Masterì—ì„œ WorkerIdentity ì¶”ì 
  - [ ] Re-registration ì‹œ ë™ì¼í•œ worker ID/index ì¬í• ë‹¹
  - [ ] Worker ì—°ê²° ì •ë³´ ì—…ë°ì´íŠ¸ (IP/port)

Priority 2: Deterministic Execution (PDF page 19)
  - [ ] Deterministic sampling (ê³ ì • seed = workerID.hashCode)
  - [ ] ì…ë ¥ íŒŒì¼ ì •ë ¬ëœ ìˆœì„œë¡œ ì²˜ë¦¬
  - [ ] ì‹œì‘ ì‹œ cleanup (ì„ì‹œ/ì¶œë ¥ íŒŒì¼)
  - [ ] ì‹¤íŒ¨ ì‹œ cleanup

Priority 3: ì¬ì‹œì‘ í…ŒìŠ¤íŠ¸
  - [ ] Worker í¬ë˜ì‹œ ì‹œë®¬ë ˆì´ì…˜ (ê° Phaseë³„)
  - [ ] ì¬ì‹œì‘ í›„ ë™ì¼í•œ worker ID ì¬í• ë‹¹ í™•ì¸
  - [ ] ì¬ì‹œì‘ëœ Workerê°€ ë™ì¼í•œ partition ë‹´ë‹¹ í™•ì¸
  - [ ] ìµœì¢… ì¶œë ¥ ì •í™•ì„± í™•ì¸ (valsort)
  - [ ] ë‹¤ë¥¸ Workerë“¤ì´ ì¤‘ë‹¨ ì—†ì´ ê³„ì† ì‘ì—…í•˜ëŠ”ì§€ í™•ì¸
```

### Milestone 5: í…ŒìŠ¤íŠ¸ & ìµœì í™” (Week 7-8)
```
Priority 1: ì •í™•ì„± í…ŒìŠ¤íŠ¸
  - [ ] valsort ê²€ì¦
  - [ ] ASCII/Binary ëª¨ë‘ í…ŒìŠ¤íŠ¸

Priority 2: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
  - [ ] ëŒ€ìš©ëŸ‰ ë°ì´í„° (50GB+)
  - [ ] Worker ìˆ˜ ì¦ê°€ ì‹œ ì„±ëŠ¥ ì¸¡ì •

Priority 3: ìµœì í™”
  - [ ] I/O ë²„í¼ë§
  - [ ] Thread pool ì¡°ì •
```

---

## âœ… í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
```
ì…ë ¥/ì¶œë ¥:
  - [ ] ASCII ì…ë ¥ â†’ ì •ë ¬ëœ ì¶œë ¥
  - [ ] Binary ì…ë ¥ â†’ ì •ë ¬ëœ ì¶œë ¥
  - [ ] í˜¼í•© ì…ë ¥ (ASCII + Binary)
  - [ ] ì—¬ëŸ¬ ì…ë ¥ ë””ë ‰í† ë¦¬
  - [ ] ì¬ê·€ ë””ë ‰í† ë¦¬ ìŠ¤ìº”

ì •ë ¬:
  - [ ] ì‘ì€ ë°ì´í„° (1GB)
  - [ ] ì¤‘ê°„ ë°ì´í„° (10GB)
  - [ ] í° ë°ì´í„° (50GB+)
  - [ ] valsort ê²€ì¦ í†µê³¼

ë¶„ì‚°:
  - [ ] 3 workers
  - [ ] 10 workers
  - [ ] 20 workers

Fault Tolerance (PDF page 19) - Worker Re-registration:
  - [ ] Worker í¬ë˜ì‹œ í›„ ì¬ì‹œì‘
  - [ ] ì¬ì‹œì‘ ì‹œ ë™ì¼í•œ worker ID ì¬í• ë‹¹ í™•ì¸
  - [ ] ì¬ì‹œì‘ëœ Workerê°€ ë™ì¼í•œ partition ë‹´ë‹¹ í™•ì¸
  - [ ] ì¬ì‹œì‘ í›„ ë™ì¼í•œ ì¶œë ¥ ìƒì„± í™•ì¸
  - [ ] ì¤‘ê°„ ë°ì´í„° ì†ì‹¤ ì‹œë‚˜ë¦¬ì˜¤ (ëª¨ë“  ì„ì‹œ íŒŒì¼ ì‚­ì œ)
  - [ ] Deterministic execution ê²€ì¦ (ë™ì¼ input â†’ ë™ì¼ output)
  - [ ] ë‹¤ë¥¸ Workerë“¤ì´ ì¤‘ë‹¨ ì—†ì´ ê³„ì† ì‘ì—…í•˜ëŠ”ì§€ í™•ì¸
```

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```
ë³‘ë ¬ì„±:
  - [ ] ë‹¨ì¼ ìŠ¤ë ˆë“œ vs ë©€í‹° ìŠ¤ë ˆë“œ
  - [ ] Thread pool í¬ê¸° ì˜í–¥
  - [ ] CPU ì‚¬ìš©ë¥ 

ì²˜ë¦¬ëŸ‰:
  - [ ] ì´ˆë‹¹ ì²˜ë¦¬ ë ˆì½”ë“œ ìˆ˜
  - [ ] MB/s throughput
  - [ ] Worker ìˆ˜ ì¦ê°€ ì‹œ ìŠ¤ì¼€ì¼ë§

ë¹„êµ:
  - [ ] ë‹¨ì¼ ë¨¸ì‹  ì •ë ¬ vs ë¶„ì‚° ì •ë ¬
  - [ ] ì†ë„ í–¥ìƒ ì¸¡ì •
```

### ì¶œë ¥ ê²€ì¦
```
Master ì¶œë ¥:
  - [ ] Line 1: Master IP:Port
  - [ ] Line 2~N+1: Worker IPs
  - [ ] stdoutìœ¼ë¡œë§Œ ì¶œë ¥ (stderr ì œì™¸)

Worker ì¶œë ¥:
  - [ ] partition.n íŒŒì¼ ìƒì„±
  - [ ] íŒŒì¼ ë²ˆí˜¸ ìˆœì„œ í™•ì¸
  - [ ] ì „ì—­ ì •ë ¬ í™•ì¸
  - [ ] ì„ì‹œ íŒŒì¼ ì •ë¦¬ í™•ì¸
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project_2025/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”‚   â”œâ”€â”€ build.properties
â”‚   â””â”€â”€ plugins.sbt (ì„ íƒ: gRPC ì‚¬ìš© ì‹œ)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ scala/
â”‚       â”‚   â””â”€â”€ distsort/
â”‚       â”‚       â”œâ”€â”€ Main.scala
â”‚       â”‚       â”œâ”€â”€ master/
â”‚       â”‚       â”‚   â”œâ”€â”€ MasterServer.scala
â”‚       â”‚       â”‚   â””â”€â”€ MasterMain.scala
â”‚       â”‚       â”œâ”€â”€ worker/
â”‚       â”‚       â”‚   â”œâ”€â”€ WorkerNode.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ WorkerMain.scala
â”‚       â”‚       â”‚   â””â”€â”€ FaultTolerance.scala
â”‚       â”‚       â””â”€â”€ common/
â”‚       â”‚           â”œâ”€â”€ RecordReader.scala
â”‚       â”‚           â”œâ”€â”€ BinaryRecordReader.scala
â”‚       â”‚           â”œâ”€â”€ AsciiRecordReader.scala
â”‚       â”‚           â”œâ”€â”€ InputFormatDetector.scala
â”‚       â”‚           â””â”€â”€ FileUtils.scala
â”‚       â””â”€â”€ protobuf/ (ì„ íƒ: gRPC ì‚¬ìš© ì‹œ)
â”‚           â””â”€â”€ distsort.proto
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 0-implementation-decisions.md
â”‚   â”œâ”€â”€ 1-phase-coordination.md
â”‚   â”œâ”€â”€ 2-worker-state-machine.md
â”‚   â””â”€â”€ ... (ê¸°íƒ€ ì„¤ê³„ ë¬¸ì„œ)
â””â”€â”€ plan/
    â”œâ”€â”€ 2025-10-24_plan_ver3.md
    â”œâ”€â”€ requirements_verification.md
    â””â”€â”€ implementation_guide.md (ì´ ë¬¸ì„œ)
```

---

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. ê°œë°œ í™˜ê²½ ì„¤ì •
```bash
# Scala, sbt ì„¤ì¹˜ í™•ì¸
scala -version  # 2.13.x
sbt -version    # 1.9.x

# ë„¤íŠ¸ì›Œí¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± í™•ì¸ (build.sbtì— ì¶”ê°€)
```

### 2. ì²« ë²ˆì§¸ êµ¬í˜„
```bash
# 1. í”„ë¡œì íŠ¸ ì»´íŒŒì¼
sbt compile

# 2. Master ì‹¤í–‰ í…ŒìŠ¤íŠ¸
sbt "runMain distsort.Main master 3"

# 3. Worker ì‹¤í–‰ í…ŒìŠ¤íŠ¸
sbt "runMain distsort.Main worker 127.0.0.1:50051 -I /tmp/input -O /tmp/output"
```

### 3. ë””ë²„ê¹…
```bash
# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
export LOG_LEVEL=DEBUG

# verbose mode
sbt "runMain distsort.Main master 3" 2>&1 | tee master.log
```

---

## ğŸ” PDF ê²€ì¦ ê²°ê³¼ (2025-10-26)

### ê²€ì¦ ì™„ë£Œ: PDF ëª…ì‹œ ìš”êµ¬ì‚¬í•­ë§Œ ë°˜ì˜

ë³¸ êµ¬í˜„ ê°€ì´ë“œëŠ” `project.sorting.2025.pdf`ì˜ **ëª…ì‹œì  ìš”êµ¬ì‚¬í•­ë§Œ** ì •í™•íˆ ë°˜ì˜í•©ë‹ˆë‹¤.

### âœ… PDF ëª…ì‹œ ìš”êµ¬ì‚¬í•­ (ì™„ì „ ë°˜ì˜)

1. **ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤** (PDF Section 2.1, 2.2)
   - `master <N>` í˜•ì‹
   - `worker <master_address> -I <dir1> <dir2> ... -O <output_dir>` í˜•ì‹
   - ì—¬ëŸ¬ ì…ë ¥ ë””ë ‰í† ë¦¬ ì§€ì›

2. **í•„ìˆ˜ ê¸°ëŠ¥** (PDF Section 3)
   - ë¶„ì‚° ì •ë ¬ (Nê°œ ì›Œì»¤)
   - ë©€í‹°ì½”ì–´ í™œìš© (ë³‘ë ¬ ì²˜ë¦¬)
   - **Fault Tolerance** (PDF page 19)
   - ASCII/Binary ìë™ ê°ì§€ (PDF page 5)
   - í¬íŠ¸ ë™ì  í• ë‹¹

3. **ì¶œë ¥ í˜•ì‹** (PDF Section 2.3)
   - Line 1: `<Master IP>:<Master Port>`
   - Line 2~N+1: Worker IPs

4. **ë°ì´í„° í˜•ì‹** (PDF Section 2.1.1)
   - Binary: 100ë°”ì´íŠ¸ ë ˆì½”ë“œ (10 key + 90 value)
   - ASCII: 102ë°”ì´íŠ¸ â†’ 100ë°”ì´íŠ¸ ë³€í™˜
   - Key ë¹„êµ: unsigned byte ê¸°ì¤€

5. **íŒŒì¼ ì²˜ë¦¬** (PDF Section 2.1.2)
   - ì¬ê·€ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
   - ì…ë ¥ ë””ë ‰í† ë¦¬: Read-only
   - ì¶œë ¥ ë””ë ‰í† ë¦¬: ìµœì¢… ê²°ê³¼ë§Œ (ì„ì‹œ íŒŒì¼ ì •ë¦¬)

---

## âš ï¸ PDFì—ì„œ ëª…ì‹œí•˜ì§€ ì•Šì€ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

ë‹¤ìŒ í•­ëª©ë“¤ì€ **PDFì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ìš”êµ¬í•˜ì§€ ì•ŠìŒ**. êµ¬í˜„ ì‹œ ì„ íƒì‚¬í•­:

1. **í†µì‹  ë°©ì‹** (PDF page 26)
   - gRPC: "**recommended**" (ê¶Œì¥ì‚¬í•­, í•„ìˆ˜ ì•„ë‹˜)
   - Netty, java.net.* ëª¨ë‘ í—ˆìš©
   - êµ¬í˜„ íŒ€ì´ ì„ íƒ

2. **Worker ë“±ë¡ íƒ€ì„ì•„ì›ƒ ê°’**
   - PDFì— ëª…ì‹œ ì—†ìŒ
   - ì ì ˆí•œ ê°’ì„ êµ¬í˜„ì—ì„œ ê²°ì •

3. **ìƒ˜í”Œë§ ë¹„ìœ¨**
   - PDFì— ëª…ì‹œ ì—†ìŒ
   - ë™ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥ (ë°ì´í„° í¬ê¸° ê¸°ë°˜)

4. **íŒŒí‹°ì…˜ ê°œìˆ˜**
   - PDFì— ëª…ì‹œ ì—†ìŒ
   - **êµ¬í˜„ ê²°ì •**: numPartitions = numWorkers (N â†’ N mapping)
   - ê° workerê°€ ì •í™•íˆ í•˜ë‚˜ì˜ partition ë‹´ë‹¹
   - shuffleMap: partition.i â†’ Worker i

---

## ğŸ› ï¸ Fault Tolerance êµ¬í˜„ (PDF page 19 ê¸°ì¤€)

**PDF ìš”êµ¬ì‚¬í•­**:
```
Scenario:
- A worker process crashes in the middle of execution
- All its intermediate data is lost  â¬…ï¸ ì¤‘ìš”!
- A new worker starts on the same node (using the same parameters)

Expectation:
- The new worker should generate the same output expected of the initial worker
```

**í•µì‹¬**: ì¤‘ê°„ ë°ì´í„° ì†ì‹¤ë˜ì–´ë„ **ë™ì¼í•œ ê²°ê³¼** ìƒì„± í•„ìš”

### êµ¬í˜„ ì „ëµ: Worker Re-registration (í•´ì„ B)

**í•µì‹¬ ì•„ì´ë””ì–´**:
1. Workerê°€ **input/output ë””ë ‰í† ë¦¬ ê¸°ë°˜**ìœ¼ë¡œ deterministic ID ìƒì„±
2. Masterê°€ **ë™ì¼í•œ input/output ì¡°í•©**ì„ ê°ì§€í•˜ë©´ ë™ì¼í•œ worker ID ì¬í• ë‹¹
3. ì¬ì‹œì‘ëœ Workerê°€ **ì›ë˜ ë‹´ë‹¹í–ˆë˜ partitionì„ ë‹¤ì‹œ ìƒì„±**
4. **ë‹¤ë¥¸ Workerë“¤ì€ ì¤‘ë‹¨ ì—†ì´** ê³„ì† ì‘ì—…

**ì¥ì **:
- âœ… Partial recovery: í•œ Workerë§Œ ì¬ì‹œì‘, ë‚˜ë¨¸ì§€ëŠ” ê³„ì† ì‘ì—…
- âœ… ì‹œê°„ ì ˆì•½: ì „ì²´ job ì¬ì‹œì‘ ë¶ˆí•„ìš”
- âœ… Worker-partition consistency: Worker i â†’ partition.i ìœ ì§€

**ë‹¨ì **:
- âš ï¸ êµ¬í˜„ ë³µì¡ë„ ì¦ê°€
- âš ï¸ Deterministic execution í•„ìˆ˜

### Worker Re-registration êµ¬í˜„

**íŒŒì¼**: `src/main/scala/distsort/worker/FaultTolerance.scala`

```scala
class WorkerNode(
  masterAddress: String,
  inputDirs: Seq[String],
  outputDir: String
) {
  // Deterministic worker ID based on input/output directories
  // ì¬ì‹œì‘ ì‹œì—ë„ ë™ì¼í•œ ID ìƒì„± ë³´ì¥
  private val workerId = generateDeterministicWorkerId()
  private var workerIndex: Int = -1

  /**
   * Worker identity ìƒì„± (input/output dir ê¸°ë°˜)
   * ë™ì¼í•œ input/output dir â†’ ë™ì¼í•œ worker ID
   */
  private def generateDeterministicWorkerId(): String = {
    val identity = inputDirs.sorted.mkString("/") + "::" + outputDir
    val hash = identity.hashCode.abs
    s"W$hash"
  }

  def start(): Unit = {
    try {
      // ì¬ì‹œì‘ ì‹œ cleanup
      cleanupTemporaryFiles()
      cleanupOutputFiles()

      // Phase 0: ì´ˆê¸°í™”
      val inputFiles = scanInputFiles(inputDirs)
      val totalSize = inputFiles.map(_.length).sum
      registerWithMaster()  // ë™ì¼í•œ index ì¬í• ë‹¹ë¨

      // Phase 1: ìƒ˜í”Œë§ (deterministic)
      val samples = extractSamplesWithSeed(inputFiles, totalSize, seed = workerId.hashCode)
      sendSamplesToMaster(samples)
      val config = waitForPartitionConfig()

      // Phase 2: ì •ë ¬ ë° íŒŒí‹°ì…”ë‹ (deterministic)
      val partitionFiles = sortAndPartition(inputFiles, config)

      // Phase 3: Shuffle
      val receivedPartitions = shuffle(partitionFiles, config)

      // Phase 4: Merge
      val outputFiles = merge(receivedPartitions, config)

      // ì™„ë£Œ ë³´ê³ 
      reportCompletion(outputFiles)

    } catch {
      case e: Exception =>
        logger.error("Worker failed", e)
        cleanupTemporaryFiles()
        cleanupOutputFiles()
        throw e
    }
  }

  /**
   * Deterministic ìƒ˜í”Œë§ (ì¬ì‹œì‘ ì‹œ ë™ì¼í•œ ìƒ˜í”Œ ìƒì„±)
   */
  private def extractSamplesWithSeed(
    files: Seq[File],
    totalSize: Long,
    seed: Int
  ): Seq[Array[Byte]] = {
    val sampleRate = determineSampleRate(totalSize)  // êµ¬í˜„ì—ì„œ ê²°ì •
    val samples = mutable.ArrayBuffer[Array[Byte]]()
    val random = new Random(seed)  // ê³ ì • seed ì‚¬ìš©

    files.sortBy(_.getAbsolutePath).foreach { file =>  // ì •ë ¬ëœ ìˆœì„œ
      val format = detectFormat(file)
      val reader = RecordReader.create(format)
      val input = new BufferedInputStream(new FileInputStream(file))

      var record = reader.readRecord(input)
      while (record.isDefined) {
        if (random.nextDouble() < sampleRate) {
          samples += record.get.take(10)
        }
        record = reader.readRecord(input)
      }
      input.close()
    }

    samples.toSeq
  }

  private def cleanupTemporaryFiles(): Unit = {
    val tempDir = new File(System.getProperty("java.io.tmpdir"), s"distsort_$workerId")
    if (tempDir.exists()) {
      FileUtils.deleteRecursively(tempDir)
    }
  }

  private def cleanupOutputFiles(): Unit = {
    val outputDirFile = new File(outputDir)
    if (outputDirFile.exists()) {
      outputDirFile.listFiles().foreach { file =>
        if (file.getName.startsWith("partition.") ||
            file.getName.startsWith(".partition.")) {
          file.delete()
        }
      }
    }
  }
}
```

**í•µì‹¬ í¬ì¸íŠ¸**:
1. âœ… **Deterministic worker ID** (input/output dir ê¸°ë°˜, ì¬ì‹œì‘ ì‹œ ë™ì¼)
2. âœ… **Masterê°€ re-registration ê°ì§€** (ë™ì¼í•œ input/output â†’ ë™ì¼í•œ worker ID ì¬í• ë‹¹)
3. âŒ **ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—†ìŒ** (PDF: "intermediate data is lost")
4. âœ… **Deterministic execution** (ê°™ì€ input â†’ ê°™ì€ output)
5. âœ… **ê³ ì • seed ì‚¬ìš©** (seed = workerID.hashCode)
6. âœ… **ì¬ì‹œì‘ ì‹œ ì²˜ìŒë¶€í„°** (Phase 0ë¶€í„° ë‹¤ì‹œ ì‹œì‘)
7. âœ… **ì…ë ¥ì€ read-only** (ë³€ê²½ ì—†ìŒ)
8. âœ… **Worker-partition consistency** (ë™ì¼í•œ WorkerëŠ” ë™ì¼í•œ partition ë‹´ë‹¹)

### Phaseë³„ ë³µêµ¬ ì „ëµ

**Phaseë³„ Worker crash ì‹œë‚˜ë¦¬ì˜¤**:

```
Phase 1 (Sampling):
  - Worker W1 crash
  - W1 ì¬ì‹œì‘ â†’ ë™ì¼í•œ worker ID/index ì¬í• ë‹¹
  - W1ì´ ìƒ˜í”Œë§ ë‹¤ì‹œ ìˆ˜í–‰ (deterministic seed)
  - MasterëŠ” W1ì˜ ìƒ˜í”Œ ë‹¤ì‹œ ìˆ˜ì‹ 
  - ëª¨ë“  Workerì˜ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ í›„ ê²½ê³„ ê³„ì‚°

Phase 2-3 (Sorting & Partitioning):
  - Worker W1 crash
  - W1 ì¬ì‹œì‘ â†’ ë™ì¼í•œ worker ID/index ì¬í• ë‹¹
  - W1ì´ ì •ë ¬/íŒŒí‹°ì…”ë‹ ë‹¤ì‹œ ìˆ˜í–‰
  - ë‹¤ë¥¸ Workerë“¤ì€ ì¤‘ë‹¨ ì—†ì´ ê³„ì† ì‘ì—…

Phase 4 (Shuffle):
  - Worker W1 crash (partition.1 ì „ì†¡ ì¤‘)
  - W1 ì¬ì‹œì‘ â†’ ë™ì¼í•œ worker ID/index ì¬í• ë‹¹
  - W1ì´ ì •ë ¬/íŒŒí‹°ì…”ë‹ ë‹¤ì‹œ ìˆ˜í–‰ â†’ partition.1 ì¬ìƒì„±
  - partition.1ì„ ë‹´ë‹¹ Workerì—ê²Œ ë‹¤ì‹œ ì „ì†¡
  - ìˆ˜ì‹  WorkerëŠ” ì¤‘ë³µ ê°ì§€ ë° ì²˜ë¦¬ (idempotent)

Phase 5 (Merge):
  - Worker W1 crash (partition.1 merge ì¤‘)
  - W1 ì¬ì‹œì‘ â†’ ë™ì¼í•œ worker ID/index ì¬í• ë‹¹
  - W1ì´ ì „ì²´ ê³¼ì • ë‹¤ì‹œ ìˆ˜í–‰ â†’ partition.1 ì¬ìƒì„±
  - ìµœì¢… output íŒŒì¼ ìƒì„±
```

**í•µì‹¬**:
- ì¬ì‹œì‘ëœ WorkerëŠ” **í•­ìƒ ì²˜ìŒë¶€í„° (Phase 0)** ì‹œì‘
- Deterministic executionìœ¼ë¡œ **ë™ì¼í•œ ê²°ê³¼** ë³´ì¥
- ë‹¤ë¥¸ Workerë“¤ì€ **ì¤‘ë‹¨ ì—†ì´** ê³„ì† ì‘ì—…

---

## ğŸ“‹ ìµœì¢… íŒŒì¼ êµ¬ì¡°

```
project_2025/
â”œâ”€â”€ build.sbt
â”œâ”€â”€ project/
â”‚   â”œâ”€â”€ build.properties
â”‚   â””â”€â”€ plugins.sbt
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ scala/
â”‚       â”‚   â””â”€â”€ distsort/
â”‚       â”‚       â”œâ”€â”€ Main.scala
â”‚       â”‚       â”œâ”€â”€ master/
â”‚       â”‚       â”‚   â”œâ”€â”€ MasterServer.scala
â”‚       â”‚       â”‚   â””â”€â”€ MasterMain.scala
â”‚       â”‚       â”œâ”€â”€ worker/
â”‚       â”‚       â”‚   â”œâ”€â”€ WorkerNode.scala
â”‚       â”‚       â”‚   â”œâ”€â”€ WorkerMain.scala
â”‚       â”‚       â”‚   â””â”€â”€ FaultTolerance.scala  (ìˆ˜ì •ë¨)
â”‚       â”‚       â””â”€â”€ common/
â”‚       â”‚           â”œâ”€â”€ RecordReader.scala
â”‚       â”‚           â”œâ”€â”€ BinaryRecordReader.scala
â”‚       â”‚           â”œâ”€â”€ AsciiRecordReader.scala
â”‚       â”‚           â”œâ”€â”€ InputFormatDetector.scala
â”‚       â”‚           â””â”€â”€ FileUtils.scala
â”‚       â””â”€â”€ protobuf/
â”‚           â””â”€â”€ distsort.proto
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ... (ê¸°ì¡´ ë¬¸ì„œë“¤)
â””â”€â”€ plan/
    â””â”€â”€ implementation_guide.md (ì´ ë¬¸ì„œ)
```

---

**ë¬¸ì„œ ì™„ì„±ë„**: 100% (PDF ê²€ì¦ ì™„ë£Œ - 2025-10-26)

**ê²€ì¦ ë‚´ìš©**:
- âœ… PDF ëª…ì‹œ ìš”êµ¬ì‚¬í•­ë§Œ í¬í•¨
- âœ… êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì€ ì„ íƒì‚¬í•­ìœ¼ë¡œ ëª…ì‹œ
- âœ… gRPCëŠ” ê¶Œì¥ì‚¬í•­ìœ¼ë¡œ í‘œì‹œ (í•„ìˆ˜ ì•„ë‹˜)
- âœ… íƒ€ì„ì•„ì›ƒ, ìƒ˜í”Œë§ ë¹„ìœ¨, íŒŒí‹°ì…˜ ê°œìˆ˜ ë“±ì€ êµ¬í˜„ ê²°ì • ì‚¬í•­

**ë‹¤ìŒ ë‹¨ê³„**:
1. build.sbt ì„¤ì •
2. ë„¤íŠ¸ì›Œí¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ (gRPC/Netty/java.net)
3. Main.scala êµ¬í˜„
4. Milestone 1 ì²´í¬ë¦¬ìŠ¤íŠ¸ ì™„ë£Œ
