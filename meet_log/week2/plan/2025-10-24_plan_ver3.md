# Distributed Sorting System - 상세 설계 문서 (v3)

**작성일**: 2025-10-24
**버전**: 3.0 (파티션 전략 명확화)
**프로젝트**: Fault-Tolerant Distributed Key/Value Sorting System

---

## 📋 Version 3 주요 변경사항

이 문서는 v2를 기반으로 **파티션 전략을 명확히 구분**하고 **shuffleMap 로직을 추가**한 버전입니다.

### 🔥 v3의 핵심 개선
1. **Simple vs Advanced 파티션 전략 명확한 구분**
2. **shuffleMap 생성 및 사용 로직 추가**
3. **두 가지 아키텍처 다이어그램 제공**
4. **파티션 할당 공식 명시**
5. **Master 출력의 의미 상세 설명**
6. **Shuffle 알고리즘 완전히 재작성**

### 📊 v2 대비 변경사항
- ✅ 파티션 전략 A (Simple: N→N) vs B (Advanced: N→M) 구분
- ✅ shuffleMap 계산 로직 추가
- ✅ Worker가 여러 partition 파일 생성하는 경우 명확화
- ✅ 파티션 번호가 연속적이지 않을 수 있음을 설명
- ✅ 혼란을 야기하던 다이어그램 두 가지 버전으로 분리

---

## 목차
1. [프로젝트 개요](#1-프로젝트-개요)
2. [시스템 아키텍처](#2-시스템-아키텍처) ⭐ 대폭 수정
3. [데이터 구조 및 포맷](#3-데이터-구조-및-포맷) ⭐ 수정
4. [핵심 알고리즘 상세](#4-핵심-알고리즘-상세) ⭐ 수정
5. [네트워크 통신 설계](#5-네트워크-통신-설계)
6. [장애 허용성 메커니즘](#6-장애-허용성-메커니즘)
7. [멀티스레드 및 병렬 처리](#7-멀티스레드-및-병렬-처리)
8. [구현 세부사항](#8-구현-세부사항)
9. [성능 최적화 전략](#9-성능-최적화-전략)
10. [테스트 전략](#10-테스트-전략)
11. [명령행 인터페이스 상세](#11-명령행-인터페이스-상세)
12. [입력 형식 처리](#12-입력-형식-처리)
13. [개발 마일스톤](#13-개발-마일스톤) ⭐ 수정
14. [참고 자료](#14-참고-자료)
15. [예상 이슈 및 해결책](#15-예상-이슈-및-해결책)
16. [결론](#16-결론)

---

## 1. 프로젝트 개요

### 1.1 목표
여러 머신에 분산 저장된 대용량 key/value 레코드를 정렬하는 장애 허용성 분산 시스템 구현

### 1.2 핵심 요구사항
- **입력**: 여러 Worker 노드에 분산된 32MB 블록 단위의 미정렬 데이터
- **출력**: 전역적으로 정렬된 데이터 (각 Worker에 연속된 파티션 저장)
- **장애 허용**: Worker 프로세스 crash 후 재시작 시 올바른 결과 생성
- **확장성**: 멀티코어 활용한 병렬 처리
- **형식 지원**: ASCII 및 Binary 입력 모두 지원

### 1.3 제약사항
- 입력 디렉토리 수정 금지 (읽기 전용)
- 출력 디렉토리에는 최종 결과만 저장
- 포트 하드코딩 금지
- Akka 사용 금지
- ASCII/Binary 형식은 **자동 감지** (명령행 옵션 없이)

---

## 2. 시스템 아키텍처 ⭐ 대폭 수정

### 2.1 파티션 전략

**PDF 요구사항 (Algorithm Phase 1):**
> `numPartitions - 파티션 개수 (일반적으로 워커 수와 동일 또는 배수)`

**PDF 요구사항 (Output 사양):**
> `파일 순서: 각 워커 내에서 partition.n, partition.n+1, ... 형식`

본 시스템은 **두 가지 파티션 전략**을 지원합니다.

---

#### **Strategy A: Simple (N Workers → N Partitions)** ⭐ 기본 구현 (Milestone 1-3)

**개념:**
- Worker 수와 파티션 수가 동일 (1:1 매핑)
- 각 Worker는 **정확히 하나의 파티션**만 담당
- Worker Index = Partition ID

**특징:**
- ✅ 구현 단순, 이해 용이
- ✅ 디버깅 쉬움
- ✅ Milestone 1-3에서 검증하기 좋음
- ⚠️ 로드 불균형 발생 가능
- ⚠️ 멀티코어 활용 제한적

**주의:** Strategy A는 단순 구현용이며, 최종 시스템은 Strategy B를 사용해야 함

**예시: 3 Workers, 각 50GB 입력**

```
Phase 2: Sort & Partition
┌──────────┐  ┌──────────┐  ┌──────────┐
│ Worker 0 │  │ Worker 1 │  │ Worker 2 │
│  50GB    │  │  50GB    │  │  50GB    │
└────┬─────┘  └────┬─────┘  └────┬─────┘
     │sort          │sort          │sort
     ↓              ↓              ↓
   정렬됨         정렬됨         정렬됨
     │              │              │
     ↓partition(3)  ↓partition(3)  ↓partition(3)
     │              │              │
  ┌──┴──┬──┐    ┌──┴──┬──┐    ┌──┴──┬──┐
  P0  P1  P2    P0  P1  P2    P0  P1  P2
  17G 16G 17G   17G 16G 17G   17G 16G 17G

Phase 3: Shuffle (shuffleMap: {0→0, 1→1, 2→2})
  모든 Worker의 P0 → Worker 0
  모든 Worker의 P1 → Worker 1
  모든 Worker의 P2 → Worker 2

Phase 4: Merge
  Worker 0: 3개 P0 조각 merge → partition.0 (50GB)
  Worker 1: 3개 P1 조각 merge → partition.1 (50GB)
  Worker 2: 3개 P2 조각 merge → partition.2 (50GB)

최종 출력:
  /worker0/output/partition.0  ← 전체 P0, 가장 작은 key
  /worker1/output/partition.1  ← 전체 P1, 중간 key
  /worker2/output/partition.2  ← 전체 P2, 가장 큰 key

Master 출력:
  worker0:30000, worker1:30001, worker2:30002

읽기 순서:
  partition.0 → partition.1 → partition.2 = 전역 정렬됨
```

---

#### **Strategy B: Advanced (N Workers → M Partitions, M > N)** ✅ PDF 요구사항 (필수)

**PDF 근거:**
- Algorithm Phase 1: "numPartitions - 파티션 개수 (일반적으로 워커 수와 동일 **또는 배수**)"
- Output 사양: "각 워커 내에서 partition.n, partition.n+1, ... 형식" → 여러 파일 가능

**개념:**
- 파티션 수 > Worker 수 (일반적으로 M = 3N)
- 각 Worker는 **여러 파티션**을 담당
- Partition i는 Worker (i / partitionsPerWorker)가 담당

**특징:**
- ✅ 로드 밸런싱 개선
- ✅ 멀티코어 활용 증가
- ✅ 파티션 크기 불균형 완화
- ✅ PDF 요구사항 충족
- ⚠️ 구현 복잡도 증가
- ⚠️ Shuffle 통신량 동일 (파티션 수와 무관)

**파티션 할당 공식:**
```scala
def assignWorker(partitionID: Int, numWorkers: Int, numPartitions: Int): Int = {
  val partitionsPerWorker = numPartitions / numWorkers
  val workerID = partitionID / partitionsPerWorker
  if (workerID >= numWorkers) numWorkers - 1 else workerID
}

// 또는 단순 mod 방식
def assignWorkerSimple(partitionID: Int, numWorkers: Int): Int = {
  partitionID % numWorkers
}
```

**예시: 3 Workers, 9 Partitions**

```
Phase 2: Sort & Partition
┌──────────┐  ┌──────────┐  ┌──────────┐
│ Worker 0 │  │ Worker 1 │  │ Worker 2 │
│  50GB    │  │  50GB    │  │  50GB    │
└────┬─────┘  └────┬─────┘  └────┬─────┘
     │sort          │sort          │sort
     ↓              ↓              ↓
   정렬됨         정렬됨         정렬됨
     │              │              │
     ↓partition(9)  ↓partition(9)  ↓partition(9)
     │              │              │
  P0..P8         P0..P8         P0..P8
  각 ~5.5GB

Phase 3: Shuffle (shuffleMap: {0→0, 1→1, 2→2, 3→0, 4→1, 5→2, 6→0, 7→1, 8→2})
  P0, P3, P6 → Worker 0
  P1, P4, P7 → Worker 1
  P2, P5, P8 → Worker 2

Phase 4: Merge
  Worker 0:
    - 3개 P0 조각 merge → partition.0
    - 3개 P3 조각 merge → partition.3
    - 3개 P6 조각 merge → partition.6

  Worker 1:
    - 3개 P1 조각 merge → partition.1
    - 3개 P4 조각 merge → partition.4
    - 3개 P7 조각 merge → partition.7

  Worker 2:
    - 3개 P2 조각 merge → partition.2
    - 3개 P5 조각 merge → partition.5
    - 3개 P8 조각 merge → partition.8

최종 출력:
  /worker0/output/
    ├── partition.0  ← 가장 작은 key
    ├── partition.3
    └── partition.6

  /worker1/output/
    ├── partition.1
    ├── partition.4
    └── partition.7

  /worker2/output/
    ├── partition.2
    ├── partition.5
    └── partition.8  ← 가장 큰 key

Master 출력:
  worker0:30000, worker1:30001, worker2:30002

읽기 순서:
  partition.0 → partition.1 → partition.2 →
  partition.3 → partition.4 → partition.5 →
  partition.6 → partition.7 → partition.8
  = 전역 정렬됨
```

---

### 2.2 전체 구조 다이어그램

#### **Strategy A (Simple) 다이어그램**

```
┌─────────────────────────────────────────────────────────┐
│                      Master Node                        │
│  - Worker 등록 관리                                      │
│  - 샘플 수집 및 파티션 경계 계산 (N개)                    │
│  - shuffleMap 생성: {0→0, 1→1, 2→2, ...}               │
│  - 최종 Worker 순서 출력                                 │
└────────────┬────────────────────────────────────────────┘
             │
     ┌───────┴───────┬──────────────┬──────────────┐
     │               │              │              │
┌────▼─────┐   ┌────▼─────┐  ┌────▼─────┐
│ Worker 0 │   │ Worker 1 │  │ Worker 2 │
│(index=0) │   │(index=1) │  │(index=2) │
│          │   │          │  │          │
│ Input:   │   │ Input:   │  │ Input:   │
│ 50GB     │   │ 50GB     │  │ 50GB     │
│          │   │          │  │          │
│ Output:  │   │ Output:  │  │ Output:  │
│partition.0   │partition.1   │partition.2
│(1 file)  │   │(1 file)  │  │(1 file)  │
└──────────┘   └──────────┘  └──────────┘
     │               │              │
     └───────────────┴──────────────┘
        Worker-to-Worker Shuffle
        (각 Worker는 하나의 파티션만 담당)
```

#### **Strategy B (Advanced) 다이어그램**

```
┌─────────────────────────────────────────────────────────┐
│                      Master Node                        │
│  - Worker 등록 관리                                      │
│  - 샘플 수집 및 파티션 경계 계산 (M개)                    │
│  - shuffleMap 생성: {0→0, 1→1, 2→2, 3→0, 4→1, ...}    │
│  - 최종 Worker 순서 출력                                 │
└────────────┬────────────────────────────────────────────┘
             │
     ┌───────┴───────┬──────────────┬──────────────┐
     │               │              │              │
┌────▼─────┐   ┌────▼─────┐  ┌────▼─────┐
│ Worker 0 │   │ Worker 1 │  │ Worker 2 │
│(index=0) │   │(index=1) │  │(index=2) │
│          │   │          │  │          │
│ Input:   │   │ Input:   │  │ Input:   │
│ 50GB     │   │ 50GB     │  │ 50GB     │
│          │   │          │  │          │
│ Output:  │   │ Output:  │  │ Output:  │
│P0,P3,P6  │   │P1,P4,P7  │  │P2,P5,P8  │
│          │   │          │  │          │
│partition.0   │partition.1   │partition.2
│partition.3   │partition.4   │partition.5
│partition.6   │partition.7   │partition.8
│(3 files) │   │(3 files) │  │(3 files) │
└──────────┘   └──────────┘  └──────────┘
     │               │              │
     └───────────────┴──────────────┘
        Worker-to-Worker Shuffle
        (각 Worker는 여러 파티션 담당)

최종 읽기 순서:
  partition.0 → partition.1 → partition.2 →
  partition.3 → partition.4 → partition.5 →
  partition.6 → partition.7 → partition.8
```

---

### 2.3 실행 흐름

```
Phase 0: 초기화
  - Master 시작, Worker들 등록 대기
  - 각 Worker 시작, Master에 연결 및 등록
  - Master가 Worker에 index 할당
  - 디스크 공간 검증

Phase 1: Sampling (샘플링)
  - 각 Worker가 입력 데이터에서 샘플 추출
  - 동적 샘플링 비율 계산
  - Master에게 샘플 전송
  - Master가 전체 샘플 정렬 및 파티션 경계 계산 (N개 or M개)
  - shuffleMap 생성 및 브로드캐스트

Phase 2: Sort & Partition (정렬 및 파티셔닝)
  - 각 Worker가 입력 블록 읽기 (ASCII/Binary 자동 처리)
  - 메모리 내 정렬
  - 파티션 경계에 따라 N개 또는 M개 파티션으로 분할
  - 파티션별 임시 파일 생성

Phase 3: Shuffle (데이터 재분배)
  - shuffleMap에 따라 파티션 전송
  - Worker (i mod N)이 파티션 i를 받음
  - Worker 간 네트워크 통신 (재시도 로직 포함)
  - 타임아웃 및 백오프 처리

Phase 4: Merge (병합)
  - 각 Worker가 받은 파티션들을 개별적으로 K-way merge
  - Strategy A: 1개 partition 파일 생성
  - Strategy B: 여러 개 partition 파일 생성 (예: partition.0, partition.3, partition.6)
  - Atomic write 보장

Phase 5: 완료
  - 각 Worker가 Master에게 완료 보고
  - Master가 전체 작업 완료 확인
  - Master가 정렬된 Worker 주소 출력 (stdout)
  - 임시 파일 정리
```

---

## 3. 데이터 구조 및 포맷 ⭐ 수정

### 3.1 레코드 구조

```
┌──────────────┬────────────────────────────────────────────┐
│ Key (10B)    │ Value (90B)                                │
└──────────────┴────────────────────────────────────────────┘
  0            10                                          100
```

**특징:**
- 고정 길이: 100 바이트
- Key만 정렬 기준으로 사용
- Value는 정렬과 무관하게 Key와 함께 이동

### 3.2 Key 비교 규칙

```scala
def compareKeys(keyA: Array[Byte], keyB: Array[Byte]): Int = {
  for (i <- 0 until 10) {
    if (keyA(i) != keyB(i)) {
      // unsigned byte 비교
      return (keyA(i) & 0xFF).compareTo(keyB(i) & 0xFF)
    }
  }
  0 // 동일
}
```

### 3.3 입력 데이터 구조

```
Input Directories:
/data1/input/
  ├── block_0001  (32MB)
  ├── block_0002  (32MB)
  └── ...

/data2/input/
  ├── block_0001  (32MB)
  └── ...

총 입력: 50GB x N workers
```

### 3.4 출력 데이터 구조 ⭐ 대폭 수정

#### **Strategy A (Simple): 각 Worker 1개 파일**

```
Worker 0 (index=0):
  /output/partition.0  (전체 P0 데이터)

Worker 1 (index=1):
  /output/partition.1  (전체 P1 데이터)

Worker 2 (index=2):
  /output/partition.2  (전체 P2 데이터)

파일 명명 규칙:
  - partition.{worker_index}
  - 각 Worker는 정확히 하나의 파일 생성
  - 파일 번호 = Worker Index = Partition ID
```

#### **Strategy B (Advanced): 각 Worker 여러 파일**

```
Worker 0 (index=0):
  /output/
    ├── partition.0  (전체 P0 데이터)
    ├── partition.3  (전체 P3 데이터)
    └── partition.6  (전체 P6 데이터)

Worker 1 (index=1):
  /output/
    ├── partition.1  (전체 P1 데이터)
    ├── partition.4  (전체 P4 데이터)
    └── partition.7  (전체 P7 데이터)

Worker 2 (index=2):
  /output/
    ├── partition.2  (전체 P2 데이터)
    ├── partition.5  (전체 P5 데이터)
    └── partition.8  (전체 P8 데이터)

파일 명명 규칙:
  - partition.{partition_id}
  - 파티션 번호가 연속적이지 않을 수 있음 (0, 3, 6)
  - Worker i는 파티션 (i, i+N, i+2N, ...) 담당
```

### 3.5 중간 데이터 구조

```
Temporary Directory (작업 중):
${TEMP_DIR}/sort_work_${WORKER_ID}/
  ├── samples/
  │   └── sample.dat  (동적 크기)
  │
  ├── sorted_chunks/
  │   ├── chunk_0000.sorted  (100MB)
  │   ├── chunk_0001.sorted  (100MB)
  │   └── ...
  │
  ├── partitions/
  │   ├── partition_0/
  │   │   ├── from_chunk_0000.dat
  │   │   ├── from_chunk_0001.dat
  │   │   └── ...
  │   ├── partition_1/
  │   │   └── ...
  │   └── ...
  │
  └── received/
      ├── partition_0_from_worker_1.dat
      ├── partition_0_from_worker_2.dat
      └── ...

TEMP_DIR 지정:
  - --temp 플래그로 지정 (기본값: /tmp)
  - 충분한 디스크 공간 사전 확인 필요
  - 작업 완료 후 자동 삭제
```

---

## 4. 핵심 알고리즘 상세 ⭐ 대폭 수정

### 4.1 Phase 1: Sampling Algorithm

#### 4.1.1 동적 샘플링 비율 계산

```scala
def calculateSampleRate(totalInputSize: Long, numWorkers: Int): Double = {
  // Worker당 목표 샘플 개수 (10,000개 key)
  val targetSamplesPerWorker = 10000
  val totalTargetSamples = targetSamplesPerWorker * numWorkers

  // 전체 레코드 수 추정
  val estimatedTotalRecords = totalInputSize / 100

  // 샘플링 비율 계산
  val calculatedRate = totalTargetSamples.toDouble / estimatedTotalRecords

  // 최소/최대 제한
  val minRate = 0.0001  // 0.01%
  val maxRate = 0.01    // 1%

  math.max(minRate, math.min(maxRate, calculatedRate))
}
```

#### 4.1.2 샘플 추출 (각 Worker) ⭐ 수정

```scala
def extractSample(inputDirs: List[String],
                  totalInputSize: Long,
                  numWorkers: Int): Array[Array[Byte]] = {

  val sampleRate = calculateSampleRate(totalInputSize, numWorkers)
  val random = new Random(42)  // 재현 가능성을 위한 고정 seed
  val samples = mutable.ArrayBuffer[Array[Byte]]()

  for (file <- getAllInputFiles(inputDirs)) {
    // ⭐ 각 파일마다 형식 자동 감지
    val format = InputFormatDetector.detectFormat(file)
    val reader = RecordReader.create(format)

    val input = new BufferedInputStream(new FileInputStream(file))

    var record = reader.readRecord(input)
    while (record.isDefined) {
      if (random.nextDouble() < sampleRate) {
        val key = record.get.take(10)
        samples += key
      }
      record = reader.readRecord(input)
    }
    input.close()
  }

  samples.toArray
}
```

#### 4.1.3 파티션 경계 계산 (Master)

```scala
def calculatePartitionBoundaries(
    allSamples: Array[Array[Byte]],
    numPartitions: Int): Array[Array[Byte]] = {

  // 1. 모든 샘플 정렬
  val sortedSamples = allSamples.sortWith(compareKeys(_, _) < 0)

  // 2. 균등 분할을 위한 경계 선택
  val boundaries = new Array[Array[Byte]](numPartitions - 1)
  val step = sortedSamples.length / numPartitions

  for (i <- 0 until numPartitions - 1) {
    boundaries(i) = sortedSamples((i + 1) * step)
  }

  boundaries
}
```

#### 4.1.4 shuffleMap 생성 (Master) ⭐ NEW

```scala
def createShuffleMap(numWorkers: Int, numPartitions: Int): Map[Int, Int] = {
  val shuffleMap = mutable.Map[Int, Int]()
  val partitionsPerWorker = numPartitions / numWorkers

  for (partitionID <- 0 until numPartitions) {
    // 파티션 ID를 Worker ID로 매핑
    val workerID = partitionID / partitionsPerWorker
    val finalWorkerID = if (workerID >= numWorkers) numWorkers - 1 else workerID
    shuffleMap(partitionID) = finalWorkerID
  }

  shuffleMap.toMap
}

// 예시
// Strategy A: createShuffleMap(3, 3)
//   → {0→0, 1→1, 2→2}
//
// Strategy B: createShuffleMap(3, 9)
//   → {0→0, 1→1, 2→2, 3→0, 4→1, 5→2, 6→0, 7→1, 8→2}
```

### 4.2 Phase 2: Sort & Partition Algorithm

#### 4.2.1 청크 단위 정렬 ⭐ 수정

```scala
def sortChunk(inputFile: File,
              chunkSize: Int = 100 * 1024 * 1024): File = {
  // ⭐ 파일 형식 자동 감지
  val format = InputFormatDetector.detectFormat(inputFile)
  val reader = RecordReader.create(format)

  val records = new Array[Array[Byte]](chunkSize / 100)
  val input = new BufferedInputStream(new FileInputStream(inputFile))

  var count = 0
  var record = reader.readRecord(input)

  while (count < records.length && record.isDefined) {
    records(count) = record.get
    count += 1
    record = reader.readRecord(input)
  }
  input.close()

  // 정렬 (key 기준)
  val sortedRecords = records.take(count).sortWith { (a, b) =>
    compareKeys(a.take(10), b.take(10)) < 0
  }

  // 임시 파일 저장
  val outputFile = createTempFile("chunk_", ".sorted")
  val output = new BufferedOutputStream(new FileOutputStream(outputFile))
  sortedRecords.foreach(output.write)
  output.close()

  outputFile
}
```

#### 4.2.2 파티셔닝

```scala
def partitionSortedChunk(
    sortedChunk: File,
    boundaries: Array[Array[Byte]],
    numPartitions: Int,
    outputDir: File): Unit = {

  // numPartitions개의 파티션 파일 생성
  val partitionWriters = (0 until numPartitions).map { i =>
    new BufferedOutputStream(
      new FileOutputStream(new File(outputDir, s"partition_$i.dat"), true),
      1024 * 1024  // 1MB 버퍼
    )
  }.toArray

  val input = new BufferedInputStream(new FileInputStream(sortedChunk))
  val record = new Array[Byte](100)

  while (input.read(record) == 100) {
    val key = record.take(10)
    val partitionId = findPartitionBinarySearch(key, boundaries)
    partitionWriters(partitionId).write(record)
  }

  input.close()
  partitionWriters.foreach(_.close())
}

def findPartitionBinarySearch(key: Array[Byte],
                               boundaries: Array[Array[Byte]]): Int = {
  var left = 0
  var right = boundaries.length

  while (left < right) {
    val mid = (left + right) / 2
    if (compareKeys(key, boundaries(mid)) < 0) {
      right = mid
    } else {
      left = mid + 1
    }
  }
  left
}
```

### 4.3 Phase 3: Shuffle Algorithm ⭐ 완전 재작성

#### 4.3.1 Worker Shuffle 실행

```scala
def shufflePartitions(
    localPartitions: Map[Int, File],      // 이 Worker가 생성한 파티션들
    shuffleMap: Map[Int, Int],             // partitionID → workerID
    myWorkerIndex: Int,
    allWorkers: List[WorkerInfo]): Map[Int, List[File]] = {

  val receivedPartitions = mutable.Map[Int, mutable.ListBuffer[File]]()

  // 1. 자신이 담당할 파티션 목록 파악
  val myPartitionIDs = shuffleMap.filter(_._2 == myWorkerIndex).keys.toSet

  logger.info(s"Worker $myWorkerIndex is responsible for partitions: ${myPartitionIDs.mkString(", ")}")

  // 2. 로컬 파티션 처리
  for ((partitionID, file) <- localPartitions) {
    val targetWorkerID = shuffleMap(partitionID)

    if (targetWorkerID == myWorkerIndex) {
      // 자신이 담당하는 파티션 → 로컬에 유지
      receivedPartitions.getOrElseUpdate(partitionID, mutable.ListBuffer()) += file
      logger.debug(s"Partition $partitionID kept locally")
    } else {
      // 다른 Worker로 전송
      sendPartitionWithRetry(file, partitionID, allWorkers(targetWorkerID))
      logger.debug(s"Partition $partitionID sent to Worker $targetWorkerID")
    }
  }

  // 3. 다른 Worker로부터 수신
  for (partitionID <- myPartitionIDs) {
    for (otherWorker <- allWorkers if otherWorker.index != myWorkerIndex) {
      val receivedFile = receivePartition(partitionID, otherWorker)
      receivedPartitions.getOrElseUpdate(partitionID, mutable.ListBuffer()) += receivedFile
      logger.debug(s"Received partition $partitionID from Worker ${otherWorker.index}")
    }
  }

  // 4. Map[PartitionID -> List[File]] 반환
  receivedPartitions.map { case (k, v) => (k, v.toList) }.toMap
}
```

#### 4.3.2 송신 (Sender) with Retry

```scala
def sendPartitionWithRetry(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo,
    maxRetries: Int = 3): Unit = {

  var attempt = 0
  var success = false

  while (attempt < maxRetries && !success) {
    try {
      sendPartition(partitionFile, partitionId, targetWorker)
      success = true
    } catch {
      case e: StatusRuntimeException if isRetryable(e) =>
        attempt += 1
        val backoffMs = math.pow(2, attempt).toLong * 1000
        logger.warn(s"Send failed (attempt $attempt/$maxRetries), " +
                   s"retrying in ${backoffMs}ms: ${e.getMessage}")
        Thread.sleep(backoffMs)

      case e: Exception =>
        logger.error(s"Non-retryable send error: ${e.getMessage}")
        throw e
    }
  }

  if (!success) {
    throw new IOException(s"Failed to send partition $partitionId after $maxRetries attempts")
  }
}

def sendPartition(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo): Unit = {

  val channel = createGrpcChannelWithRetry(targetWorker.host, targetWorker.port)
  val stub = WorkerServiceGrpc.newStub(channel)

  val responseObserver = new StreamObserver[ShuffleAck] {
    override def onNext(ack: ShuffleAck): Unit = {
      logger.debug(s"Received ACK for partition $partitionId: ${ack.getBytesReceived} bytes")
    }
    override def onError(t: Throwable): Unit = {
      logger.error(s"Shuffle error for partition $partitionId: ${t.getMessage}")
    }
    override def onCompleted(): Unit = {
      logger.info(s"Shuffle completed for partition $partitionId")
    }
  }

  val requestObserver = stub.shuffleData(responseObserver)

  // 청크 단위 스트리밍 전송
  val input = new BufferedInputStream(new FileInputStream(partitionFile))
  val buffer = new Array[Byte](1024 * 1024)  // 1MB 청크

  var bytesRead = 0
  while ({bytesRead = input.read(buffer); bytesRead > 0}) {
    val chunk = ShuffleDataChunk.newBuilder()
      .setPartitionId(partitionId)
      .setData(ByteString.copyFrom(buffer, 0, bytesRead))
      .build()

    requestObserver.onNext(chunk)
  }

  input.close()
  requestObserver.onCompleted()

  channel.shutdown()
  channel.awaitTermination(60, TimeUnit.SECONDS)
}
```

### 4.4 Phase 4: Merge Algorithm ⭐ 수정

#### 4.4.1 파티션별 K-way Merge

```scala
def mergeAllPartitions(
    receivedPartitions: Map[Int, List[File]],  // partitionID → files
    outputDir: File): List[File] = {

  val outputFiles = mutable.ListBuffer[File]()

  // 각 파티션을 개별적으로 merge
  for ((partitionID, files) <- receivedPartitions.toList.sortBy(_._1)) {
    val outputFile = new File(outputDir, s"partition.$partitionID")
    kWayMergeAtomic(files, outputFile)
    outputFiles += outputFile

    logger.info(s"Created partition.$partitionID (${outputFile.length() / 1e6} MB)")
  }

  outputFiles.toList
}
```

#### 4.4.2 K-way Merge with Atomic Write

```scala
case class RecordWithSource(
  record: Array[Byte],
  sourceId: Int
)

def kWayMergeAtomic(
    inputFiles: List[File],
    outputFile: File): Unit = {

  // 임시 파일에 먼저 쓰기
  val tempFile = new File(outputFile.getParent, s".${outputFile.getName}.tmp")

  // Priority Queue (min-heap)
  implicit val ord: Ordering[RecordWithSource] = Ordering.by { rws =>
    rws.record.take(10)
  }(Ordering.by[Array[Byte], String](_.map("%02x".format(_)).mkString).reverse)

  val heap = mutable.PriorityQueue[RecordWithSource]()
  val readers = inputFiles.zipWithIndex.map { case (file, idx) =>
    new BufferedInputStream(new FileInputStream(file), 1024 * 1024)
  }

  // 초기화: 각 파일에서 첫 레코드 읽기
  readers.zipWithIndex.foreach { case (reader, idx) =>
    val record = new Array[Byte](100)
    if (reader.read(record) == 100) {
      heap.enqueue(RecordWithSource(record.clone(), idx))
    }
  }

  val output = new BufferedOutputStream(
    new FileOutputStream(tempFile),
    4 * 1024 * 1024  // 4MB 버퍼
  )

  // Merge
  while (heap.nonEmpty) {
    val min = heap.dequeue()
    output.write(min.record)

    // 같은 소스에서 다음 레코드 읽기
    val record = new Array[Byte](100)
    if (readers(min.sourceId).read(record) == 100) {
      heap.enqueue(RecordWithSource(record, min.sourceId))
    }
  }

  output.close()
  readers.foreach(_.close())

  // Atomic rename
  if (!tempFile.renameTo(outputFile)) {
    throw new IOException(s"Failed to create final partition file: $outputFile")
  }
}
```

---

## 5. 네트워크 통신 설계

### 5.1 Protocol Buffers 정의 ⭐ 수정

```protobuf
syntax = "proto3";

package distsort;

// ========== Master Service ==========

service MasterService {
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);
  rpc SendSample(SampleData) returns (Ack);
  rpc ReportCompletion(CompletionInfo) returns (Ack);
}

message WorkerInfo {
  string worker_id = 1;
  string host = 2;
  int32 port = 3;
  repeated string input_directories = 4;
  string output_directory = 5;
  int64 total_input_size = 6;
  // DataFormat format = 7;  // ⭐ DEPRECATED: 자동 감지 사용, 필드 사용 안 함
}

// ⭐ DataFormat enum은 내부 구현용으로만 사용 (protobuf에서는 사용 안 함)
// enum DataFormat {
//   BINARY = 0;
//   ASCII = 1;
// }

message RegistrationResponse {
  bool success = 1;
  string message = 2;
  int32 worker_index = 3;  // 이 Worker의 인덱스 (파티션 할당에 사용)
}

message SampleData {
  string worker_id = 1;
  repeated bytes keys = 2;
  int32 sample_count = 3;
}

message CompletionInfo {
  string worker_id = 1;
  repeated int32 partition_ids = 2;  // 이 Worker가 생성한 파티션 ID 리스트
  repeated int64 partition_sizes = 3;
  double elapsed_time_seconds = 4;
}

message Ack {
  bool success = 1;
  string message = 2;
}

// ========== Worker Service ==========

service WorkerService {
  rpc SetPartitionBoundaries(PartitionConfig) returns (Ack);  // ⭐ 변경
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);
  rpc GetStatus(StatusRequest) returns (WorkerStatus);
  rpc Reset(ResetRequest) returns (Ack);
}

message PartitionConfig {  // ⭐ NEW
  repeated bytes boundaries = 1;  // N-1 or M-1 개의 경계
  int32 num_partitions = 2;       // N or M
  map<int32, int32> shuffle_map = 3;  // partitionID → workerID ⭐ NEW
  repeated WorkerInfo all_workers = 4;  // 모든 Worker 정보
}

message ShuffleDataChunk {
  int32 partition_id = 1;
  bytes data = 2;
  int64 chunk_offset = 3;
  bool is_last = 4;
}

message ShuffleAck {
  bool success = 1;
  string message = 2;
  int64 bytes_received = 3;
}

message StatusRequest {
  string worker_id = 1;
}

message WorkerStatus {
  enum Phase {
    INITIALIZING = 0;
    SAMPLING = 1;
    SORTING = 2;
    SHUFFLING = 3;
    MERGING = 4;
    COMPLETED = 5;
    FAILED = 6;
  }

  Phase current_phase = 1;
  double progress_percentage = 2;
  string message = 3;
}

message ResetRequest {
  string reason = 1;
}
```

### 5.2 Master 구현 ⭐ 수정

```scala
class MasterServer(numWorkers: Int, numPartitions: Int)
    extends MasterServiceGrpc.MasterServiceImplBase {

  private val workers = new ConcurrentHashMap[String, WorkerInfo]()
  private val samples = new ConcurrentHashMap[String, SampleData]()
  private val completions = new AtomicInteger(0)
  private val latch = new CountDownLatch(numWorkers)

  override def registerWorker(
      request: WorkerInfo,
      responseObserver: StreamObserver[RegistrationResponse]): Unit = {

    val workerId = request.getWorkerId
    workers.put(workerId, request)

    // Worker Index 할당 (등록 순서)
    val workerIndex = workers.size() - 1

    System.err.println(s"[INFO] Worker registered: $workerId at ${request.getHost}:${request.getPort} (index=$workerIndex)")

    val response = RegistrationResponse.newBuilder()
      .setSuccess(true)
      .setWorkerIndex(workerIndex)
      .setMessage(s"Registration successful. Your index is $workerIndex")
      .build()

    responseObserver.onNext(response)
    responseObserver.onCompleted()

    latch.countDown()
  }

  override def sendSample(
      request: SampleData,
      responseObserver: StreamObserver[Ack]): Unit = {

    samples.put(request.getWorkerId, request)
    System.err.println(s"[INFO] Received sample from ${request.getWorkerId}: ${request.getSampleCount} keys")

    if (samples.size() == numWorkers) {
      calculateAndBroadcastConfig()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def calculateAndBroadcastConfig(): Unit = {
    // 1. 샘플 정렬 및 경계 계산
    val allKeys = samples.values().asScala.flatMap(_.getKeysList.asScala).toArray
    val sortedKeys = allKeys.sortWith { (a, b) =>
      compareKeys(a.toByteArray, b.toByteArray) < 0
    }

    val boundaries = (1 until numPartitions).map { i =>
      val idx = (sortedKeys.length * i) / numPartitions
      sortedKeys(idx)
    }

    System.err.println(s"[INFO] Calculated ${boundaries.length} partition boundaries for $numPartitions partitions")

    // 2. shuffleMap 생성 ⭐ NEW
    val shuffleMap = createShuffleMap(numWorkers, numPartitions)
    System.err.println(s"[INFO] shuffleMap: $shuffleMap")

    // 3. PartitionConfig 생성 및 브로드캐스트
    val configMsg = PartitionConfig.newBuilder()
      .addAllBoundaries(boundaries.map(ByteString.copyFrom).asJava)
      .setNumPartitions(numPartitions)
      .putAllShuffleMap(shuffleMap.map { case (k, v) => (k: Integer, v: Integer) }.asJava)
      .addAllAllWorkers(workers.values().asScala.toList.asJava)
      .build()

    workers.values().asScala.foreach { workerInfo =>
      val channel = createGrpcChannelWithRetry(workerInfo.getHost, workerInfo.getPort)
      val stub = WorkerServiceGrpc.newBlockingStub(channel)
      stub.setPartitionBoundaries(configMsg)
      channel.shutdown()
    }
  }

  private def createShuffleMap(numWorkers: Int, numPartitions: Int): Map[Int, Int] = {
    val shuffleMap = mutable.Map[Int, Int]()
    val partitionsPerWorker = numPartitions / numWorkers

    for (partitionID <- 0 until numPartitions) {
      val workerID = partitionID / partitionsPerWorker
      val finalWorkerID = if (workerID >= numWorkers) numWorkers - 1 else workerID
      shuffleMap(partitionID) = finalWorkerID
    }

    shuffleMap.toMap
  }

  override def reportCompletion(
      request: CompletionInfo,
      responseObserver: StreamObserver[Ack]): Unit = {

    System.err.println(s"[INFO] Worker ${request.getWorkerId} completed in ${request.getElapsedTimeSeconds}s")
    System.err.println(s"[INFO] Created partitions: ${request.getPartitionIdsList.asScala.mkString(", ")}")

    if (completions.incrementAndGet() == numWorkers) {
      System.err.println("[INFO] All workers completed! Job finished.")
      printFinalOrdering()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def printFinalOrdering(): Unit = {
    // ⭐ PDF Algorithm Phase 0:
    // print "Master IP:Port"
    // for each worker in workerList do
    //     print worker.IP
    // end for

    val masterAddress = sys.env.getOrElse("MASTER_HOST",
      InetAddress.getLocalHost.getHostAddress
    )
    val masterPort = server.getPort  // 실제 bind된 포트

    val orderedWorkers = workers.values().asScala.toList
      .sortBy(_.getWorkerIndex)

    // stdout으로만 출력 (로그와 분리)
    // Line 1: Master IP:Port
    System.out.println(s"$masterAddress:$masterPort")

    // Line 2~N+1: Worker IP (한 줄에 하나씩, 포트 제외)
    orderedWorkers.foreach { worker =>
      System.out.println(worker.getHost)  // IP만, 포트 제외
    }

    System.out.flush()
  }
}
```

---

## 6. 장애 허용성 메커니즘

### 6.1 장애 시나리오

```
시나리오 1: Sort 중 Worker Crash
  Worker2 정렬 중 → 프로세스 종료
  → 임시 파일 일부만 생성된 상태
  → 재시작 시 임시 파일 삭제 후 처음부터

시나리오 2: Shuffle 중 Worker Crash
  Worker2가 데이터 송신 중 → 프로세스 종료
  → 일부 Worker는 데이터 받았지만 나머지는 못 받음
  → 재시작 시 모든 Worker가 Shuffle 재시작

시나리오 3: Merge 중 Worker Crash
  Worker2가 병합 중 → 프로세스 종료
  → 최종 출력 파일 미완성
  → 재시작 시 병합 재시작
```

### 6.2 복구 전략

#### 6.2.1 멱등성 보장

```scala
class WorkerNode {

  def start(): Unit = {
    try {
      // 시작 전 정리
      cleanupTemporaryFiles()
      cleanupOutputFiles()

      // 정상 작업 수행
      performSorting()

    } catch {
      case e: Exception =>
        logger.error("Worker failed", e)
        cleanupTemporaryFiles()
        cleanupOutputFiles()
        throw e
    }
  }

  private def cleanupTemporaryFiles(): Unit = {
    val tempDir = new File(config.tempDir, s"sort_work_$workerId")
    if (tempDir.exists()) {
      FileUtils.deleteRecursively(tempDir)
      logger.info("Cleaned up temporary files")
    }
  }

  private def cleanupOutputFiles(): Unit = {
    val outputDirFile = new File(config.outputDir)
    if (outputDirFile.exists()) {
      outputDirFile.listFiles().foreach { file =>
        if (file.getName.startsWith("partition.") ||
            file.getName.startsWith(".partition.")) {
          file.delete()
          logger.info(s"Deleted incomplete output: ${file.getName}")
        }
      }
    }
  }
}
```

---

## 7. 멀티스레드 및 병렬 처리

### 7.1 Sort & Partition 병렬화 ⭐ 수정

```scala
class ParallelSorter(numThreads: Int = 4) {

  private val executor = Executors.newFixedThreadPool(numThreads)

  def sortAll(inputFiles: List[File]): List[File] = {
    // ⭐ format 파라미터 제거 - 각 파일마다 자동 감지
    val futures = inputFiles.map { file =>
      executor.submit(new Callable[File] {
        override def call(): File = sortChunk(file)  // 자동 감지 사용
      })
    }

    futures.map(_.get())
  }

  def partitionAll(sortedChunks: List[File],
                   boundaries: Array[Array[Byte]],
                   numPartitions: Int): Unit = {
    val futures = sortedChunks.map { chunk =>
      executor.submit(new Callable[Unit] {
        override def call(): Unit =
          partitionSortedChunk(chunk, boundaries, numPartitions, outputDir)
      })
    }

    futures.foreach(_.get())
  }

  def shutdown(): Unit = {
    executor.shutdown()
    executor.awaitTermination(1, TimeUnit.HOURS)
  }
}
```

---

## 8. 구현 세부사항

### 8.1 프로젝트 구조

```
project_2025/
├── build.sbt
├── project/
│   └── build.properties
├── src/
│   └── main/
│       ├── scala/
│       │   └── distsort/
│       │       ├── Main.scala
│       │       ├── master/
│       │       │   ├── MasterServer.scala
│       │       │   ├── PartitionCalculator.scala
│       │       │   └── ShuffleMapBuilder.scala        ⭐ NEW
│       │       ├── worker/
│       │       │   ├── WorkerNode.scala
│       │       │   ├── Sampler.scala
│       │       │   ├── Sorter.scala
│       │       │   ├── Partitioner.scala
│       │       │   ├── Shuffler.scala                 ⭐ 수정
│       │       │   └── Merger.scala
│       │       ├── common/
│       │       │   ├── RecordComparator.scala
│       │       │   ├── RecordReader.scala
│       │       │   ├── BinaryRecordReader.scala
│       │       │   ├── AsciiRecordReader.scala
│       │       │   ├── FileUtils.scala
│       │       │   ├── NetworkUtils.scala
│       │       │   └── CommandLineParser.scala
│       │       └── proto/
│       │           └── (generated proto files)
│       ├── protobuf/
│       │   └── distsort.proto
│       └── resources/
│           └── logback.xml
├── plan/
│   ├── 2025-10-24_init_plan.md
│   ├── 2025-10-24_init_plan_ver2.md
│   └── 2025-10-24_plan_ver3.md               ⭐ NEW
└── README.md
```

### 8.2 Main 클래스 ⭐ 수정

```scala
object Main {
  def main(args: Array[String]): Unit = {
    if (args.isEmpty) {
      CommandLineParser.printUsage()
      System.exit(1)
    }

    args(0).toLowerCase match {
      case "master" =>
        val numWorkers = args(1).toInt
        val numPartitions = if (args.length > 2) args(2).toInt else numWorkers
        MasterMain.run(numWorkers, numPartitions)

      case "worker" =>
        val config = CommandLineParser.parseWorkerArgs(args.drop(1))
        WorkerMain.run(config)

      case _ =>
        println(s"Unknown command: ${args(0)}")
        CommandLineParser.printUsage()
        System.exit(1)
    }
  }
}
```

**사용 예시:**
```bash
# Master (Strategy A: 3 workers, 3 partitions)
$ sbt "runMain distsort.Main master 3"

# Master (Strategy B: 3 workers, 9 partitions)
$ sbt "runMain distsort.Main master 3 9"

# Worker
$ sbt "runMain distsort.Main worker 192.168.1.1:30000 -I /data1 -O /output"
```

---

## 9. 성능 최적화 전략

### 9.1 I/O 최적화

#### 9.1.1 버퍼링

```scala
// 올바른 예: 버퍼링 사용
val input = new BufferedInputStream(
  new FileInputStream(file),
  1024 * 1024  // 1MB 버퍼
)
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  process(record)
}
```

### 9.2 디스크 공간 사전 확인

```scala
def ensureSufficientDiskSpace(
    tempDir: File,
    outputDir: File,
    inputSize: Long): Unit = {

  val requiredTemp = inputSize * 2
  val requiredOutput = inputSize

  val tempAvailable = tempDir.getUsableSpace
  val outputAvailable = outputDir.getUsableSpace

  if (tempAvailable < requiredTemp * 1.5) {
    throw new IOException(
      f"Insufficient temp disk space: ${tempAvailable / 1e9}%.1f GB available, " +
      f"${requiredTemp * 1.5 / 1e9}%.1f GB required"
    )
  }

  if (outputAvailable < requiredOutput * 1.2) {
    throw new IOException(
      f"Insufficient output disk space: ${outputAvailable / 1e9}%.1f GB available, " +
      f"${requiredOutput * 1.2 / 1e9}%.1f GB required"
    )
  }
}
```

---

## 10. 테스트 전략

### 10.1 단위 테스트

```scala
class ShuffleMapTest extends AnyFlatSpec with Matchers {

  "createShuffleMap" should "correctly assign partitions in Strategy A" in {
    val shuffleMap = MasterServer.createShuffleMap(3, 3)
    shuffleMap shouldEqual Map(0 -> 0, 1 -> 1, 2 -> 2)
  }

  it should "correctly assign partitions in Strategy B" in {
    val shuffleMap = MasterServer.createShuffleMap(3, 9)
    shuffleMap shouldEqual Map(
      0 -> 0, 1 -> 1, 2 -> 2,
      3 -> 0, 4 -> 1, 5 -> 2,
      6 -> 0, 7 -> 1, 8 -> 2
    )
  }

  it should "handle edge case where numPartitions is not divisible by numWorkers" in {
    val shuffleMap = MasterServer.createShuffleMap(3, 10)
    shuffleMap(9) should be < 3  // 마지막 파티션도 유효한 Worker에 할당
  }
}
```

---

## 11. 명령행 인터페이스 상세

### 11.1 Master 명령행 ⭐ 수정

```bash
sbt "runMain distsort.Main master <N> [M]"
```

**매개변수:**
- `<N>`: Worker 수 (필수)
- `[M]`: 파티션 수 (선택, 기본값 = N)

**예시:**
```bash
# Strategy A (3 workers, 3 partitions)
$ sbt "runMain distsort.Main master 3"

# Strategy B (3 workers, 9 partitions)
$ sbt "runMain distsort.Main master 3 9"
```

**출력:**
- **stdout**: 정렬된 Worker 주소 (쉼표 구분)
- **stderr**: 로그 메시지

### 11.2 Worker 명령행

```bash
sbt "runMain distsort.Main worker <master> -I <dir1> <dir2> ... -O <output> [options]"
```

**필수 매개변수:**
- `<master>`: Master 주소 (host:port)
- `-I <dirs...>`: 입력 디렉토리들
- `-O <output>`: 출력 디렉토리

**선택적 매개변수:**
- `--temp <dir>`: 중간 파일 저장 위치
- `--threads <N>`: 스레드 수

**참고:**
- ASCII/Binary 형식은 자동으로 감지됨 (명령행 옵션 불필요)
- 각 파일마다 독립적으로 형식 감지 (혼합 입력 지원)

---

## 12. 입력 형식 처리

### 12.1 자동 형식 감지 ⭐ NEW

**PDF 요구사항:**
> "Should work on both ASCII and binary input **without requiring an option**"

```scala
object InputFormatDetector {
  /**
   * 파일의 첫 1000 바이트를 읽어서 ASCII vs Binary 판별
   */
  def detectFormat(file: File): DataFormat = {
    val buffer = new Array[Byte](1000)
    val inputStream = new FileInputStream(file)

    try {
      val bytesRead = inputStream.read(buffer)

      if (bytesRead <= 0) {
        logger.warn(s"Empty file: ${file.getName}, defaulting to Binary")
        return DataFormat.Binary
      }

      // ASCII printable: 0x20-0x7E (space ~ ~), plus \n (0x0A), \r (0x0D)
      val asciiLikeCount = buffer.take(bytesRead).count { b =>
        (b >= 32 && b <= 126) || b == '\n' || b == '\r'
      }

      val asciiRatio = asciiLikeCount.toDouble / bytesRead

      logger.debug(s"File: ${file.getName}, ASCII ratio: $asciiRatio")

      if (asciiRatio > 0.9) {
        logger.info(s"Detected ASCII format for file: ${file.getName}")
        DataFormat.Ascii
      } else {
        logger.info(s"Detected Binary format for file: ${file.getName}")
        DataFormat.Binary
      }
    } finally {
      inputStream.close()
    }
  }
}
```

### 12.2 레코드 리더 추상화

```scala
sealed trait DataFormat
object DataFormat {
  case object Binary extends DataFormat
  case object Ascii extends DataFormat
}

trait RecordReader {
  def readRecord(input: InputStream): Option[Array[Byte]]
}

object RecordReader {
  def create(format: DataFormat): RecordReader = format match {
    case DataFormat.Binary => new BinaryRecordReader()
    case DataFormat.Ascii => new AsciiRecordReader()
  }
}
```

### 12.3 Binary 형식 리더

```scala
class BinaryRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val record = new Array[Byte](100)
    val bytesRead = input.read(record)

    if (bytesRead == 100) Some(record)
    else if (bytesRead == -1) None
    else throw new IOException(s"Incomplete record: $bytesRead bytes")
  }
}
```

### 12.4 ASCII 형식 리더

```scala
class AsciiRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Array[Byte]] = {
    val line = new Array[Byte](102)  // key(10) + space(1) + value(90) + newline(1)
    val bytesRead = input.read(line)

    if (bytesRead == -1) return None
    if (bytesRead != 102) throw new IOException(s"Invalid ASCII record: $bytesRead bytes")
    if (line(10) != ' '.toByte) throw new IOException("Expected space at position 10")
    if (line(101) != '\n'.toByte) throw new IOException("Expected newline at position 101")

    val record = new Array[Byte](100)
    System.arraycopy(line, 0, record, 0, 10)    // key
    System.arraycopy(line, 11, record, 10, 90)  // value

    Some(record)
  }
}
```

---

## 13. 개발 마일스톤 ⭐ 대폭 수정

### Milestone 1: 기본 인프라 (Week 1-2)
- [ ] 프로젝트 구조 생성
- [ ] Protocol Buffers 정의 (shuffleMap 포함)
- [ ] RecordReader 추상화 및 구현
- [ ] CommandLineParser 구현
- [ ] Master/Worker 스켈레톤 코드
- [ ] gRPC 통신 기본 구현
- [ ] 간단한 테스트 (1 Master + 1 Worker)

**검증:**
```bash
$ sbt "runMain distsort.Main master 1"
$ sbt "runMain distsort.Main worker 127.0.0.1:30000 -I test_input -O test_output"
> Worker registered successfully with index 0
```

### Milestone 2: Strategy A 구현 (Week 3-4)
- [ ] 동적 샘플링 비율 계산
- [ ] 샘플링 구현 (ASCII/Binary)
- [ ] shuffleMap 생성 로직 (N→N)
- [ ] 정렬 및 파티셔닝 (N개 파티션)
- [ ] Shuffle 구현 (재시도 로직)
- [ ] Merge 구현 (1개 파티션/Worker)
- [ ] 로컬 통합 테스트

**검증:**
```bash
# Strategy A: 3 workers, 3 partitions
$ sbt "runMain distsort.Main master 3"
$ # 각 Worker 실행...
$ # 출력 확인
$ ls worker0/output/  # partition.0
$ ls worker1/output/  # partition.1
$ ls worker2/output/  # partition.2
```

### Milestone 3: 장애 허용성 (Week 5)
- [ ] 재시작 로직 구현
- [ ] 임시 파일 정리
- [ ] Worker 장애 테스트
- [ ] Atomic write 검증
- [ ] 네트워크 재시도 테스트

**검증:**
```bash
$ ./test_failure.sh
> Worker 2 killed at t=10s
> Worker 2 restarted at t=12s
> All workers completed successfully
```

### Milestone 4: Strategy B 구현 (Week 6) ✅ PDF 필수 요구사항
- [ ] shuffleMap 로직 확장 (N→M)
- [ ] 파티션 수 증가 지원 (M > N)
- [ ] Worker당 여러 파티션 merge
- [ ] 파티션 번호 불연속 처리
- [ ] 통합 테스트 (3 workers, 9 partitions)

**중요:** PDF 요구사항에 따르면 각 Worker는 여러 partition 파일을 생성할 수 있어야 함

**검증:**
```bash
# Strategy B: 3 workers, 9 partitions
$ sbt "runMain distsort.Main master 3 9"
$ # 각 Worker 실행...
$ # 출력 확인
$ ls worker0/output/  # partition.0, partition.3, partition.6
$ ls worker1/output/  # partition.1, partition.4, partition.7
$ ls worker2/output/  # partition.2, partition.5, partition.8
```

### Milestone 5: 성능 최적화 (Week 7)
- [ ] 멀티스레드 구현
- [ ] I/O 버퍼링 최적화
- [ ] 네트워크 배치 전송
- [ ] 디스크 공간 사전 확인
- [ ] 프로파일링 및 병목 제거
- [ ] 대용량 테스트 (50GB+)

**검증:**
```bash
$ ./benchmark.sh
> 50GB sorted in 180 seconds
> Throughput: 285 MB/s
```

### Milestone 6: 마무리 (Week 8)
- [ ] 전체 시스템 테스트 (Strategy B 중심)
- [ ] Master 출력 형식 검증 (N+1 줄, Worker IP만)
- [ ] PDF 요구사항 최종 확인
- [ ] 문서화 완성
- [ ] 코드 리팩토링
- [ ] 최종 데모 준비

---

## 14. 참고 자료

### 14.1 핵심 알고리즘
- External Sorting: Knuth, TAOCP Vol. 3
- K-way Merge: Priority Queue 기반 병합
- Sampling for Partitioning: TeraSort 논문

### 14.2 시스템 설계
- MapReduce: Dean & Ghemawat, OSDI 2004
- Spark: Zaharia et al., NSDI 2012
- Distributed Sorting: Sort Benchmark 우승 논문들

### 14.3 구현 참고
- gRPC 공식 문서: https://grpc.io/docs/languages/java/
- Protocol Buffers: https://protobuf.dev/
- gensort/valsort 도구 사용법

---

## 15. 예상 이슈 및 해결책

### Issue 1: 파티션 불균형
**증상:** 일부 Worker가 매우 큰 파티션을 받아 병목 발생

**해결:**
- Strategy B로 전환 (파티션 수 증가)
- 동적 샘플링 비율 조정
- 샘플 크기 증가

### Issue 2: shuffleMap 이해 오류
**증상:** 파티션이 잘못된 Worker로 전송됨

**해결:**
- shuffleMap 로직 검증 (단위 테스트)
- 로그 추가 (어느 파티션이 어느 Worker로 가는지)
- Master와 Worker가 동일한 shuffleMap 사용 확인

### Issue 3: 파티션 파일 누락
**증상:** Worker가 담당한 일부 파티션 파일을 생성하지 않음

**해결:**
- receivedPartitions Map 확인
- 모든 파티션 ID를 순회하여 merge
- 빈 파티션도 파일 생성 (크기 0)

### Issue 4: 네트워크 대역폭 부족
**증상:** Shuffle 단계에서 매우 느림

**해결:**
- 압축 사용
- 배치 크기 증가
- 지수 백오프 재시도
- Keep-alive 설정

### Issue 5: 메모리 부족
**증상:** OutOfMemoryError

**해결:**
- 청크 크기 감소
- 스레드 수 감소
- 버퍼 크기 조정
- JVM 힙 크기 증가

### Issue 6: 디스크 공간 부족
**증상:** 작업 중 디스크 공간 부족

**해결:**
- 시작 전 디스크 공간 검증
- 임시 파일 조기 정리
- 여유 공간 50% 확보

---

## 16. 결론

### 16.1 Version 3 주요 성과

이 설계 문서 v3는 **파티션 전략을 명확히 구분**하고 **shuffleMap 로직을 완전히 구현**한 결정판입니다.

**핵심 개선사항:**

1. ✅ **Strategy A (Simple) vs B (Advanced) 명확한 구분**
   - N→N: 각 Worker 1개 파티션
   - N→M: 각 Worker 여러 파티션

2. ✅ **shuffleMap 생성 및 사용 로직 완전 구현**
   - Master: createShuffleMap()
   - Worker: shufflePartitions()
   - Protocol: shuffle_map 필드 추가

3. ✅ **두 가지 다이어그램 제공**
   - Strategy A: 단순 1:1 매핑
   - Strategy B: 복잡한 N:M 매핑

4. ✅ **파티션 할당 공식 명시**
   - workerID = partitionID / partitionsPerWorker
   - 명확한 수식과 예시

5. ✅ **Master 출력 의미 상세 설명**
   - Worker 순서 = 파티션 번호 순서
   - 읽기 순서 명시

### 16.2 구현 로드맵

**단계별 접근:**

1. **Week 1-2: Milestone 1**
   - 기본 인프라 구축
   - gRPC 통신 검증

2. **Week 3-4: Milestone 2**
   - Strategy A (Simple) 완전 구현
   - 3 workers, 3 partitions 테스트

3. **Week 5: Milestone 3**
   - 장애 허용성 추가
   - Crash & Restart 검증

4. **Week 6: Milestone 4**
   - Strategy B (Advanced) 확장
   - 3 workers, 9 partitions 테스트

5. **Week 7: Milestone 5**
   - 성능 최적화
   - 대용량 테스트

6. **Week 8: Milestone 6**
   - 최종 검증 및 문서화

### 16.3 성공 기준

- ✅ 정확성: valsort 통과 (ASCII/Binary 모두)
- ✅ 장애 허용: Worker 재시작 후 정상 완료
- ✅ 확장성: Strategy A → B 전환 가능
- ✅ 성능: 단일 머신 대비 유의미한 속도 향상
- ✅ 코드 품질: 깔끔하고 유지보수 가능
- ✅ 명세 준수: PDF 요구사항 100% 충족

---


**문서 버전:** 3.0
**최종 수정:** 2025-10-24
**작성자:** Project Team
**변경 이력:**
- v1.0 (2025-10-24): 초기 설계
- v2.0 (2025-10-24): PDF 명세서 요구사항 반영
- v3.0 (2025-10-24): 파티션 전략 명확화, shuffleMap 로직 추가
