# Distributed Sorting System - 상세 설계 문서

**작성일**: 2025-10-24
**프로젝트**: Fault-Tolerant Distributed Key/Value Sorting System

---

## 목차
1. [프로젝트 개요](#1-프로젝트-개요)
2. [시스템 아키텍처](#2-시스템-아키텍처)
3. [데이터 구조 및 포맷](#3-데이터-구조-및-포맷)
4. [핵심 알고리즘 상세](#4-핵심-알고리즘-상세)
5. [네트워크 통신 설계](#5-네트워크-통신-설계)
6. [장애 허용성 메커니즘](#6-장애-허용성-메커니즘)
7. [멀티스레드 및 병렬 처리](#7-멀티스레드-및-병렬-처리)
8. [구현 세부사항](#8-구현-세부사항)
9. [성능 최적화 전략](#9-성능-최적화-전략)
10. [테스트 전략](#10-테스트-전략)

---

## 1. 프로젝트 개요

### 1.1 목표
여러 머신에 분산 저장된 대용량 key/value 레코드를 정렬하는 장애 허용성 분산 시스템 구현

### 1.2 핵심 요구사항
- **입력**: 여러 Worker 노드에 분산된 32MB 블록 단위의 미정렬 데이터
- **출력**: 전역적으로 정렬된 데이터 (각 Worker에 연속된 파티션 저장)
- **장애 허용**: Worker 프로세스 crash 후 재시작 시 올바른 결과 생성
- **확장성**: 멀티코어 활용한 병렬 처리

### 1.3 제약사항
- 입력 디렉토리 수정 금지 (읽기 전용)
- 출력 디렉토리에는 최종 결과만 저장
- 포트 하드코딩 금지
- Akka 사용 금지
- ASCII/Binary 입력 모두 지원 (자동 감지 불필요)

---

## 2. 시스템 아키텍처

### 2.1 전체 구조

```
┌─────────────────────────────────────────────────────────┐
│                      Master Node                        │
│  - Worker 등록 관리                                      │
│  - 샘플 수집 및 파티션 경계 계산                          │
│  - 전역 조정 (Coordination)                             │
└────────────┬────────────────────────────────────────────┘
             │
     ┌───────┴───────┬──────────────┬──────────────┐
     │               │              │              │
┌────▼─────┐   ┌────▼─────┐  ┌────▼─────┐   ┌────▼─────┐
│ Worker 1 │   │ Worker 2 │  │ Worker 3 │   │ Worker N │
│          │   │          │  │          │   │          │
│ Input:   │   │ Input:   │  │ Input:   │   │ Input:   │
│ 50GB     │   │ 50GB     │  │ 50GB     │   │ 50GB     │
│          │   │          │  │          │   │          │
│ Output:  │   │ Output:  │  │ Output:  │   │ Output:  │
│ P0,P1,P2 │   │ P3,P4,P5 │  │ P6,P7,P8 │   │ Px,Py,Pz │
└──────────┘   └──────────┘  └──────────┘   └──────────┘
     │               │              │              │
     └───────────────┴──────────────┴──────────────┘
              Worker-to-Worker Shuffle
```

### 2.2 실행 흐름

```
Phase 0: 초기화
  - Master 시작, Worker들 등록 대기
  - 각 Worker 시작, Master에 연결 및 등록

Phase 1: Sampling (샘플링)
  - 각 Worker가 입력 데이터에서 샘플 추출 (~1MB)
  - Master에게 샘플 전송
  - Master가 전체 샘플 정렬 및 파티션 경계 계산
  - 모든 Worker에게 파티션 경계 브로드캐스트

Phase 2: Sort & Partition (정렬 및 파티셔닝)
  - 각 Worker가 입력 블록(100MB 단위) 읽기
  - 메모리 내 정렬
  - 파티션 경계에 따라 데이터 분할
  - 파티션별 임시 파일 생성

Phase 3: Shuffle (데이터 재분배)
  - Worker 간 네트워크 통신
  - 각 파티션 데이터를 해당 Worker로 전송
  - 모든 Worker의 P0 → Worker1
  - 모든 Worker의 P1 → Worker2
  - ...

Phase 4: Merge (병합)
  - 각 Worker가 받은 정렬된 파티션들을 K-way merge
  - 최종 정렬된 파일 생성 (partition.n, partition.n+1, ...)

Phase 5: 완료
  - 각 Worker가 Master에게 완료 보고
  - Master가 전체 작업 완료 확인
  - 임시 파일 정리
```

---

## 3. 데이터 구조 및 포맷

### 3.1 레코드 구조

```
┌──────────────┬────────────────────────────────────────────┐
│ Key (10B)    │ Value (90B)                                │
└──────────────┴────────────────────────────────────────────┘
  0            10                                          100
```

**특징:**
- 고정 길이: 100 바이트
- Key만 정렬 기준으로 사용
- Value는 정렬과 무관하게 Key와 함께 이동

### 3.2 Key 비교 규칙

```scala
def compareKeys(keyA: Array[Byte], keyB: Array[Byte]): Int = {
  for (i <- 0 until 10) {
    if (keyA(i) != keyB(i)) {
      // unsigned byte 비교
      return (keyA(i) & 0xFF).compareTo(keyB(i) & 0xFF)
    }
  }
  0 // 동일
}
```

**주의사항:**
- 바이트 단위 사전식 비교
- Unsigned 비교 필수 (Java/Scala의 byte는 signed)
- 첫 번째로 다른 바이트에서 대소 판단

### 3.3 입력 데이터 구조

```
Input Directories:
/data1/input/
  ├── block_0001  (32MB)
  ├── block_0002  (32MB)
  └── ...

/data2/input/
  ├── block_0001  (32MB)
  └── ...

총 입력: 50GB x N workers
```

### 3.4 출력 데이터 구조

```
Output Directory (각 Worker):
/output/
  ├── partition.0
  ├── partition.1
  ├── partition.2
  └── ...

파일 크기: 가변 (균등 분배 시 총 입력 / 파티션 수)
```

### 3.5 중간 데이터 구조

```
Temporary Directory (작업 중):
/tmp/sort_work_${WORKER_ID}/
  ├── samples/
  │   └── sample.dat  (1MB)
  │
  ├── sorted_chunks/
  │   ├── chunk_0000.sorted  (100MB)
  │   ├── chunk_0001.sorted  (100MB)
  │   └── ...
  │
  ├── partitions/
  │   ├── partition_0/
  │   │   ├── from_chunk_0000.dat
  │   │   ├── from_chunk_0001.dat
  │   │   └── ...
  │   ├── partition_1/
  │   │   └── ...
  │   └── ...
  │
  └── received/
      ├── partition_0_from_worker_1.dat
      ├── partition_0_from_worker_2.dat
      └── ...

작업 완료 후 삭제
```

---

## 4. 핵심 알고리즘 상세

### 4.1 Phase 1: Sampling Algorithm

#### 4.1.1 샘플 추출 (각 Worker)

```scala
def extractSample(inputDirs: List[String],
                  sampleRate: Double = 0.001): Array[Array[Byte]] = {
  val totalSize = calculateTotalSize(inputDirs)
  val targetSampleSize = (totalSize * sampleRate).toLong
  val numRecords = (targetSampleSize / 100).toInt

  val random = new Random(42)  // 재현 가능성을 위한 고정 seed
  val samples = mutable.ArrayBuffer[Array[Byte]]()

  for (file <- getAllInputFiles(inputDirs)) {
    val fileSize = file.length()
    val numSamplesFromFile = (fileSize / 100 * sampleRate).toInt

    val input = new FileInputStream(file)
    val record = new Array[Byte](100)
    var recordCount = 0L

    while (input.read(record) == 100) {
      if (random.nextDouble() < sampleRate) {
        val key = record.take(10)
        samples += key
      }
      recordCount += 1
    }
    input.close()
  }

  samples.toArray
}
```

**샘플링 전략:**
- 확률적 샘플링 (0.1% ~ 0.5%)
- 총 샘플 크기: 약 1MB (10,000개 key)
- Uniform random sampling

#### 4.1.2 파티션 경계 계산 (Master)

```scala
def calculatePartitionBoundaries(
    allSamples: Array[Array[Byte]],
    numPartitions: Int): Array[Array[Byte]] = {

  // 1. 모든 샘플 정렬
  val sortedSamples = allSamples.sortWith(compareKeys(_, _) < 0)

  // 2. 균등 분할을 위한 경계 선택
  val boundaries = new Array[Array[Byte]](numPartitions - 1)
  val step = sortedSamples.length / numPartitions

  for (i <- 0 until numPartitions - 1) {
    boundaries(i) = sortedSamples((i + 1) * step)
  }

  boundaries
}
```

**경계 결정 전략:**
- N개 Worker → N개 파티션 (기본)
- 또는 멀티코어 활용 위해 N*K개 파티션
- 균등 분배를 위한 quantile 계산

**예시:**
```
3 Workers, 30,000개 샘플
→ boundaries[0] = 10,000번째 key
→ boundaries[1] = 20,000번째 key

결과:
- Partition 0: key < boundaries[0]
- Partition 1: boundaries[0] ≤ key < boundaries[1]
- Partition 2: key ≥ boundaries[1]
```

### 4.2 Phase 2: Sort & Partition Algorithm

#### 4.2.1 청크 단위 정렬

```scala
def sortChunk(inputFile: File,
              chunkSize: Int = 100 * 1024 * 1024): File = {
  // 100MB 청크 읽기
  val records = new Array[Array[Byte]](chunkSize / 100)
  val input = new FileInputStream(inputFile)

  var count = 0
  val record = new Array[Byte](100)

  while (count < records.length && input.read(record) == 100) {
    records(count) = record.clone()
    count += 1
  }
  input.close()

  // 정렬 (key 기준)
  val sortedRecords = records.take(count).sortWith { (a, b) =>
    compareKeys(a.take(10), b.take(10)) < 0
  }

  // 임시 파일 저장
  val outputFile = createTempFile("chunk_", ".sorted")
  val output = new FileOutputStream(outputFile)
  sortedRecords.foreach(output.write)
  output.close()

  outputFile
}
```

**정렬 알고리즘 선택:**
- 100MB 이하: In-memory quicksort (Scala 기본 정렬)
- 100MB 초과: External merge sort 또는 청크 분할

**메모리 관리:**
```
가용 메모리: 8GB (예시)
- 정렬 버퍼: 200MB x 4 스레드 = 800MB
- 네트워크 버퍼: 100MB x 10 연결 = 1GB
- Merge 버퍼: 10MB x 100 파일 = 1GB
- 기타 오버헤드: 500MB
총 사용: ~3.3GB → 안전
```

#### 4.2.2 파티셔닝

```scala
def partitionSortedChunk(
    sortedChunk: File,
    boundaries: Array[Array[Byte]],
    outputDir: File): Unit = {

  val partitionWriters = boundaries.indices.map { i =>
    new FileOutputStream(new File(outputDir, s"partition_$i.dat"), true)
  }.toArray :+ new FileOutputStream(
    new File(outputDir, s"partition_${boundaries.length}.dat"), true
  )

  val input = new FileInputStream(sortedChunk)
  val record = new Array[Byte](100)

  while (input.read(record) == 100) {
    val key = record.take(10)
    val partitionId = findPartition(key, boundaries)
    partitionWriters(partitionId).write(record)
  }

  input.close()
  partitionWriters.foreach(_.close())
}

def findPartition(key: Array[Byte],
                  boundaries: Array[Array[Byte]]): Int = {
  boundaries.indexWhere(boundary => compareKeys(key, boundary) < 0) match {
    case -1 => boundaries.length  // 마지막 파티션
    case idx => idx
  }
}
```

**파티셔닝 최적화:**
- 정렬된 청크 → 순차 스캔으로 파티션 분할
- Binary search로 경계 찾기
- 버퍼링으로 디스크 I/O 최소화

### 4.3 Phase 3: Shuffle Algorithm

#### 4.3.1 송신 (Sender)

```scala
def shufflePartition(
    partitionFile: File,
    partitionId: Int,
    targetWorker: WorkerInfo): Unit = {

  val channel = ManagedChannelBuilder
    .forTarget(s"${targetWorker.host}:${targetWorker.port}")
    .usePlaintext()
    .build()

  val stub = WorkerServiceGrpc.newStub(channel)
  val responseObserver = new StreamObserver[ShuffleAck] {
    override def onNext(ack: ShuffleAck): Unit = {
      // ACK 처리
    }
    override def onError(t: Throwable): Unit = {
      // 재전송 로직
      logger.error(s"Shuffle failed: ${t.getMessage}")
    }
    override def onCompleted(): Unit = {
      logger.info(s"Shuffle completed for partition $partitionId")
    }
  }

  val requestObserver = stub.shuffleData(responseObserver)

  // 청크 단위 스트리밍 전송
  val input = new FileInputStream(partitionFile)
  val buffer = new Array[Byte](1024 * 1024)  // 1MB 청크

  var bytesRead = 0
  while ({bytesRead = input.read(buffer); bytesRead > 0}) {
    val chunk = ShuffleDataChunk.newBuilder()
      .setPartitionId(partitionId)
      .setData(ByteString.copyFrom(buffer, 0, bytesRead))
      .build()

    requestObserver.onNext(chunk)
  }

  input.close()
  requestObserver.onCompleted()

  // 완료 대기 및 채널 종료
  channel.shutdown()
  channel.awaitTermination(60, TimeUnit.SECONDS)
}
```

#### 4.3.2 수신 (Receiver)

```scala
class WorkerServiceImpl extends WorkerServiceGrpc.WorkerServiceImplBase {

  override def shuffleData(
      responseObserver: StreamObserver[ShuffleAck]):
      StreamObserver[ShuffleDataChunk] = {

    new StreamObserver[ShuffleDataChunk] {
      val receivedFiles = mutable.Map[Int, FileOutputStream]()

      override def onNext(chunk: ShuffleDataChunk): Unit = {
        val partitionId = chunk.getPartitionId
        val output = receivedFiles.getOrElseUpdate(
          partitionId,
          new FileOutputStream(
            new File(receivedDir, s"partition_${partitionId}_${UUID.randomUUID()}.dat")
          )
        )

        output.write(chunk.getData.toByteArray)
      }

      override def onError(t: Throwable): Unit = {
        logger.error(s"Shuffle receive error: ${t.getMessage}")
        receivedFiles.values.foreach(_.close())
        receivedFiles.clear()
      }

      override def onCompleted(): Unit = {
        receivedFiles.values.foreach(_.close())
        responseObserver.onNext(ShuffleAck.newBuilder().setSuccess(true).build())
        responseObserver.onCompleted()
      }
    }
  }
}
```

**Shuffle 최적화:**
- 스트리밍 전송 (메모리 효율)
- 배치 전송 (네트워크 효율)
- 비동기 I/O
- 압축 고려 (선택적)

### 4.4 Phase 4: Merge Algorithm

#### 4.4.1 K-way Merge

```scala
case class RecordWithSource(
  record: Array[Byte],
  sourceId: Int
)

def kWayMerge(
    inputFiles: List[File],
    outputFile: File): Unit = {

  // Priority Queue (min-heap)
  implicit val ord: Ordering[RecordWithSource] = Ordering.by { rws =>
    rws.record.take(10)  // key만 비교
  }(Ordering.by[Array[Byte], String](_.map("%02x".format(_)).mkString).reverse)

  val heap = mutable.PriorityQueue[RecordWithSource]()
  val readers = inputFiles.zipWithIndex.map { case (file, idx) =>
    new BufferedInputStream(new FileInputStream(file))
  }

  // 초기화: 각 파일에서 첫 레코드 읽기
  readers.zipWithIndex.foreach { case (reader, idx) =>
    val record = new Array[Byte](100)
    if (reader.read(record) == 100) {
      heap.enqueue(RecordWithSource(record, idx))
    }
  }

  val output = new BufferedOutputStream(new FileOutputStream(outputFile))

  // Merge
  while (heap.nonEmpty) {
    val min = heap.dequeue()
    output.write(min.record)

    // 같은 소스에서 다음 레코드 읽기
    val record = new Array[Byte](100)
    if (readers(min.sourceId).read(record) == 100) {
      heap.enqueue(RecordWithSource(record, min.sourceId))
    }
  }

  output.close()
  readers.foreach(_.close())
}
```

**Merge 최적화:**
- 버퍼링으로 디스크 I/O 최소화
- Heap 크기 = 입력 파일 수 (메모리 효율)
- Sequential I/O 패턴 유지

#### 4.4.2 멀티스레드 Merge (고급)

```scala
def parallelMerge(
    partitionGroups: List[List[File]],
    outputDir: File,
    numThreads: Int): Unit = {

  val executor = Executors.newFixedThreadPool(numThreads)
  val futures = partitionGroups.zipWithIndex.map { case (files, partitionId) =>
    executor.submit(new Callable[Unit] {
      override def call(): Unit = {
        val outputFile = new File(outputDir, s"partition.$partitionId")
        kWayMerge(files, outputFile)
      }
    })
  }

  futures.foreach(_.get())  // 모든 작업 완료 대기
  executor.shutdown()
}
```

---

## 5. 네트워크 통신 설계

### 5.1 Protocol Buffers 정의

```protobuf
syntax = "proto3";

package distsort;

// ========== Master Service ==========

service MasterService {
  // Worker 등록
  rpc RegisterWorker(WorkerInfo) returns (RegistrationResponse);

  // 샘플 전송
  rpc SendSample(SampleData) returns (Ack);

  // 작업 완료 보고
  rpc ReportCompletion(CompletionInfo) returns (Ack);
}

message WorkerInfo {
  string worker_id = 1;
  string host = 2;
  int32 port = 3;
  repeated string input_directories = 4;
  string output_directory = 5;
  int64 total_input_size = 6;
}

message RegistrationResponse {
  bool success = 1;
  string message = 2;
  int32 worker_index = 3;  // Master가 할당한 순서
}

message SampleData {
  string worker_id = 1;
  repeated bytes keys = 2;  // 각 10바이트
  int32 sample_count = 3;
}

message CompletionInfo {
  string worker_id = 1;
  repeated int32 partition_ids = 2;
  repeated int64 partition_sizes = 3;
  double elapsed_time_seconds = 4;
}

message Ack {
  bool success = 1;
  string message = 2;
}

// ========== Worker Service ==========

service WorkerService {
  // 파티션 경계 수신
  rpc SetPartitionBoundaries(PartitionBoundaries) returns (Ack);

  // Shuffle 데이터 수신 (스트리밍)
  rpc ShuffleData(stream ShuffleDataChunk) returns (ShuffleAck);

  // 상태 확인
  rpc GetStatus(StatusRequest) returns (WorkerStatus);
}

message PartitionBoundaries {
  repeated bytes boundaries = 1;  // N-1 개의 경계
  int32 num_partitions = 2;
  map<int32, WorkerInfo> partition_assignment = 3;  // 파티션 → Worker 매핑
}

message ShuffleDataChunk {
  int32 partition_id = 1;
  bytes data = 2;  // 여러 레코드 (100의 배수 바이트)
  int64 chunk_offset = 3;
  bool is_last = 4;
}

message ShuffleAck {
  bool success = 1;
  string message = 2;
  int64 bytes_received = 3;
}

message StatusRequest {
  string worker_id = 1;
}

message WorkerStatus {
  enum Phase {
    INITIALIZING = 0;
    SAMPLING = 1;
    SORTING = 2;
    SHUFFLING = 3;
    MERGING = 4;
    COMPLETED = 5;
    FAILED = 6;
  }

  Phase current_phase = 1;
  double progress_percentage = 2;
  string message = 3;
}
```

### 5.2 Master 구현

```scala
class MasterServer(numWorkers: Int) extends MasterServiceGrpc.MasterServiceImplBase {

  private val workers = new ConcurrentHashMap[String, WorkerInfo]()
  private val samples = new ConcurrentHashMap[String, SampleData]()
  private val completions = new AtomicInteger(0)
  private val latch = new CountDownLatch(numWorkers)

  override def registerWorker(
      request: WorkerInfo,
      responseObserver: StreamObserver[RegistrationResponse]): Unit = {

    val workerId = request.getWorkerId
    workers.put(workerId, request)

    logger.info(s"Worker registered: $workerId at ${request.getHost}:${request.getPort}")

    val response = RegistrationResponse.newBuilder()
      .setSuccess(true)
      .setWorkerIndex(workers.size())
      .setMessage("Registration successful")
      .build()

    responseObserver.onNext(response)
    responseObserver.onCompleted()

    latch.countDown()
  }

  override def sendSample(
      request: SampleData,
      responseObserver: StreamObserver[Ack]): Unit = {

    samples.put(request.getWorkerId, request)
    logger.info(s"Received sample from ${request.getWorkerId}: ${request.getSampleCount} keys")

    // 모든 샘플 도착 확인
    if (samples.size() == numWorkers) {
      calculateAndBroadcastBoundaries()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def calculateAndBroadcastBoundaries(): Unit = {
    // 모든 샘플 수집
    val allKeys = samples.values().asScala.flatMap(_.getKeysList.asScala).toArray

    // 정렬
    val sortedKeys = allKeys.sortWith { (a, b) =>
      compareKeys(a.toByteArray, b.toByteArray) < 0
    }

    // 경계 계산 (균등 분배)
    val boundaries = (1 until numWorkers).map { i =>
      val idx = (sortedKeys.length * i) / numWorkers
      sortedKeys(idx)
    }

    logger.info(s"Calculated ${boundaries.length} partition boundaries")

    // 파티션 할당 (Worker i → Partition i)
    val assignment = workers.asScala.toList.sortBy(_._2.getWorkerIndex).zipWithIndex.map {
      case ((_, workerInfo), partitionId) => (partitionId, workerInfo)
    }.toMap

    // 모든 Worker에게 브로드캐스트
    val boundariesMsg = PartitionBoundaries.newBuilder()
      .addAllBoundaries(boundaries.map(ByteString.copyFrom).asJava)
      .setNumPartitions(numWorkers)
      .putAllPartitionAssignment(assignment.map { case (k, v) => (k: Integer, v) }.asJava)
      .build()

    workers.values().asScala.foreach { workerInfo =>
      val channel = ManagedChannelBuilder
        .forTarget(s"${workerInfo.getHost}:${workerInfo.getPort}")
        .usePlaintext()
        .build()

      val stub = WorkerServiceGrpc.newBlockingStub(channel)
      stub.setPartitionBoundaries(boundariesMsg)
      channel.shutdown()
    }
  }

  override def reportCompletion(
      request: CompletionInfo,
      responseObserver: StreamObserver[Ack]): Unit = {

    logger.info(s"Worker ${request.getWorkerId} completed in ${request.getElapsedTimeSeconds}s")

    if (completions.incrementAndGet() == numWorkers) {
      logger.info("All workers completed! Job finished.")
      printFinalOrdering()
    }

    responseObserver.onNext(Ack.newBuilder().setSuccess(true).build())
    responseObserver.onCompleted()
  }

  private def printFinalOrdering(): Unit = {
    val orderedWorkers = workers.values().asScala.toList
      .sortBy(_.getWorkerIndex)
      .map(w => s"${w.getHost}:${w.getPort}")

    println(orderedWorkers.mkString(", "))
  }
}
```

### 5.3 Worker 구현

```scala
class WorkerNode(
    masterId: String,
    inputDirs: List[String],
    outputDir: String) {

  private val workerId = UUID.randomUUID().toString
  private var boundaries: Option[Array[Array[Byte]]] = None
  private var partitionAssignment: Map[Int, WorkerInfo] = Map.empty

  def start(): Unit = {
    // 1. Worker 서버 시작
    val server = startWorkerServer()
    val localPort = server.getPort

    // 2. Master에 연결 및 등록
    registerWithMaster(localPort)

    // 3. 샘플링
    val samples = extractSample(inputDirs)
    sendSamplesToMaster(samples)

    // 4. 파티션 경계 수신 대기
    waitForBoundaries()

    // 5. Sort & Partition
    sortAndPartition()

    // 6. Shuffle
    shuffle()

    // 7. Merge
    merge()

    // 8. 완료 보고
    reportCompletion()

    server.shutdown()
  }

  private def startWorkerServer(): Server = {
    val server = ServerBuilder
      .forPort(0)  // 자동 포트 할당
      .addService(new WorkerServiceImpl(this))
      .build()
      .start()

    logger.info(s"Worker server started on port ${server.getPort}")
    server
  }

  private def registerWithMaster(port: Int): Unit = {
    val channel = ManagedChannelBuilder
      .forTarget(masterId)
      .usePlaintext()
      .build()

    val stub = MasterServiceGrpc.newBlockingStub(channel)

    val workerInfo = WorkerInfo.newBuilder()
      .setWorkerId(workerId)
      .setHost(getLocalIP())
      .setPort(port)
      .addAllInputDirectories(inputDirs.asJava)
      .setOutputDirectory(outputDir)
      .setTotalInputSize(calculateTotalSize(inputDirs))
      .build()

    val response = stub.registerWorker(workerInfo)
    logger.info(s"Registered with master: ${response.getMessage}")

    channel.shutdown()
  }

  // ... 나머지 메서드들
}
```

---

## 6. 장애 허용성 메커니즘

### 6.1 장애 시나리오

```
시나리오 1: Sort 중 Worker Crash
  Worker2 정렬 중 → 프로세스 종료
  → 임시 파일 일부만 생성된 상태
  → 재시작 시 임시 파일 삭제 후 처음부터

시나리오 2: Shuffle 중 Worker Crash
  Worker2가 데이터 송신 중 → 프로세스 종료
  → 일부 Worker는 데이터 받았지만 나머지는 못 받음
  → 재시작 시 모든 Worker가 Shuffle 재시작

시나리오 3: Merge 중 Worker Crash
  Worker2가 병합 중 → 프로세스 종료
  → 최종 출력 파일 미완성
  → 재시작 시 병합 재시작
```

### 6.2 복구 전략

#### 6.2.1 멱등성 보장

```scala
class WorkerNode {

  def start(): Unit = {
    try {
      // 시작 전 정리
      cleanupTemporaryFiles()
      cleanupOutputFiles()

      // 정상 작업 수행
      performSorting()

    } catch {
      case e: Exception =>
        logger.error("Worker failed", e)
        cleanupTemporaryFiles()
        cleanupOutputFiles()
        throw e
    }
  }

  private def cleanupTemporaryFiles(): Unit = {
    val tempDir = new File(s"/tmp/sort_work_$workerId")
    if (tempDir.exists()) {
      FileUtils.deleteDirectory(tempDir)
      logger.info("Cleaned up temporary files")
    }
  }

  private def cleanupOutputFiles(): Unit = {
    val outputDirFile = new File(outputDir)
    if (outputDirFile.exists()) {
      outputDirFile.listFiles().foreach { file =>
        if (file.getName.startsWith("partition.")) {
          file.delete()
          logger.info(s"Deleted incomplete output: ${file.getName}")
        }
      }
    }
  }
}
```

**핵심 원칙:**
- 재시작 시 항상 clean state에서 시작
- 중간 상태 복구 시도하지 않음 (복잡도 증가)
- 임시 파일은 unique ID 사용 (충돌 방지)

#### 6.2.2 재시작 감지 및 조정

```scala
class MasterServer {

  private val workerGenerations = new ConcurrentHashMap[String, Int]()

  override def registerWorker(request: WorkerInfo, ...): Unit = {
    val workerId = request.getWorkerId

    // 재등록 감지
    if (workers.containsKey(workerId)) {
      val generation = workerGenerations.merge(workerId, 1, _ + _)
      logger.warn(s"Worker $workerId re-registered (generation $generation)")

      // 전체 작업 재시작 트리거
      if (generation > 1) {
        resetAllWorkers()
      }
    } else {
      workers.put(workerId, request)
      workerGenerations.put(workerId, 1)
    }

    // ... 응답 전송
  }

  private def resetAllWorkers(): Unit = {
    logger.info("Worker failure detected. Resetting all workers...")

    // 모든 Worker에게 리셋 명령 (프로토콜에 추가 필요)
    workers.values().asScala.foreach { workerInfo =>
      // sendResetCommand(workerInfo)
    }

    // 내부 상태 초기화
    samples.clear()
    completions.set(0)
  }
}
```

### 6.3 원자적 출력

```scala
def atomicWriteOutput(data: Iterator[Array[Byte]],
                      outputFile: File): Unit = {
  // 임시 파일에 먼저 쓰기
  val tempFile = new File(outputFile.getParent,
                          s".${outputFile.getName}.tmp")

  val output = new BufferedOutputStream(new FileOutputStream(tempFile))
  data.foreach(output.write)
  output.close()

  // 완료 후 atomic rename
  if (!tempFile.renameTo(outputFile)) {
    throw new IOException(s"Failed to rename $tempFile to $outputFile")
  }
}
```

**이점:**
- 부분적으로 쓰여진 파일 방지
- 출력 파일이 존재하면 완전히 쓰여진 것이 보장됨

---

## 7. 멀티스레드 및 병렬 처리

### 7.1 Sort & Partition 병렬화

```scala
class ParallelSorter(numThreads: Int = 4) {

  private val executor = Executors.newFixedThreadPool(numThreads)

  def sortAll(inputFiles: List[File]): List[File] = {
    val futures = inputFiles.map { file =>
      executor.submit(new Callable[File] {
        override def call(): File = sortChunk(file)
      })
    }

    futures.map(_.get())
  }

  def partitionAll(sortedChunks: List[File],
                   boundaries: Array[Array[Byte]]): Unit = {
    val futures = sortedChunks.map { chunk =>
      executor.submit(new Callable[Unit] {
        override def call(): Unit = partitionSortedChunk(chunk, boundaries)
      })
    }

    futures.foreach(_.get())
  }

  def shutdown(): Unit = {
    executor.shutdown()
    executor.awaitTermination(1, TimeUnit.HOURS)
  }
}
```

**스레드 수 결정:**
```
최적 스레드 수 = min(
  CPU 코어 수,
  디스크 개수 * 2,
  메모리 / 청크 크기
)

예: 8코어, 2 디스크, 8GB 메모리, 100MB 청크
  → min(8, 4, 80) = 4 스레드
```

### 7.2 Shuffle 병렬화

```scala
class ParallelShuffler(numThreads: Int = 10) {

  private val executor = Executors.newFixedThreadPool(numThreads)

  def shuffleAllPartitions(
      partitionFiles: Map[Int, File],
      assignment: Map[Int, WorkerInfo]): Unit = {

    val futures = partitionFiles.map { case (partitionId, file) =>
      executor.submit(new Callable[Unit] {
        override def call(): Unit = {
          val targetWorker = assignment(partitionId)
          shufflePartition(file, partitionId, targetWorker)
        }
      })
    }

    // 모든 전송 완료 대기
    futures.foreach(_.get())
  }
}
```

**동시 연결 관리:**
```
N개 Worker × M개 파티션 = N*M 개 연결 가능
→ 연결 풀로 제한 (예: 최대 20개)
→ 순차 재사용으로 네트워크 부하 조절
```

### 7.3 Merge 병렬화

```scala
def parallelMerge(
    receivedPartitions: List[Int],
    receivedFiles: Map[Int, List[File]],
    outputDir: File): Unit = {

  val numThreads = min(receivedPartitions.length,
                       Runtime.getRuntime.availableProcessors())
  val executor = Executors.newFixedThreadPool(numThreads)

  val futures = receivedPartitions.map { partitionId =>
    executor.submit(new Callable[Unit] {
      override def call(): Unit = {
        val inputFiles = receivedFiles(partitionId)
        val outputFile = new File(outputDir, s"partition.$partitionId")
        kWayMerge(inputFiles, outputFile)
      }
    })
  }

  futures.foreach(_.get())
  executor.shutdown()
}
```

### 7.4 스레드 안전성

```scala
// Thread-safe 상태 관리
class WorkerState {
  private val phase = new AtomicReference[Phase](Phase.INITIALIZING)
  private val progress = new AtomicInteger(0)
  private val errors = new ConcurrentLinkedQueue[String]()

  def setPhase(newPhase: Phase): Unit = phase.set(newPhase)
  def getPhase: Phase = phase.get()

  def incrementProgress(): Int = progress.incrementAndGet()
  def getProgress: Int = progress.get()

  def addError(error: String): Unit = errors.offer(error)
  def getErrors: List[String] = errors.asScala.toList
}
```

---

## 8. 구현 세부사항

### 8.1 프로젝트 구조

```
project_2025/
├── build.sbt
├── project/
│   └── build.properties
├── src/
│   └── main/
│       ├── scala/
│       │   └── distsort/
│       │       ├── Main.scala
│       │       ├── master/
│       │       │   ├── MasterServer.scala
│       │       │   ├── PartitionCalculator.scala
│       │       │   └── WorkerRegistry.scala
│       │       ├── worker/
│       │       │   ├── WorkerNode.scala
│       │       │   ├── Sampler.scala
│       │       │   ├── Sorter.scala
│       │       │   ├── Partitioner.scala
│       │       │   ├── Shuffler.scala
│       │       │   └── Merger.scala
│       │       ├── common/
│       │       │   ├── RecordComparator.scala
│       │       │   ├── FileUtils.scala
│       │       │   └── NetworkUtils.scala
│       │       └── proto/
│       │           └── (generated proto files)
│       └── protobuf/
│           └── distsort.proto
├── plan/
│   └── 2025-10-24_init_plan.md
└── README.md
```

### 8.2 build.sbt

```scala
name := "distributed-sorting"
version := "1.0"
scalaVersion := "2.13.12"

libraryDependencies ++= Seq(
  // gRPC
  "io.grpc" % "grpc-netty" % "1.58.0",
  "io.grpc" % "grpc-protobuf" % "1.58.0",
  "io.grpc" % "grpc-stub" % "1.58.0",
  "com.thesamet.scalapb" %% "scalapb-runtime-grpc" % "0.11.13",

  // Logging
  "ch.qos.logback" % "logback-classic" % "1.4.11",
  "com.typesafe.scala-logging" %% "scala-logging" % "3.9.5",

  // Testing
  "org.scalatest" %% "scalatest" % "3.2.17" % Test
)

// Protocol Buffers 설정
Compile / PB.targets := Seq(
  scalapb.gen() -> (Compile / sourceManaged).value / "scalapb"
)
```

### 8.3 메인 클래스

```scala
object Main {
  def main(args: Array[String]): Unit = {
    if (args.isEmpty) {
      println("Usage:")
      println("  master <num_workers>")
      println("  worker <master_ip:port> -I <input_dirs...> -O <output_dir>")
      System.exit(1)
    }

    args(0).toLowerCase match {
      case "master" =>
        val numWorkers = args(1).toInt
        MasterMain.run(numWorkers)

      case "worker" =>
        val masterAddress = args(1)
        val inputDirs = parseInputDirs(args)
        val outputDir = parseOutputDir(args)
        WorkerMain.run(masterAddress, inputDirs, outputDir)

      case _ =>
        println(s"Unknown command: ${args(0)}")
        System.exit(1)
    }
  }

  private def parseInputDirs(args: Array[String]): List[String] = {
    val inputIdx = args.indexOf("-I")
    val outputIdx = args.indexOf("-O")

    if (inputIdx == -1 || outputIdx == -1) {
      throw new IllegalArgumentException("Missing -I or -O flag")
    }

    args.slice(inputIdx + 1, outputIdx).toList
  }

  private def parseOutputDir(args: Array[String]): String = {
    val outputIdx = args.indexOf("-O")
    if (outputIdx == -1 || outputIdx == args.length - 1) {
      throw new IllegalArgumentException("Missing -O flag or output directory")
    }
    args(outputIdx + 1)
  }
}
```

### 8.4 유틸리티 클래스

```scala
object RecordComparator {
  /**
   * 두 key를 비교 (unsigned byte 기준)
   * @return <0 if keyA < keyB, 0 if equal, >0 if keyA > keyB
   */
  def compareKeys(keyA: Array[Byte], keyB: Array[Byte]): Int = {
    require(keyA.length == 10 && keyB.length == 10, "Keys must be 10 bytes")

    for (i <- 0 until 10) {
      val diff = (keyA(i) & 0xFF) - (keyB(i) & 0xFF)
      if (diff != 0) return diff
    }
    0
  }

  /**
   * 레코드 전체 비교 (key 기준)
   */
  def compareRecords(recordA: Array[Byte], recordB: Array[Byte]): Int = {
    require(recordA.length == 100 && recordB.length == 100,
            "Records must be 100 bytes")
    compareKeys(recordA.take(10), recordB.take(10))
  }

  /**
   * Ordering 인스턴스 (Scala 정렬에 사용)
   */
  implicit val recordOrdering: Ordering[Array[Byte]] =
    Ordering.by[Array[Byte], Array[Byte]](_.take(10))(
      Ordering.fromLessThan(compareKeys(_, _) < 0)
    )
}

object FileUtils {
  /**
   * 디렉토리의 모든 파일 재귀 탐색
   */
  def getAllFiles(dir: File): List[File] = {
    if (!dir.exists()) {
      throw new IOException(s"Directory does not exist: ${dir.getAbsolutePath}")
    }

    if (dir.isFile) {
      List(dir)
    } else {
      dir.listFiles().toList.flatMap(getAllFiles)
    }
  }

  /**
   * 전체 입력 크기 계산
   */
  def calculateTotalSize(dirs: List[String]): Long = {
    dirs.map(new File(_)).flatMap(getAllFiles).map(_.length()).sum
  }

  /**
   * 디렉토리 생성 (필요 시)
   */
  def ensureDirectory(dir: File): Unit = {
    if (!dir.exists() && !dir.mkdirs()) {
      throw new IOException(s"Failed to create directory: ${dir.getAbsolutePath}")
    }
  }

  /**
   * 안전한 파일 삭제
   */
  def deleteRecursively(file: File): Unit = {
    if (file.isDirectory) {
      file.listFiles().foreach(deleteRecursively)
    }
    file.delete()
  }
}

object NetworkUtils {
  /**
   * 로컬 IP 주소 가져오기 (외부 통신 가능한 주소)
   */
  def getLocalIP: String = {
    val socket = new DatagramSocket()
    try {
      socket.connect(InetAddress.getByName("8.8.8.8"), 10002)
      socket.getLocalAddress.getHostAddress
    } finally {
      socket.close()
    }
  }

  /**
   * IP:Port 파싱
   */
  def parseAddress(address: String): (String, Int) = {
    val parts = address.split(":")
    if (parts.length != 2) {
      throw new IllegalArgumentException(s"Invalid address format: $address")
    }
    (parts(0), parts(1).toInt)
  }
}
```

### 8.5 로깅 설정

```xml
<!-- src/main/resources/logback.xml -->
<configuration>
  <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
    </encoder>
  </appender>

  <appender name="FILE" class="ch.qos.logback.core.FileAppender">
    <file>logs/distsort.log</file>
    <encoder>
      <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
    </encoder>
  </appender>

  <root level="INFO">
    <appender-ref ref="STDOUT" />
    <appender-ref ref="FILE" />
  </root>

  <logger name="distsort" level="DEBUG" />
  <logger name="io.grpc" level="INFO" />
</configuration>
```

---

## 9. 성능 최적화 전략

### 9.1 I/O 최적화

#### 9.1.1 버퍼링

```scala
// 잘못된 예: 버퍼링 없음
val input = new FileInputStream(file)
val record = new Array[Byte](100)
while (input.read(record) == 100) {  // 매번 시스템 콜!
  process(record)
}

// 올바른 예: 버퍼링 사용
val input = new BufferedInputStream(new FileInputStream(file),
                                    1024 * 1024)  // 1MB 버퍼
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  process(record)
}
```

#### 9.1.2 Sequential I/O 유지

```scala
// 좋음: 순차 읽기/쓰기
for (chunk <- chunks) {
  val input = new FileInputStream(chunk)
  // 순차 읽기
  input.close()
}

// 나쁨: Random I/O
val inputs = chunks.map(new FileInputStream(_))
while (hasData) {
  // 여러 파일을 번갈아 읽기 → seek 발생
}
```

### 9.2 네트워크 최적화

#### 9.2.1 배치 전송

```scala
// 비효율: 레코드 하나씩
for (record <- records) {
  send(record)  // 100 바이트씩 전송
}

// 효율: 배치로 묶어서
val batchSize = 1024 * 1024  // 1MB
val batch = mutable.ArrayBuffer[Byte]()

for (record <- records) {
  batch ++= record
  if (batch.size >= batchSize) {
    send(batch.toArray)
    batch.clear()
  }
}
if (batch.nonEmpty) send(batch.toArray)  // 마지막 배치
```

#### 9.2.2 압축 (선택적)

```scala
def compressAndSend(data: Array[Byte]): Unit = {
  val compressed = compress(data)  // Snappy, LZ4 등

  // 압축률 확인
  if (compressed.length < data.length * 0.9) {
    sendCompressed(compressed)
  } else {
    sendUncompressed(data)  // 압축 효과 없으면 원본 전송
  }
}
```

### 9.3 메모리 최적화

#### 9.3.1 스트리밍 처리

```scala
// 메모리 효율적인 정렬
def externalSort(largeFile: File, memoryLimit: Int): File = {
  // 청크 단위로 나눠서 정렬
  val sortedChunks = splitAndSort(largeFile, memoryLimit)

  // K-way merge로 병합
  kWayMerge(sortedChunks, outputFile)
}
```

#### 9.3.2 객체 재사용

```scala
// 비효율: 매번 배열 생성
while (hasMore) {
  val record = new Array[Byte](100)  // 할당!
  input.read(record)
  process(record)
}

// 효율: 배열 재사용
val record = new Array[Byte](100)
while (input.read(record) == 100) {
  process(record)
  // record는 다음 read()에서 덮어씀
}
```

### 9.4 알고리즘 최적화

#### 9.4.1 파티션 찾기 최적화

```scala
// Binary search로 파티션 찾기 (O(log N))
def findPartitionBinarySearch(key: Array[Byte],
                              boundaries: Array[Array[Byte]]): Int = {
  var left = 0
  var right = boundaries.length

  while (left < right) {
    val mid = (left + right) / 2
    if (compareKeys(key, boundaries(mid)) < 0) {
      right = mid
    } else {
      left = mid + 1
    }
  }
  left
}
```

#### 9.4.2 Merge 최적화

```scala
// Priority Queue 대신 Tournament Tree (캐시 효율)
// 또는 Loser Tree 사용
class LoserTree(val k: Int) {
  private val tree = new Array[Int](k)
  private val values = new Array[Array[Byte]](k)

  def init(initialValues: Array[Array[Byte]]): Unit = {
    // 토너먼트 트리 초기화
  }

  def getMin: Array[Byte] = values(tree(0))

  def updateMin(newValue: Array[Byte]): Unit = {
    // 최소값 교체 및 트리 재조정 (O(log k))
  }
}
```

### 9.5 프로파일링

```scala
object Profiler {
  private val timings = new ConcurrentHashMap[String, AtomicLong]()

  def time[T](label: String)(block: => T): T = {
    val start = System.nanoTime()
    try {
      block
    } finally {
      val elapsed = System.nanoTime() - start
      timings.computeIfAbsent(label, _ => new AtomicLong())
              .addAndGet(elapsed)
    }
  }

  def printStats(): Unit = {
    println("=== Performance Statistics ===")
    timings.asScala.toList.sortBy(-_._2.get()).foreach { case (label, nanos) =>
      println(f"$label%-30s: ${nanos.get() / 1e9}%.3f seconds")
    }
  }
}

// 사용 예
Profiler.time("sampling") {
  extractSample(inputDirs)
}

Profiler.time("sorting") {
  sortAndPartition()
}
```

---

## 10. 테스트 전략

### 10.1 단위 테스트

```scala
class RecordComparatorTest extends AnyFlatSpec with Matchers {

  "compareKeys" should "compare unsigned bytes correctly" in {
    val key1 = Array[Byte](0x00, 0x00, 0x00, 0x00, 0x00,
                           0x00, 0x00, 0x00, 0x00, 0x00)
    val key2 = Array[Byte](0xFF.toByte, 0xFF.toByte, 0xFF.toByte,
                           0xFF.toByte, 0xFF.toByte, 0xFF.toByte,
                           0xFF.toByte, 0xFF.toByte, 0xFF.toByte,
                           0xFF.toByte)

    RecordComparator.compareKeys(key1, key2) should be < 0
    RecordComparator.compareKeys(key2, key1) should be > 0
    RecordComparator.compareKeys(key1, key1) shouldEqual 0
  }

  it should "handle partially matching keys" in {
    val key1 = Array[Byte](0x01, 0x02, 0x03, 0x04, 0x05,
                           0x06, 0x07, 0x08, 0x09, 0x0A)
    val key2 = Array[Byte](0x01, 0x02, 0x03, 0x04, 0x05,
                           0x06, 0x07, 0x08, 0x09, 0x0B)

    RecordComparator.compareKeys(key1, key2) should be < 0
  }
}
```

### 10.2 통합 테스트

```scala
class IntegrationTest extends AnyFlatSpec with Matchers {

  "Distributed Sorting" should "sort small dataset correctly" in {
    // 테스트 데이터 생성
    val inputDir = createTestData(records = 10000)
    val outputDir = Files.createTempDirectory("output").toFile

    // Master 시작 (백그라운드)
    val master = startMaster(numWorkers = 2)

    // Worker 시작
    val worker1 = startWorker(master.address, List(inputDir), outputDir)
    val worker2 = startWorker(master.address, List(inputDir), outputDir)

    // 완료 대기
    worker1.waitForCompletion(timeout = 5.minutes)
    worker2.waitForCompletion(timeout = 5.minutes)

    // 검증
    val output = readAllRecords(outputDir)
    output should have size 10000
    output should equal (output.sorted(RecordComparator.recordOrdering))

    // 정리
    cleanup(inputDir, outputDir)
  }

  it should "handle worker failure and restart" in {
    // 테스트 데이터
    val inputDir = createTestData(records = 50000)
    val outputDir = Files.createTempDirectory("output").toFile

    // Master 시작
    val master = startMaster(numWorkers = 2)

    // Worker 시작
    val worker1 = startWorker(master.address, List(inputDir), outputDir)
    val worker2 = startWorker(master.address, List(inputDir), outputDir)

    // Worker2 중간에 강제 종료
    Thread.sleep(2000)
    worker2.kill()

    // Worker2 재시작
    val worker2Restarted = startWorker(master.address, List(inputDir), outputDir)

    // 완료 대기
    worker1.waitForCompletion(timeout = 10.minutes)
    worker2Restarted.waitForCompletion(timeout = 10.minutes)

    // 검증 (결과 동일해야 함)
    val output = readAllRecords(outputDir)
    output should have size 50000
    output should equal (output.sorted(RecordComparator.recordOrdering))
  }
}
```

### 10.3 성능 테스트

```bash
#!/bin/bash
# 성능 테스트 스크립트

# 테스트 데이터 생성
echo "Generating test data..."
./gensort -b 1000 input1  # 100MB
./gensort -b 1000 input2
./gensort -b 1000 input3

# Master 시작
echo "Starting master..."
sbt "runMain distsort.Main master 3" &
MASTER_PID=$!
sleep 2

MASTER_ADDRESS=$(grep "Master started" logs/master.log | awk '{print $NF}')

# Workers 시작 및 시간 측정
echo "Starting workers..."
START_TIME=$(date +%s)

sbt "runMain distsort.Main worker $MASTER_ADDRESS -I input1 -O output1" &
W1_PID=$!

sbt "runMain distsort.Main worker $MASTER_ADDRESS -I input2 -O output2" &
W2_PID=$!

sbt "runMain distsort.Main worker $MASTER_ADDRESS -I input3 -O output3" &
W3_PID=$!

# 완료 대기
wait $W1_PID $W2_PID $W3_PID

END_TIME=$(date +%s)
ELAPSED=$((END_TIME - START_TIME))

echo "Sorting completed in $ELAPSED seconds"

# 결과 검증
echo "Verifying output..."
cat output1/partition.* output2/partition.* output3/partition.* > final_output
./valsort final_output

# 정리
kill $MASTER_PID
rm -rf input* output* final_output
```

### 10.4 스트레스 테스트

```scala
object StressTest extends App {

  // 대용량 데이터 테스트
  def testLargeData(): Unit = {
    val inputSize = 50L * 1024 * 1024 * 1024  // 50GB
    val numWorkers = 10

    // 데이터 생성
    println(s"Generating ${inputSize / 1e9} GB of data...")
    generateLargeData(inputSize, numWorkers)

    // 정렬 수행
    println("Starting distributed sort...")
    val startTime = System.currentTimeMillis()

    runDistributedSort(numWorkers)

    val endTime = System.currentTimeMillis()
    val elapsedSeconds = (endTime - startTime) / 1000.0

    println(f"Completed in $elapsedSeconds%.2f seconds")
    println(f"Throughput: ${inputSize / elapsedSeconds / 1e6}%.2f MB/s")

    // 검증
    verifyOutput()
  }

  // Worker 장애 테스트
  def testWorkerFailures(): Unit = {
    val numWorkers = 5
    val numFailures = 2

    println(s"Testing with $numFailures worker failures...")

    val workers = startAllWorkers(numWorkers)

    // 랜덤 시간에 랜덤 Worker 종료
    val random = new Random()
    for (_ <- 0 until numFailures) {
      Thread.sleep(random.nextInt(10000))
      val victimIdx = random.nextInt(workers.length)
      println(s"Killing worker $victimIdx")
      workers(victimIdx).kill()

      // 재시작
      Thread.sleep(1000)
      workers(victimIdx) = restartWorker(victimIdx)
    }

    // 완료 대기 및 검증
    waitForCompletion(workers)
    verifyOutput()
  }

  testLargeData()
  testWorkerFailures()
}
```

---

## 11. 개발 마일스톤

### Milestone 1: 기본 인프라 (Week 1-2)
- [ ] 프로젝트 구조 생성
- [ ] Protocol Buffers 정의
- [ ] Master/Worker 스켈레톤 코드
- [ ] gRPC 통신 기본 구현
- [ ] 간단한 테스트 (1 Master + 1 Worker, 작은 데이터)

**검증:**
```bash
# Master 실행
$ sbt "runMain distsort.Main master 1"
> 127.0.0.1:30000
> 127.0.0.1:30001

# Worker 실행
$ sbt "runMain distsort.Main worker 127.0.0.1:30000 -I test_input -O test_output"
> Worker registered successfully
```

### Milestone 2: 핵심 알고리즘 (Week 3-4)
- [ ] 샘플링 구현
- [ ] 정렬 및 파티셔닝 구현
- [ ] Shuffle 구현
- [ ] Merge 구현
- [ ] 로컬 통합 테스트 (1 Master + 3 Workers)

**검증:**
```bash
# 작은 데이터로 정확성 검증
$ ./gensort -b 100 input
$ ./run_test.sh
$ ./valsort output
> SUCCESS
```

### Milestone 3: 장애 허용성 (Week 5)
- [ ] 재시작 로직 구현
- [ ] 임시 파일 정리 구현
- [ ] Worker 장애 테스트
- [ ] 원자적 출력 구현

**검증:**
```bash
# Worker 강제 종료 테스트
$ ./test_failure.sh
> Worker 2 killed at t=10s
> Worker 2 restarted at t=12s
> All workers completed successfully
> Output verified: SUCCESS
```

### Milestone 4: 성능 최적화 (Week 6-7)
- [ ] 멀티스레드 구현
- [ ] I/O 버퍼링 최적화
- [ ] 네트워크 배치 전송
- [ ] 프로파일링 및 병목 제거
- [ ] 대용량 테스트 (50GB+)

**검증:**
```bash
# 성능 측정
$ ./benchmark.sh
> 50GB sorted in 180 seconds
> Throughput: 285 MB/s
> Speedup vs single machine: 8.2x
```

### Milestone 5: 마무리 (Week 8)
- [ ] 전체 시스템 테스트
- [ ] 문서화 완성
- [ ] 코드 리팩토링 및 정리
- [ ] 최종 데모 준비

---

## 12. 참고 자료

### 12.1 핵심 알고리즘
- External Sorting: Knuth, TAOCP Vol. 3
- K-way Merge: Priority Queue 기반 병합
- Sampling for Partitioning: TeraSort 논문

### 12.2 시스템 설계
- MapReduce: Dean & Ghemawat, OSDI 2004
- Spark: Zaharia et al., NSDI 2012
- Distributed Sorting: 다양한 Sort Benchmark 우승 논문들

### 12.3 구현 참고
- gRPC 공식 문서: https://grpc.io/docs/languages/java/
- Scala 동시성: Akka 문서 (사용 불가하지만 패턴 참고)
- Protocol Buffers: https://protobuf.dev/

---

## 13. 예상 이슈 및 해결책

### Issue 1: 파티션 불균형
**증상:** 일부 Worker가 매우 큰 파티션을 받아 병목 발생

**해결:**
- 더 정밀한 샘플링 (샘플 크기 증가)
- 파티션 수 증가 (Worker 당 여러 파티션)
- 동적 리밸런싱 (고급)

### Issue 2: 네트워크 대역폭 부족
**증상:** Shuffle 단계에서 매우 느림

**해결:**
- 압축 사용
- 배치 크기 증가
- 동시 전송 수 조절

### Issue 3: 메모리 부족
**증상:** OutOfMemoryError

**해결:**
- 청크 크기 감소
- 스레드 수 감소
- 버퍼 크기 조정
- JVM 힙 크기 증가 (`-Xmx8g`)

### Issue 4: 디스크 I/O 병목
**증상:** CPU 유휴 상태인데 작업 느림

**해결:**
- 버퍼링 증가
- Sequential I/O 패턴 유지
- SSD 사용
- 임시 파일을 별도 디스크에 저장

### Issue 5: Worker 재시작 무한 루프
**증상:** Worker가 계속 실패하고 재시작

**해결:**
- 실패 원인 로깅 강화
- 재시작 횟수 제한
- 백오프 전략 (exponential backoff)
- Master의 전역 재시작 트리거

---

## 14. 결론

이 설계 문서는 분산 정렬 시스템의 핵심 요소들을 상세히 다루었습니다:

1. **명확한 아키텍처**: Master-Worker 구조, 4단계 파이프라인
2. **구체적인 알고리즘**: 샘플링, 정렬, 파티셔닝, Shuffle, Merge
3. **네트워크 프로토콜**: gRPC/Protobuf 기반 통신
4. **장애 허용성**: 재시작 기반 복구, 멱등성 보장
5. **성능 최적화**: 멀티스레드, I/O 버퍼링, 네트워크 배치
6. **구현 가이드**: 프로젝트 구조, 유틸리티, 테스트 전략

**다음 단계:**
1. 프로젝트 환경 설정
2. Milestone 1 구현 시작
3. 주간 진행 상황 문서화
4. 정기적인 통합 테스트

**성공 기준:**
- ✅ 정확성: valsort 통과
- ✅ 장애 허용: Worker 재시작 후 정상 완료
- ✅ 성능: 단일 머신 대비 유의미한 속도 향상
- ✅ 코드 품질: 깔끔하고 유지보수 가능한 구조

---

**문서 버전:** 1.0
**최종 수정:** 2025-10-24
**작성자:** Project Team
