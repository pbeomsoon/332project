# Implementation Decisions (êµ¬í˜„ ê²°ì • ì‚¬í•­)

**ì‘ì„±ì¼**: 2025-10-24
**ëª©ì **: PDF ìš”êµ¬ì‚¬í•­ í•´ì„ ë° ëª¨í˜¸í•œ ë¶€ë¶„ì— ëŒ€í•œ ëª…í™•í•œ êµ¬í˜„ ê²°ì •

---

## 1. Fault Tolerance ì „ëµ ê²°ì •

### 1.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "The system must be fault-tolerant, which means that if a worker crashes and restarts, the overall computation should still produce correct results."

**í•µì‹¬ ì§ˆë¬¸**: "Worker crash & restart í›„ ì–´ë–»ê²Œ ë³µêµ¬?"

### 1.2 êµ¬í˜„ ê²°ì •: **Checkpoint-based Recovery**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fault Tolerance Strategy (ì‹¤ì œ êµ¬í˜„)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Checkpoint ì €ì¥:                                        â”‚
â”‚   âœ… ê° Phase ì™„ë£Œ ì‹œ ìë™ ì €ì¥                          â”‚
â”‚   âœ… WorkerStateë¥¼ JSONìœ¼ë¡œ ì˜ì†í™”                       â”‚
â”‚   âœ… ìœ„ì¹˜: /tmp/distsort/checkpoints/                   â”‚
â”‚   âœ… ìµœê·¼ 3ê°œ checkpoint ìœ ì§€                            â”‚
â”‚                                                         â”‚
â”‚ Worker Crash & Restart:                                 â”‚
â”‚   âœ… ì‹œì‘ ì‹œ ìµœì‹  checkpoint ë¡œë“œ                        â”‚
â”‚   âœ… ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ                          â”‚
â”‚   âœ… Sampling/Sort ì¬ìˆ˜í–‰ ë¶ˆí•„ìš”                         â”‚
â”‚                                                         â”‚
â”‚ Graceful Shutdown:                                      â”‚
â”‚   âœ… 30ì´ˆ grace period                                  â”‚
â”‚   âœ… í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸°                                â”‚
â”‚   âœ… Checkpoint ì €ì¥ í›„ ì¢…ë£Œ                             â”‚
â”‚                                                         â”‚
â”‚ Data Integrity:                                         â”‚
â”‚   âœ… Atomic writes (temp + rename)                      â”‚
â”‚   âœ… State-based cleanup on failure                     â”‚
â”‚   âœ… Idempotent operations                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ì •ë‹¹í™” (Justification)

**ì™œ "Checkpoint ê¸°ë°˜ ë³µêµ¬"ë¥¼ ì„ íƒí–ˆë‚˜?**

1. **ë¹ ë¥¸ ë³µêµ¬**: ë§ˆì§€ë§‰ ì™„ë£Œ Phaseë¶€í„° ì¬ê°œ â†’ ì „ì²´ ì¬ì‹œì‘ë³´ë‹¤ íš¨ìœ¨ì 
2. **ì •í™•ì„± ë³´ì¥**: Phaseë³„ ì™„ë£Œ checkpoint â†’ ë¶€ë¶„ ê²°ê³¼ ìœ ì‹¤ ì—†ìŒ
3. **ì‹¤ì œ ì‹œìŠ¤í…œ ì‚¬ë¡€**:
   - Spark: RDD lineage + checkpoint í˜¼í•©
   - Flink: Checkpoint ê¸°ë°˜ exactly-once semantics
   - MapReduce: Task-level restart (ìš°ë¦¬ëŠ” Worker-level)
4. **êµ¬í˜„ ê°€ëŠ¥ì„±**:
   - CheckpointManagerë¡œ ìƒíƒœ ê´€ë¦¬
   - JSON ì§ë ¬í™”ë¡œ ê°„ë‹¨í•œ ì˜ì†í™”
   - Graceful Shutdownê³¼ í†µí•©
5. **PDF í•´ì„**: "produce correct results" â† Checkpointê°€ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ëª¨ë‘ ì¶©ì¡±

### 1.4 Worker Restart ì‹œë‚˜ë¦¬ì˜¤ (Checkpoint ê¸°ë°˜)

```
ì‹œë‚˜ë¦¬ì˜¤ 1: Worker crashes during Sorting (Phase 3)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Before crash:
  - Worker 2 is sorting local data (50% complete)
  - Last checkpoint: PHASE_SAMPLING (100% complete)
  - Worker 2 crashes (kill -9)

Detection:
  - Master: No heartbeat from Worker 2 for 60s
  - Master: Marks Worker 2 as FAILED

Recovery:
  Worker 2 restarts:
    1. run() í˜¸ì¶œ
    2. recoverFromCheckpoint() ì„±ê³µ
       - Loads: checkpoint_1699999999_PHASE_SAMPLING.json
       - Restores: partitionBoundaries, shuffleMap, etc.
       - Sets currentPhase = PHASE_SAMPLING

    3. Phaseë³„ ì²´í¬:
       if (currentPhase == PHASE_SAMPLING) {
         getPartitionConfiguration()  // âœ… ì´ë¯¸ ì™„ë£Œ (checkpointì—ì„œ)
       }
       if (currentPhase == PHASE_SORTING) {
         performLocalSort()  // â­ ì—¬ê¸°ë¶€í„° ì¬ê°œ (Sampling ìŠ¤í‚µ)
       }

    4. Sorting ì¬ì‹œì‘ â†’ Checkpoint ì €ì¥ (PHASE_SORTING 100%)
    5. Shuffle â†’ Merge â†’ ì™„ë£Œ

Result:
  âœ… Sampling ì¬ìˆ˜í–‰ ë¶ˆí•„ìš” (10ë¶„ ì ˆì•½)
  âœ… ì •í™•ì„± ë³´ì¥ (checkpoint ìƒíƒœì—ì„œ ì¬ê°œ)
  âœ… ë‹¤ë¥¸ Worker ì˜í–¥ ì—†ìŒ
```

```
ì‹œë‚˜ë¦¬ì˜¤ 2: Worker crashes during Shuffle (Phase 4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Before crash:
  - Worker 2 is sending partition.5 to Worker 3 (50% complete)
  - Last checkpoint: PHASE_SORTING (100% complete)
  - Worker 2 crashes (network failure)

Detection:
  - Worker 3: Partial file received, timeout after 60s
  - Worker 3: Cleanup incomplete temp file
  - Master: No heartbeat from Worker 2 for 60s

Recovery:
  Worker 2 restarts:
    1. recoverFromCheckpoint() ì„±ê³µ
       - Loads: checkpoint_1700000000_PHASE_SORTING.json
       - currentPhase = PHASE_SORTING

    2. Phaseë³„ ì²´í¬:
       if (currentPhase == PHASE_SORTING) {
         // Sorting already done (checkpoint)
         performShuffle(sortedChunks)  // â­ Shuffle ì¬ì‹œì‘
       }

    3. Shuffle ì¬ì‹œë„:
       - ì •ë ¬ëœ chunk íŒŒì¼ë“¤ì€ ì´ë¯¸ ì¡´ì¬
       - partitionBoundaries, shuffleMap ë³µì›ë¨
       - Shuffle ì²˜ìŒë¶€í„° ì¬ì‹œì‘ (ë©±ë“±ì„± ë³´ì¥)
       - ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ ì „ì†¡ ì„±ê³µ

    4. Merge â†’ ì™„ë£Œ

Result:
  âœ… Sorting ì¬ìˆ˜í–‰ ë¶ˆí•„ìš” (ì‹œê°„ ëŒ€í­ ì ˆì•½)
  âœ… Shuffleë§Œ ì¬ì‹œë„
  âœ… ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
```

```
ì‹œë‚˜ë¦¬ì˜¤ 3: Worker crashes just before Checkpoint save
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Before crash:
  - Worker 2 completes Sorting
  - savePhaseCheckpoint(PHASE_SORTING, 1.0) í˜¸ì¶œ ì¤‘
  - Checkpoint ì €ì¥ 50% â†’ Crash

Recovery:
  Worker 2 restarts:
    1. recoverFromCheckpoint()
       - Latest checkpoint: PHASE_SAMPLING (ì´ì „ ì™„ë£Œ)
       - PHASE_SORTING checkpointëŠ” ë¶ˆì™„ì „ (ê²€ì¦ ì‹¤íŒ¨)

    2. Phaseë³„ ì²´í¬:
       if (currentPhase == PHASE_SAMPLING) {
         getPartitionConfiguration()  // Skip
         performLocalSort()  // â­ Sorting ì¬ìˆ˜í–‰
       }

Result:
  âœ… ë¶ˆì™„ì „í•œ checkpointëŠ” ë¬´ì‹œ
  âœ… ê°€ì¥ ìµœê·¼ì˜ ì™„ì „í•œ checkpointë¶€í„° ì¬ê°œ
  âœ… Worst case: Sorting ì¬ìˆ˜í–‰ (ì—¬ì „íˆ Samplingì€ ìŠ¤í‚µ)
```

### 1.5 ì‹¤ì œ êµ¬í˜„ ìƒì„¸

#### CheckpointManager êµ¬í˜„

```scala
package distsort.checkpoint

case class Checkpoint(
  id: String,
  workerId: String,
  phase: String,                   // WorkerPhaseë¥¼ Stringìœ¼ë¡œ ì§ë ¬í™”
  timestamp: Instant,
  progress: Double,
  state: WorkerState
)

case class WorkerState(
  processedRecords: Long,
  partitionBoundaries: List[Array[Byte]],
  shuffleMap: Map[Int, Int],
  completedPartitions: Set[Int],
  currentFiles: List[String],
  phaseMetadata: Map[String, String]
)

class CheckpointManager(workerId: String, checkpointDir: String) {
  private val gson = new GsonBuilder().setPrettyPrinting().create()
  private val checkpointPath = Paths.get(checkpointDir, workerId)

  /**
   * Save checkpoint to disk (JSON format)
   */
  def saveCheckpoint(
    phase: WorkerPhase,
    state: WorkerState,
    progress: Double
  ): Future[String] = Future {
    val checkpointId = s"checkpoint_${System.currentTimeMillis()}_${phase.toString}"
    val checkpoint = Checkpoint(checkpointId, workerId, phase.toString,
                                 Instant.now(), progress, state)

    val file = checkpointPath.resolve(s"$checkpointId.json").toFile
    val writer = new PrintWriter(file)

    try {
      writer.write(gson.toJson(checkpoint))
      logger.info(s"Saved checkpoint $checkpointId (${(progress * 100).toInt}%)")
      checkpointId
    } finally {
      writer.close()
    }
  }

  /**
   * Load latest valid checkpoint
   */
  def loadLatestCheckpoint(): Option[Checkpoint] = {
    val checkpointFiles = checkpointPath.toFile.listFiles()
      .filter(_.getName.endsWith(".json"))
      .sortBy(_.lastModified()).reverse

    checkpointFiles.headOption.flatMap { file =>
      val json = Files.readString(file.toPath)
      val checkpoint = gson.fromJson(json, classOf[Checkpoint])

      if (validateCheckpoint(checkpoint.id)) {
        Some(checkpoint)
      } else {
        None  // ë¶ˆì™„ì „í•œ checkpoint ë¬´ì‹œ
      }
    }
  }

  /**
   * Clean old checkpoints (keep last 3)
   */
  def cleanOldCheckpoints(keepLast: Int = 3): Unit = {
    val checkpointFiles = checkpointPath.toFile.listFiles()
      .filter(_.getName.endsWith(".json"))
      .sortBy(_.lastModified()).reverse

    checkpointFiles.drop(keepLast).foreach(_.delete())
  }
}
```

#### Workerì—ì„œì˜ Checkpoint ì‚¬ìš©

```scala
class Worker(...) extends ShutdownAware {
  private val checkpointManager = CheckpointManager(workerId,
                                                     s"/tmp/distsort/checkpoints")

  def run(): Unit = {
    // 1. Checkpointì—ì„œ ë³µêµ¬ ì‹œë„
    val recoveredFromCheckpoint = recoverFromCheckpoint()

    if (!recoveredFromCheckpoint || currentPhase.get() == PHASE_INITIALIZING) {
      performSampling()
    }

    // ë³µêµ¬ëœ Phaseì— ë”°ë¼ ì ì ˆí•œ ë‹¨ê³„ë¶€í„° ì¬ê°œ
    if (currentPhase.get() == PHASE_SAMPLING || ...) {
      getPartitionConfiguration()
      savePhaseCheckpoint(PHASE_WAITING_FOR_PARTITIONS, 1.0)  // â­ ì €ì¥
    }

    if (currentPhase.get() == PHASE_SORTING || ...) {
      performLocalSort()
      savePhaseCheckpoint(PHASE_SORTING, 1.0)  // â­ ì €ì¥

      performShuffle()
      savePhaseCheckpoint(PHASE_SHUFFLING, 1.0)  // â­ ì €ì¥
    }

    performMerge()
    savePhaseCheckpoint(PHASE_MERGING, 1.0)  // â­ ì €ì¥

    // ì„±ê³µ ì‹œ ëª¨ë“  checkpoint ì‚­ì œ
    checkpointManager.deleteAllCheckpoints()
  }

  private def recoverFromCheckpoint(): Boolean = {
    checkpointManager.loadLatestCheckpoint() match {
      case Some(checkpoint) =>
        // ìƒíƒœ ë³µì›
        val (processedRecords, boundaries, shuffle, completed, files, metadata) =
          WorkerState.toScala(checkpoint.state)

        processedRecordCount.set(processedRecords)
        partitionBoundaries = boundaries.toArray
        shuffleMap = shuffle
        completedPartitions.clear()
        completed.foreach(p => completedPartitions.put(p, true))

        // Phase ë³µì›
        val phase = WorkerPhase.fromName(checkpoint.phase)
          .getOrElse(WorkerPhase.PHASE_INITIALIZING)

        currentPhase.set(phase)
        true  // ë³µêµ¬ ì„±ê³µ

      case None =>
        false  // ì²˜ìŒë¶€í„° ì‹œì‘
    }
  }

  private def savePhaseCheckpoint(phase: WorkerPhase, progress: Double): Unit = {
    try {
      val state = WorkerState.fromScala(
        processedRecords = processedRecordCount.get(),
        partitionBoundaries = partitionBoundaries.toSeq,
        shuffleMap = shuffleMap,
        completedPartitions = completedPartitions.keys().asScala.toSet,
        currentFiles = fileLayout.getInputFiles.map(_.getAbsolutePath),
        phaseMetadata = Map("workerId" -> workerId, "phase" -> phase.toString)
      )

      Await.result(checkpointManager.saveCheckpoint(phase, state, progress), 10.seconds)
      checkpointManager.cleanOldCheckpoints(3)  // ìµœê·¼ 3ê°œë§Œ ìœ ì§€

    } catch {
      case ex: Exception =>
        logger.warn(s"Checkpoint save failed: ${ex.getMessage}")
        // Best-effort: ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    }
  }
}
```

#### Graceful Shutdown í†µí•©

```scala
class Worker(...) extends ShutdownAware {
  private val shutdownManager = GracefulShutdownManager(
    ShutdownConfig(
      gracePeriod = 30.seconds,
      saveCheckpoint = true,         // â­ Shutdown ì‹œ checkpoint ì €ì¥
      waitForCurrentPhase = true     // â­ í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸°
    )
  )

  override def gracefulShutdown(): Future[Unit] = {
    logger.info("Initiating graceful shutdown...")

    // 1. í˜„ì¬ Phase ì™„ë£Œ ëŒ€ê¸°
    // 2. Checkpoint ì €ì¥
    val currentState = getCurrentState()
    checkpointManager.saveCheckpoint(currentPhase.get(), currentState, 0.5)

    // 3. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cleanupResources()
  }
}
```

#### í–¥í›„ ê°œì„  ê°€ëŠ¥ì„±

**Phase ì¤‘ê°„ ì§€ì  Checkpoint** (í˜„ì¬ëŠ” Phase ì™„ë£Œ ì‹œì ë§Œ):
- Sorting ì¤‘ chunkë³„ checkpoint
- Shuffle ì¤‘ íŒŒí‹°ì…˜ë³„ checkpoint
- ë” ì„¸ë°€í•œ ë³µêµ¬ ê°€ëŠ¥ (trade-off: overhead ì¦ê°€)

**Master Checkpoint** (í˜„ì¬ëŠ” Workerë§Œ):
- Master ìƒíƒœë„ checkpoint
- Master crash ì‹œ ë³µêµ¬ ê°€ëŠ¥
- ì™„ì „í•œ Fault Tolerance (í˜„ì¬ëŠ” Workerë§Œ)

---

## 2. ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ ê²°ì •

### 2.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "Utilize multi-core processors for parallel processing"

### 2.2 êµ¬í˜„ ê²°ì •: **Phaseë³„ ë³‘ë ¬í™”**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parallelization Strategy                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Thread Pool Configuration:                              â”‚
â”‚   - numCores = Runtime.getRuntime.availableProcessors() â”‚
â”‚   - sortingThreads = numCores                           â”‚
â”‚   - shuffleThreads = min(numCores, numWorkers)          â”‚
â”‚   - mergeThreads = numCores                             â”‚
â”‚                                                         â”‚
â”‚ Phaseë³„ ë³‘ë ¬í™”:                                          â”‚
â”‚   Phase 2 (Sorting):    Parallel chunk sorting         â”‚
â”‚   Phase 3 (Partition):  Parallel partition writing     â”‚
â”‚   Phase 4 (Shuffle):    Concurrent file transfers      â”‚
â”‚   Phase 5 (Merge):      Parallel K-way merge           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 êµ¬ì²´ì  êµ¬í˜„

#### **Phase 2: Parallel Sorting**

```scala
class ExternalSorter(numThreads: Int = Runtime.getRuntime.availableProcessors()) {
  private val executor = Executors.newFixedThreadPool(numThreads)

  def sortInParallel(chunks: Seq[Chunk]): Seq[File] = {
    val futures = chunks.map { chunk =>
      Future {
        val sorted = chunk.records.sortWith((a, b) =>
          ByteArrayOrdering.compare(a.key, b.key) < 0
        )
        writeSortedChunk(sorted)
      }(ExecutionContext.fromExecutor(executor))
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

#### **Phase 3: Parallel Partitioning**

```scala
class Partitioner(numThreads: Int) {
  def writePartitionsInParallel(
    partitions: Map[Int, Seq[Record]]
  ): Seq[File] = {
    // ê° partitionì„ ë³‘ë ¬ë¡œ ì“°ê¸°
    partitions.par.map { case (partitionId, records) =>
      val file = fileLayout.getLocalPartitionFile(partitionId)
      FileOperations.atomicWriteStream(file) { out =>
        records.foreach(r => out.write(r.toBytes))
      }
      file
    }.seq.toSeq
  }
}
```

#### **Phase 4: Concurrent Shuffle**

```scala
class ShuffleManager(maxConcurrentTransfers: Int = 5) {
  private val transferSemaphore = new Semaphore(maxConcurrentTransfers)

  def shuffleInParallel(transferTasks: Seq[TransferTask]): Unit = {
    val futures = transferTasks.map { task =>
      Future {
        transferSemaphore.acquire()
        try {
          sendPartitionFile(task.file, task.target)
        } finally {
          transferSemaphore.release()
        }
      }
    }

    Await.result(Future.sequence(futures), 30.minutes)
  }
}
```

#### **Phase 5: Parallel Merge (ì„ íƒì‚¬í•­)**

```scala
class MergeManager(numThreads: Int) {
  def mergeMultiplePartitionsInParallel(
    partitionIds: Seq[Int]
  ): Unit = {
    // ì—¬ëŸ¬ partitionì„ ë™ì‹œì— merge
    partitionIds.par.foreach { partitionId =>
      mergePartition(partitionId)
    }
  }
}
```

### 2.4 Thread Pool í¬ê¸° ê²°ì •

```scala
object ThreadPoolConfig {
  private val numCores = Runtime.getRuntime.availableProcessors()

  // Sorting: CPU-bound â†’ numCores
  val sortingThreads: Int = numCores

  // Shuffle: I/O-bound â†’ 2 * numCores (ë˜ëŠ” worker ìˆ˜ ì œí•œ)
  val shuffleThreads: Int = math.min(numCores * 2, 10)

  // Merge: I/O-bound with some CPU â†’ 1.5 * numCores
  val mergeThreads: Int = (numCores * 1.5).toInt

  // Concurrent transfers limit (network bandwidth ê³ ë ¤)
  val maxConcurrentTransfers: Int = 5
}
```

---

## 3. Master ì¶œë ¥ í˜•ì‹ ê²°ì •

### 3.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸** (PDF Algorithm Phase 0):
```
print "Master IP:Port"
for each worker in workerList do
    print worker.IP
end for
```

**PDF ì˜ˆì‹œ**:
```
141.223.91.81        <- Master IP:Port
141.223.91.81        <- Worker 1 IP (í¬íŠ¸ ì—†ìŒ)
141.223.91.82        <- Worker 2 IP
141.223.91.83        <- Worker 3 IP
```

### 3.2 êµ¬í˜„ ê²°ì •

```
ì¶œë ¥ í˜•ì‹: 2 ì¤„
  Line 1: Masterì˜ IP:port
  Line 2: ëª¨ë“  Workerì˜ IP (ì½¤ë§ˆë¡œ êµ¬ë¶„, í¬íŠ¸ ì œì™¸)

ì˜ˆì‹œ (3 workers):
192.168.1.100:5000
192.168.1.10, 192.168.1.11, 192.168.1.12
```

**ê·¼ê±°**:
- PDF Algorithmì— ëª…í™•íˆ ëª…ì‹œë¨
- Master ì£¼ì†ŒëŠ” IP:Port í˜•ì‹
- Worker ì£¼ì†ŒëŠ” IPë§Œ (í¬íŠ¸ ì œì™¸)
- "in order" â†’ Worker ID ì˜¤ë¦„ì°¨ìˆœ (W0, W1, W2, ...)
- **ê° WorkerëŠ” ì—¬ëŸ¬ ì—°ì†ëœ partition ë‹´ë‹¹** (Nâ†’M Strategy)
  - ì´ìœ : **Merge ë‹¨ê³„ì—ì„œ ë©€í‹°ì½”ì–´ í™œìš©**
  - ì˜ˆ: 3 workers, 9 partitions
    - Worker 0: partition.0, 1, 2 (ì—°ì†ëœ 3ê°œ)
    - Worker 1: partition.3, 4, 5 (ì—°ì†ëœ 3ê°œ)
    - Worker 2: partition.6, 7, 8 (ì—°ì†ëœ 3ê°œ)
  - Range-based assignment: `workerID = partitionID / (numPartitions / numWorkers)`
  - ìì„¸í•œ ë‚´ìš©: `6-parallelization.md` Section 1.3 ì°¸ì¡°

### 3.3 ì •í™•í•œ êµ¬í˜„

```scala
class MasterNode {
  def outputFinalResult(): Unit = {
    // All workers completed merging
    phaseTracker.waitForPhase(WorkerPhase.PHASE_MERGING)

    logger.info("All workers completed. Outputting final result.")

    // Sort workers by ID (W0, W1, W2, ...)
    val sortedWorkers = registeredWorkers.asScala.toSeq
      .sortBy(_.workerId)

    // Output to STDOUT (not logger)
    // Line 1: Master IP:port
    println(s"${getMasterAddress()}:${actualPort}")

    // Line 2: Worker IPs (ì½¤ë§ˆë¡œ êµ¬ë¶„, í•œ ì¤„ì—, í¬íŠ¸ ì œì™¸)
    val workerIPs = sortedWorkers.map(_.address).mkString(", ")
    println(workerIPs)

    logger.info("Distributed sorting completed successfully!")
  }

  private def getMasterAddress(): String = {
    // Get the actual IP address of the master node
    // Option 1: Use environment variable
    sys.env.getOrElse("MASTER_HOST", {
      // Option 2: Auto-detect local IP
      InetAddress.getLocalHost.getHostAddress
    })
  }
}
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
$ sbt "runMain distsort.Main master 3"
[... logs ...]
192.168.1.100:5000
192.168.1.10, 192.168.1.11, 192.168.1.12
[2025-10-24 18:45:23] INFO: Distributed sorting completed successfully!
```

**í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©ë²•**:
```bash
# Master ì‹¤í–‰ ë° ì¶œë ¥ íŒŒì‹±
output=$(sbt "runMain distsort.Main master 3" 2>/dev/null)

# Line 1: Master address
master_addr=$(echo "$output" | sed -n '1p')
echo "Master: $master_addr"

# Line 2: Worker IPë“¤ (ì½¤ë§ˆë¡œ êµ¬ë¶„)
worker_line=$(echo "$output" | sed -n '2p')
IFS=', ' read -r -a workers <<< "$worker_line"

# ê° Workerì˜ partition íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ
for i in "${!workers[@]}"; do
  worker_ip="${workers[$i]}"
  echo "Downloading partitions from Worker $i at $worker_ip"

  # Workerê°€ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“  partition íŒŒì¼ ë‹¤ìš´ë¡œë“œ
  # Strategy B (range-based): Worker iëŠ” ì—°ì†ëœ partition ë²”ìœ„ ë‹´ë‹¹
  # ì˜ˆ: 3 workers, 9 partitions â†’ Worker 0 â†’ partition.0, partition.1, partition.2
  scp "${worker_ip}:/path/to/output/partition.*" ./
done
```

---

## 4. ì…ë ¥ íŒŒì¼ ì²˜ë¦¬ ë°©ì‹ ê²°ì •

### 4.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "Input files are distributed across workers in 32MB blocks"

**í•´ì„**:
- ê° WorkerëŠ” ì—¬ëŸ¬ input ë””ë ‰í† ë¦¬ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ
- ê° ë””ë ‰í† ë¦¬ ì•ˆì— ì—¬ëŸ¬ íŒŒì¼ ì¡´ì¬ ê°€ëŠ¥
- íŒŒì¼ë“¤ì€ ëŒ€ëµ 32MB ë‹¨ìœ„ë¡œ ë¶„í• ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ (í•˜ì§€ë§Œ ë³´ì¥ì€ ì•„ë‹˜)

### 4.2 êµ¬í˜„ ê²°ì •

```
ì…ë ¥ íŒŒì¼ ë°œê²¬ ë°©ì‹:
  1. ëª¨ë“  input ë””ë ‰í† ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
  2. ëª¨ë“  ì¼ë°˜ íŒŒì¼ì„ ì½ê¸° (ë””ë ‰í† ë¦¬ ì œì™¸)
  3. íŒŒì¼ í¬ê¸° ë¬´ê´€ (32MBëŠ” ì°¸ê³  ì‚¬í•­ì¼ ë¿)
```

```scala
class FileLayout {
  def getInputFiles: Seq[File] = {
    inputDirs.flatMap { dir =>
      val dirFile = new File(dir)
      if (!dirFile.exists()) {
        logger.warn(s"Input directory does not exist: $dir")
        Seq.empty
      } else if (!dirFile.isDirectory) {
        logger.warn(s"Input path is not a directory: $dir")
        Seq.empty
      } else {
        listFilesRecursively(dirFile)
          .filter(f => f.isFile)  // ì¼ë°˜ íŒŒì¼ë§Œ
      }
    }
  }

  private def listFilesRecursively(dir: File): Seq[File] = {
    val (files, subdirs) = dir.listFiles()
      .partition(_.isFile)

    val subFiles = subdirs.flatMap(listFilesRecursively)

    files ++ subFiles
  }
}
```

### 4.3 ì…ë ¥ ì½ê¸° ë°©ì‹

```scala
class InputReader(useAscii: Boolean) {
  private val recordReader = if (useAscii) {
    new AsciiRecordReader()
  } else {
    new BinaryRecordReader()
  }

  def readAllRecords(files: Seq[File]): Iterator[Record] = {
    files.iterator.flatMap { file =>
      logger.info(s"Reading file: ${file.getName}, size: ${file.length() / (1024 * 1024)} MB")

      val inputStream = new BufferedInputStream(
        new FileInputStream(file),
        BufferedIO.LARGE_BUFFER_SIZE  // 1 MB buffer
      )

      Iterator.continually {
        recordReader.readRecord(inputStream)
      }.takeWhile(_.isDefined).map(_.get)
    }
  }
}
```

---

## 5. Worker ìˆ˜ ê²°ì • ë¡œì§

### 5.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "Master expects N workers"

### 5.2 êµ¬í˜„ ê²°ì •

```
Master ì‹œì‘ ì‹œ Nì„ ëª…ì‹œ:
  - Masterê°€ ì •í™•íˆ Nê°œ worker ëŒ€ê¸°
  - Nê°œ ëª¨ë‘ ë“±ë¡ë  ë•Œê¹Œì§€ blocking
  - Timeout: 5ë¶„

Worker ë“±ë¡:
  - First N workersë§Œ ë°›ìŒ
  - N ì´ˆê³¼ ë“±ë¡ ì‹œë„ ê±°ë¶€
  - N ë¯¸ë§Œ ë“±ë¡ ì‹œ timeout í›„ ì—ëŸ¬
```

```scala
class MasterNode(expectedWorkers: Int) {
  private val registrationLatch = new CountDownLatch(expectedWorkers)
  private val registrationDeadline = System.currentTimeMillis() + (5 * 60 * 1000)

  override def registerWorker(request: RegisterRequest): Future[RegisterResponse] = {
    synchronized {
      val now = System.currentTimeMillis()

      // Check timeout
      if (now > registrationDeadline) {
        return Future.successful(RegisterResponse(
          success = false,
          workerId = "",
          totalWorkers = expectedWorkers,
          message = "Registration deadline exceeded"
        ))
      }

      // Check if already full
      if (registeredWorkers.size() >= expectedWorkers) {
        return Future.successful(RegisterResponse(
          success = false,
          workerId = "",
          totalWorkers = expectedWorkers,
          message = s"Already have $expectedWorkers workers registered"
        ))
      }

      // Register worker
      val workerId = s"W${workerIdCounter.getAndIncrement()}"
      val workerInfo = request.workerInfo.get.copy(workerId = workerId)

      registeredWorkers.add(workerInfo)
      registrationLatch.countDown()

      logger.info(s"Worker $workerId registered (${registeredWorkers.size()}/$expectedWorkers)")

      Future.successful(RegisterResponse(
        success = true,
        workerId = workerId,
        totalWorkers = expectedWorkers,
        message = "Registration successful"
      ))
    }
  }

  def waitForAllWorkers(): Unit = {
    logger.info(s"Waiting for $expectedWorkers workers to register...")

    val registered = registrationLatch.await(5, TimeUnit.MINUTES)

    if (!registered) {
      val actual = registeredWorkers.size()
      throw new TimeoutException(
        s"Only $actual/$expectedWorkers workers registered within timeout"
      )
    }

    logger.info(s"All $expectedWorkers workers registered successfully")
  }
}
```

---

## 6. ê¸°íƒ€ ëª…í™•í™” ì‚¬í•­

### 6.1 ASCII vs Binary í˜•ì‹

**PDF ìš”êµ¬ì‚¬í•­**:
> "Should work on both ASCII and binary input **without requiring an option**"

**êµ¬í˜„ ê²°ì •**: **ìë™ ê°ì§€ (Auto-detection)**

#### 6.1.1 ì…ë ¥ í˜•ì‹ ì •ì˜

```
ASCII:
  - ê° recordëŠ” 100 characters
  - Key: 10 characters (ASCII printable)
  - Value: 90 characters (ASCII printable)
  - Line ending: \n (Unix) or \r\n (Windows) - ë‘˜ ë‹¤ ì§€ì›

Binary:
  - ê° recordëŠ” ì •í™•íˆ 100 bytes
  - Key: 10 bytes (any byte value)
  - Value: 90 bytes (any byte value)
  - No line endings
```

**ì¤‘ìš”**:
- **ì •ë ¬ ì‹œ ì²˜ìŒ 10 bytes (Key)ë§Œ ë¹„êµ**ì— ì‚¬ìš©ë©ë‹ˆë‹¤
- ë‚˜ë¨¸ì§€ 90 bytes (Value)ëŠ” ì •ë ¬ì— ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë©°, ë‹¨ìˆœíˆ Keyì™€ í•¨ê»˜ ì´ë™í•©ë‹ˆë‹¤
- ì´ëŠ” gensort í‘œì¤€ í˜•ì‹ìœ¼ë¡œ, valsortë¡œ ê²€ì¦í•  ë•Œë„ ì´ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤

#### 6.1.2 ìë™ ê°ì§€ ì•Œê³ ë¦¬ì¦˜

```scala
object InputFormatDetector {
  /**
   * íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ë¥¼ ì½ì–´ì„œ ASCII vs Binary íŒë³„
   *
   * ì•Œê³ ë¦¬ì¦˜:
   *   1. íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ì½ê¸°
   *   2. ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
   *   3. ë¹„ìœ¨ > 0.9 â†’ ASCII, ê·¸ ì™¸ â†’ Binary
   */
  def detectFormat(file: File): InputFormat = {
    val buffer = new Array[Byte](1000)
    val inputStream = new FileInputStream(file)

    try {
      val bytesRead = inputStream.read(buffer)

      if (bytesRead <= 0) {
        logger.warn(s"Empty file: ${file.getName}, defaulting to Binary")
        return InputFormat.BINARY
      }

      // ASCII printable: 0x20-0x7E (space ~ ~), plus \n (0x0A), \r (0x0D)
      val asciiLikeCount = buffer.take(bytesRead).count { b =>
        (b >= 32 && b <= 126) || b == '\n' || b == '\r'
      }

      val asciiRatio = asciiLikeCount.toDouble / bytesRead

      logger.debug(s"File: ${file.getName}, ASCII ratio: $asciiRatio")

      if (asciiRatio > 0.9) {
        logger.info(s"Detected ASCII format for file: ${file.getName}")
        InputFormat.ASCII
      } else {
        logger.info(s"Detected Binary format for file: ${file.getName}")
        InputFormat.BINARY
      }
    } finally {
      inputStream.close()
    }
  }
}

sealed trait InputFormat
object InputFormat {
  case object ASCII extends InputFormat
  case object BINARY extends InputFormat
}
```

#### 6.1.3 RecordReader êµ¬í˜„

```scala
trait RecordReader {
  def readRecord(input: InputStream): Option[Record]
}

class BinaryRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Record] = {
    val buffer = new Array[Byte](100)
    val bytesRead = input.read(buffer)

    if (bytesRead == -1) {
      None
    } else if (bytesRead < 100) {
      logger.warn(s"Incomplete record: only $bytesRead bytes")
      None
    } else {
      val key = buffer.slice(0, 10)
      val value = buffer.slice(10, 100)
      Some(Record(key, value))
    }
  }
}

class AsciiRecordReader extends RecordReader {
  private val lineReader = new BufferedReader(new InputStreamReader(input))

  override def readRecord(input: InputStream): Option[Record] = {
    val line = lineReader.readLine()

    if (line == null) {
      None
    } else if (line.length < 100) {
      logger.warn(s"Incomplete line: only ${line.length} characters")
      None
    } else {
      val key = line.substring(0, 10).getBytes(StandardCharsets.UTF_8)
      val value = line.substring(10, 100).getBytes(StandardCharsets.UTF_8)
      Some(Record(key, value))
    }
  }
}
```

#### 6.1.4 ì‚¬ìš© ì˜ˆì‹œ

```scala
class InputReader {
  def readAllRecords(files: Seq[File]): Iterator[Record] = {
    files.iterator.flatMap { file =>
      // Auto-detect format for each file
      val format = InputFormatDetector.detectFormat(file)

      val recordReader = format match {
        case InputFormat.ASCII => new AsciiRecordReader()
        case InputFormat.BINARY => new BinaryRecordReader()
      }

      logger.info(s"Reading file: ${file.getName} (format: $format)")

      val inputStream = new BufferedInputStream(
        new FileInputStream(file),
        BufferedIO.LARGE_BUFFER_SIZE
      )

      Iterator.continually {
        recordReader.readRecord(inputStream)
      }.takeWhile(_.isDefined).map(_.get)
    }
  }
}
```

#### 6.1.5 í˜¼í•© ì…ë ¥ ì²˜ë¦¬

```
ì‹œë‚˜ë¦¬ì˜¤: Workerê°€ ì—¬ëŸ¬ input ë””ë ‰í† ë¦¬ë¥¼ ê°€ì§ˆ ë•Œ
  - /data/input1/file1.bin (Binary)
  - /data/input1/file2.txt (ASCII)
  - /data/input2/file3.bin (Binary)

ì²˜ë¦¬ ë°©ì‹:
  âœ… ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ format detection
  âœ… ASCIIì™€ Binary íŒŒì¼ í˜¼ì¬ ê°€ëŠ¥
  âœ… ë™ì¼í•œ Key/Value ë°”ì´íŠ¸ í‘œí˜„ìœ¼ë¡œ ì •ë ¬
```

**ì¤‘ìš”**: ëª…ë ¹í–‰ ì˜µì…˜ìœ¼ë¡œ `--ascii` ë˜ëŠ” `--binary`ë¥¼ ì§€ì •í•˜ì§€ **ì•ŠìŒ**. ëª¨ë“  íŒŒì¼ì€ ìë™ ê°ì§€ë¨.

### 6.2 Temporary íŒŒì¼ ìœ„ì¹˜

```
ê¸°ë³¸ê°’:
  - <output-dir>/.temp/

ëª…ë ¹í–‰ ì˜µì…˜:
  --temp <directory>

ì˜ˆì‹œ:
  sbt "runMain distsort.Main worker master:5000 \
    -I /data/input1,/data/input2 \
    -O /data/output \
    --temp /tmp/sorting"
```

### 6.3 í¬íŠ¸ ë²ˆí˜¸

```
Master:
  - ê¸°ë³¸ê°’: ì—†ìŒ (ëª…ë ¹í–‰ì—ì„œ ì§€ì • ë¶ˆí•„ìš”)
  - gRPC ì„œë²„ ëœë¤ í¬íŠ¸ bind
  - ì‹¤ì œ bindëœ í¬íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥

Worker:
  - ê¸°ë³¸ê°’: ì—†ìŒ
  - gRPC ì„œë²„ ëœë¤ í¬íŠ¸ bind
  - RegisterRequestì— ì‹¤ì œ í¬íŠ¸ í¬í•¨
```

```scala
class MasterNode(expectedWorkers: Int) {
  private val server = ServerBuilder.forPort(0)  // 0 = random port
    .addService(MasterServiceGrpc.bindService(this, executionContext))
    .build()
    .start()

  val actualPort: Int = server.getPort

  logger.info(s"Master started on port $actualPort")
  println(s"MASTER_PORT=$actualPort")  // For scripting
}
```

---

## 7. ì •ë¦¬ ë° ìš°ì„ ìˆœìœ„

### 7.1 í•µì‹¬ ê²°ì • ì‚¬í•­

| í•­ëª© | ê²°ì • | ë³µì¡ë„ | ìš°ì„ ìˆœìœ„ |
|------|------|--------|---------|
| Fault Tolerance | ì „ì²´ ì¬ì‹œì‘ (Phase 1-2ëŠ” ë¶€ë¶„ ë³µêµ¬) | Medium | ğŸ”´ High |
| ë³‘ë ¬ ì²˜ë¦¬ | Phaseë³„ ë³‘ë ¬í™”, numCores ê¸°ë°˜ | Low | ğŸ”´ High |
| Master ì¶œë ¥ | `address:port` í˜•ì‹, Worker ID ìˆœ | Low | ğŸ”´ High |
| ì…ë ¥ íŒŒì¼ | ì¬ê·€ íƒìƒ‰ | Low | ğŸ”´ High |
| Worker ìˆ˜ | ê³ ì • N, 5ë¶„ timeout | Low | ğŸ”´ High |
| ASCII/Binary | ë³„ë„ RecordReader, 100 char/byte | Low | ğŸ”´ High |

### 7.2 êµ¬í˜„ ìˆœì„œ

```
Milestone 1: Infrastructure
  âœ… Master/Worker skeleton
  âœ… gRPC setup
  âœ… Worker registration (N workers, 5min timeout)
  âœ… Port randomization
  âœ… Master ì¶œë ¥ í˜•ì‹

Milestone 2: Basic Sorting (Strategy A)
  âœ… ì…ë ¥ íŒŒì¼ ì¬ê·€ íƒìƒ‰
  âœ… ASCII/Binary RecordReader
  âœ… Sampling
  âœ… External sorting (sequential)
  âœ… Partitioning (Nâ†’N)
  âœ… Shuffle
  âœ… Merge

Milestone 3: Parallelization
  âœ… Parallel chunk sorting
  âœ… Concurrent shuffle transfers
  âœ… Thread pool configuration

Milestone 4: Fault Tolerance
  âœ… Heartbeat mechanism
  âœ… Health checking
  âœ… Graceful degradation (Phase 1-2)
  âœ… Job restart (Phase 3-4)
  âœ… State-based cleanup

Milestone 5: Advanced Features (Optional)
  âš ï¸ Strategy B (Nâ†’M partitioning)
  âš ï¸ Shuffle output replication
  âš ï¸ Checkpoint-based recovery
```

---

## 8. Additional Requirements (ì¶”ê°€ ìš”êµ¬ì‚¬í•­) âš ï¸ ë§¤ìš° ì¤‘ìš”

### 8.1 ì…ë ¥/ì¶œë ¥ í¬ë§· ìë™ ê°ì§€ (Requirement 1)

**ìš”êµ¬ì‚¬í•­**:
> "Should work on both ASCII and binary input **without requiring an option**"

**êµ¬í˜„**:
- âœ… ì´ë¯¸ êµ¬í˜„ë¨ (Section 6.1 ì°¸ì¡°)
- `InputFormatDetector`ê°€ íŒŒì¼ ë‚´ìš© ë¶„ì„í•˜ì—¬ ìë™ ê°ì§€
- ASCII ratio > 0.9 â†’ ASCII, ê·¸ ì™¸ â†’ Binary

### 8.2 ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸ (Requirement 2) âš ï¸

**ìš”êµ¬ì‚¬í•­**:
> "Input directories are shared and should not be updated"
> - **do not try to delete input files**
> - **do not create new files in the input directories**

**êµ¬í˜„ ì›ì¹™**:
```scala
class FileLayout(inputDirs: Seq[Path], outputDir: Path, tempBaseDir: Path) {
  // âœ… CORRECT: Read-only access to input directories
  def getInputFiles: Seq[File] = {
    inputDirs.flatMap { dir =>
      Files.walk(dir)
        .filter(Files.isRegularFile(_))
        .map(_.toFile)
        .toSeq
    }
  }

  // âŒ NEVER DO THIS: Modify input directories
  // def deleteInputFile(file: File): Unit = file.delete()  // ê¸ˆì§€!
  // def createTempInInput(dir: Path): File = ...          // ê¸ˆì§€!
}
```

**ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ì„ ì½ê¸°ë§Œ í•˜ëŠ”ê°€?
- [ ] ì…ë ¥ ë””ë ‰í† ë¦¬ì— ì„ì‹œ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠëŠ”ê°€?
- [ ] ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì„ ì‚­ì œ/ì´ë™í•˜ì§€ ì•ŠëŠ”ê°€?
- [ ] ëª¨ë“  ì„ì‹œ íŒŒì¼ì€ `tempBaseDir`ì—ë§Œ ìƒì„±í•˜ëŠ”ê°€?

### 8.3 ì„ì‹œ íŒŒì¼ ì •ë¦¬ (Requirement 3)

**ìš”êµ¬ì‚¬í•­**:
> "Output directories should contain only final output files"
> - It is okay to create temporary files/directories
> - **but delete them in the end**

**êµ¬í˜„**:
```scala
// ëª¨ë“  ì„ì‹œ íŒŒì¼ì€ .tmp í™•ì¥ì ì‚¬ìš©
val tempFile = new File(outputDir, s"partition.$id.tmp")

// Phase ì™„ë£Œ ì‹œ cleanup
fileLayout.cleanupSortedChunks()      // Sorting phase í›„
fileLayout.cleanupLocalPartitions()   // Shuffle phase í›„
fileLayout.cleanupReceivedPartitions() // Merge phase í›„

// Job ì™„ë£Œ ë˜ëŠ” ì—ëŸ¬ ì‹œ ì „ì²´ cleanup
fileLayout.cleanupTemporaryFiles()
```

**ìµœì¢… ê²€ì¦**:
```bash
# Output directory should only contain partition.* files
ls $OUTPUT_DIR
# ì˜ˆìƒ ì¶œë ¥: partition.0, partition.1, partition.2, ...
# ê¸ˆì§€: partition.0.tmp, chunk_0000.sorted, .sorting/, etc.
```

**ìì„¸í•œ ë‚´ìš©**: `5-file-management.md` Section 3, 7 ì°¸ì¡°

### 8.4 í¬íŠ¸ ë™ì  í• ë‹¹ (Requirement 4)

**ìš”êµ¬ì‚¬í•­**:
> "Do not assume a specific port (e.g., hard-coded)"
> - Running multiple workers (with different input/output directories) should work

**êµ¬í˜„**:
```scala
// âŒ WRONG: Hard-coded port
val server = ServerBuilder.forPort(50051).build()

// âœ… CORRECT: OS-assigned port (port 0)
val server = ServerBuilder.forPort(0).build()
val actualPort = server.getPort  // OSê°€ í• ë‹¹í•œ ì‹¤ì œ í¬íŠ¸

// Masterì— ë“±ë¡ ì‹œ ì‹¤ì œ í¬íŠ¸ ì „ë‹¬
val request = RegisterRequest(
  workerInfo = Some(WorkerInfo(
    workerId = "",
    address = getLocalAddress(),
    port = actualPort  // ë™ì  í• ë‹¹ëœ í¬íŠ¸
  ))
)
```

**ì´ìœ **:
- ê°™ì€ ë¨¸ì‹ ì—ì„œ ì—¬ëŸ¬ Worker ì‹¤í–‰ ê°€ëŠ¥
- í¬íŠ¸ ì¶©ëŒ ë°©ì§€
- í…ŒìŠ¤íŠ¸ ìë™í™” ìš©ì´

### 8.5 ì…ë ¥ ë¸”ë¡ í¬ê¸° ê°€ì • ê¸ˆì§€ (Requirement 5) âš ï¸

**ìš”êµ¬ì‚¬í•­**:
> "Input blocks of 32MB each on each worker"
> - **do not rely on the assumption of 32MB**

**ì˜ëª»ëœ ê°€ì •**:
```scala
// âŒ WRONG: Assuming 32MB blocks
val chunkSize = 32 * 1024 * 1024  // 32MB
val numChunks = totalSize / chunkSize

// âŒ WRONG: Allocating fixed arrays
val buffer = new Array[Byte](32 * 1024 * 1024)
```

**ì˜¬ë°”ë¥¸ êµ¬í˜„**:
```scala
// âœ… CORRECT: Dynamic block size based on memory
val availableMemory = Runtime.getRuntime.maxMemory()
val chunkSize = math.min(availableMemory / 10, 100 * 1024 * 1024)

// âœ… CORRECT: Read files regardless of size
def readInputFiles(): Iterator[Record] = {
  inputFiles.iterator.flatMap { file =>
    val reader = new RecordReader(file)
    Iterator.continually(reader.readRecord())
      .takeWhile(_.isDefined)
      .map(_.get)
  }
}
```

**ì¤‘ìš”**:
- ì…ë ¥ íŒŒì¼ í¬ê¸°ëŠ” **ì•Œ ìˆ˜ ì—†ìŒ**
- 1KB ~ 10GB ì–´ë–¤ í¬ê¸°ë„ ê°€ëŠ¥
- ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²­í¬ í¬ê¸° ë™ì  ê²°ì •
- ì „ì²´ íŒŒì¼ í¬ê¸°ì— ì˜ì¡´í•˜ì§€ ë§ ê²ƒ

### 8.6 Worker í¬ë˜ì‹œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (Fault Tolerance)

**í…ŒìŠ¤íŠ¸ ìš”êµ¬ì‚¬í•­**:
> "We will kill one of your workers during the experiment"

**ì‹œë‚˜ë¦¬ì˜¤**:
1. Worker í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ ê°‘ìê¸° ì¢…ë£Œ (kill -9)
2. ëª¨ë“  ì¤‘ê°„ ë°ì´í„° ì†ì‹¤
3. ìƒˆ Workerê°€ ê°™ì€ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œì‘
4. ê¸°ëŒ€: ìµœì¢… ê²°ê³¼ëŠ” ë™ì¼

**ëŒ€ì‘ ì „ëµ**:
- âœ… Worker Re-registration (Section 1.4 ì°¸ì¡°)
- âœ… State-based cleanup on restart
- âœ… Idempotent operations
- âœ… Atomic file operations

**ìì„¸í•œ ë‚´ìš©**: `4-error-recovery.md` ì°¸ì¡°

---

## 9. ë¬¸ì„œ í˜„í™©

### ì„¤ê³„ ë¬¸ì„œ (Design Docs)
- âœ… docs/0-implementation-decisions.md (êµ¬í˜„ ê²°ì • ì‚¬í•­ + Additional Requirements)
- âœ… docs/1-phase-coordination.md (Phase ë™ê¸°í™”)
- âœ… docs/2-worker-state-machine.md (Worker ìƒíƒœ ë¨¸ì‹ )
- âœ… docs/3-grpc-sequences.md (gRPC ì‹œí€€ìŠ¤)
- âœ… docs/4-error-recovery.md (ì¥ì•  ë³µêµ¬)
- âœ… docs/5-file-management.md (íŒŒì¼ ê´€ë¦¬)
- âœ… docs/6-parallelization.md (ë³‘ë ¬ ì²˜ë¦¬ + Nâ†’M Strategy)
- âœ… docs/7-testing-strategy.md (í…ŒìŠ¤íŠ¸ ì „ëµ + TDD ê°€ì´ë“œ) â­ NEW

### ê³„íš ë¬¸ì„œ (Planning Docs)
- âœ… plan/2025-10-24_plan_ver3.md (ì „ì²´ ì„¤ê³„ + Critical Requirements)
- âœ… plan/quickstart-tdd-guide.md (TDD ê°œë°œ QuickStart) â­ NEW

### ì£¼ìš” ê°œì„  ì‚¬í•­ (v4)
- âœ… gensort/valsort ì™„ì „ ë¬¸ì„œí™” (ê³µì‹ ì‚¬ì´íŠ¸, ì‚¬ìš©ë²•, í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤)
- âœ… Nâ†’M Strategy í•µì‹¬ ê°œë… ëª…í™•í™” (Merge ë³‘ë ¬í™” ëª©ì )
- âœ… 6ê°€ì§€ Critical Requirements ì¶”ê°€ (ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸, 32MB ê°€ì • ê¸ˆì§€ ë“±)
- âœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì „ëµ ë¬¸ì„œ (Unit/Integration/E2E/Performance/Fault Tolerance)
- âœ… TDD QuickStart ê°€ì´ë“œ (í”„ë¡œì íŠ¸ ì„¤ì • â†’ ì²« í…ŒìŠ¤íŠ¸ â†’ Red-Green-Refactor)

---

**ë¬¸ì„œ ì™„ì„±ë„**: v4 - êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ (TDD ê°€ì´ë“œ í¬í•¨) ğŸš€
