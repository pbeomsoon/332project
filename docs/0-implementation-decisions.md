# Implementation Decisions (êµ¬í˜„ ê²°ì • ì‚¬í•­)

**ì‘ì„±ì¼**: 2025-10-24
**ëª©ì **: PDF ìš”êµ¬ì‚¬í•­ í•´ì„ ë° ëª¨í˜¸í•œ ë¶€ë¶„ì— ëŒ€í•œ ëª…í™•í•œ êµ¬í˜„ ê²°ì •

---

## 1. Fault Tolerance ì „ëµ ê²°ì •

### 1.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "The system must be fault-tolerant, which means that if a worker crashes and restarts, the overall computation should still produce correct results."

**í•µì‹¬ ì§ˆë¬¸**: "Worker crash & restart í›„ ì–´ë–»ê²Œ ë³µêµ¬?"

### 1.2 êµ¬í˜„ ê²°ì •: **Graceful Degradation with Job Restart**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fault Tolerance Strategy                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 1-2 (Sampling/Sorting):                          â”‚
â”‚   âœ… Continue with N-1 workers if >50% alive           â”‚
â”‚   âœ… Recompute boundaries with remaining samples        â”‚
â”‚                                                         â”‚
â”‚ Phase 3-4 (Shuffle/Merge):                             â”‚
â”‚   âŒ Cannot continue - restart entire job               â”‚
â”‚   âœ… Worker restarts and re-registers from beginning    â”‚
â”‚                                                         â”‚
â”‚ Data Integrity:                                         â”‚
â”‚   âœ… Atomic writes (temp + rename)                      â”‚
â”‚   âœ… State-based cleanup on failure                     â”‚
â”‚   âœ… Idempotent operations                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ì •ë‹¹í™” (Justification)

**ì™œ "ì „ì²´ ì¬ì‹œì‘"ì„ ì„ íƒí–ˆë‚˜?**

1. **êµ¬í˜„ ë³µì¡ë„**: Partial recoveryëŠ” Lineage tracking + Checkpoint í•„ìš” â†’ í”„ë¡œì íŠ¸ ë²”ìœ„ ì´ˆê³¼
2. **ì •í™•ì„± ìš°ì„ **: ë¶€ë¶„ ë³µêµ¬ëŠ” corner caseê°€ ë§ìŒ â†’ ì „ì²´ ì¬ì‹œì‘ì´ ë” ì•ˆì „
3. **ì‹¤ì œ ì‹œìŠ¤í…œ ì‚¬ë¡€**:
   - Hadoop ì´ˆê¸° ë²„ì „ë„ JobTracker SPOFì˜€ìŒ
   - ë§ì€ batch processing ì‹œìŠ¤í…œì´ job-level restart ì‚¬ìš©
4. **PDF í•´ì„**: "produce correct results" â† ì„±ëŠ¥ë³´ë‹¤ ì •í™•ì„± ê°•ì¡°

### 1.4 Worker Restart ì‹œë‚˜ë¦¬ì˜¤

```
ì‹œë‚˜ë¦¬ì˜¤ 1: Worker crashes during Sampling (Phase 2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Before crash:
  - 5 workers registered
  - 3 workers submitted samples
  - Worker 2 crashes

Detection:
  - Master: No heartbeat from Worker 2 for 60s
  - Master: Sampling phase timeout (10 min)

Recovery:
  - Wait for remaining 4 workers to complete sampling
  - If 4 â‰¥ 5 * 0.5 (YES):
      â†’ Continue with 4 workers
      â†’ Recompute boundaries with 4 samples
      â†’ Set numPartitions = 4
  - Else:
      â†’ Abort job

Worker 2 restarts:
  - Tries to re-register
  - Master rejects: "Registration closed, current job in progress"
  - Worker 2 waits for next job
```

```
ì‹œë‚˜ë¦¬ì˜¤ 2: Worker crashes during Shuffle (Phase 4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Before crash:
  - Worker 2 is sending partition.5 to Worker 3
  - Transfer is 50% complete
  - Worker 2 crashes

Detection:
  - Worker 3: Partial file received, no completion signal
  - Worker 3: Timeout after 60s, cleanup temp file
  - Master: No heartbeat from Worker 2 for 60s

Recovery:
  âŒ Cannot continue:
      - Worker 2 was responsible for partition.2
      - partition.2 data is lost (only in Worker 2's temp dir)
      - Other workers are waiting for partition.2

Action:
  - Master sends AbortRequest to all workers
  - All workers cleanup temporary files
  - Job restarts from Phase 1

Worker 2 restarts:
  - Re-registers for new job
  - Starts from Phase 1 with all other workers
```

### 1.5 ê°œì„  ê°€ëŠ¥ì„± (í–¥í›„)

**Milestone 4-5ì—ì„œ ì„ íƒì  êµ¬í˜„**:

```scala
// Option A: Shuffle Output Replication (ê°„ë‹¨)
class ShuffleManager {
  def sendPartitionWithBackup(partition: File): Unit = {
    val primary = getPrimaryWorker(partitionId)
    val backup = getBackupWorker(partitionId)

    // Send to both
    Future.sequence(Seq(
      sendTo(primary, partition),
      sendTo(backup, partition)
    ))
  }
}

// Primary ì‹¤íŒ¨ ì‹œ backupì—ì„œ ë³µêµ¬ ê°€ëŠ¥
```

```scala
// Option B: Checkpoint-based Recovery (ë³µì¡)
class MasterNode {
  def checkpoint(): Unit = {
    val state = CheckpointState(
      phase = currentPhase,
      workers = registeredWorkers,
      partitionConfig = config,
      completions = phaseTracker.getAll
    )
    saveToFile(state)
  }

  def recover(): Unit = {
    val state = loadCheckpoint()
    // Resume from last completed phase
    resumeFromPhase(state.phase)
  }
}
```

**ê²°ì •**: Milestone 1-3ì—ì„œëŠ” "ì „ì²´ ì¬ì‹œì‘", Milestone 4-5ì—ì„œ ì„ íƒì  ê°œì„ 

---

## 2. ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ ê²°ì •

### 2.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "Utilize multi-core processors for parallel processing"

### 2.2 êµ¬í˜„ ê²°ì •: **Phaseë³„ ë³‘ë ¬í™”**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parallelization Strategy                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Thread Pool Configuration:                              â”‚
â”‚   - numCores = Runtime.getRuntime.availableProcessors() â”‚
â”‚   - sortingThreads = numCores                           â”‚
â”‚   - shuffleThreads = min(numCores, numWorkers)          â”‚
â”‚   - mergeThreads = numCores                             â”‚
â”‚                                                         â”‚
â”‚ Phaseë³„ ë³‘ë ¬í™”:                                          â”‚
â”‚   Phase 2 (Sorting):    Parallel chunk sorting         â”‚
â”‚   Phase 3 (Partition):  Parallel partition writing     â”‚
â”‚   Phase 4 (Shuffle):    Concurrent file transfers      â”‚
â”‚   Phase 5 (Merge):      Parallel K-way merge           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 êµ¬ì²´ì  êµ¬í˜„

#### **Phase 2: Parallel Sorting**

```scala
class ExternalSorter(numThreads: Int = Runtime.getRuntime.availableProcessors()) {
  private val executor = Executors.newFixedThreadPool(numThreads)

  def sortInParallel(chunks: Seq[Chunk]): Seq[File] = {
    val futures = chunks.map { chunk =>
      Future {
        val sorted = chunk.records.sortWith((a, b) =>
          ByteArrayOrdering.compare(a.key, b.key) < 0
        )
        writeSortedChunk(sorted)
      }(ExecutionContext.fromExecutor(executor))
    }

    Await.result(Future.sequence(futures), Duration.Inf)
  }
}
```

#### **Phase 3: Parallel Partitioning**

```scala
class Partitioner(numThreads: Int) {
  def writePartitionsInParallel(
    partitions: Map[Int, Seq[Record]]
  ): Seq[File] = {
    // ê° partitionì„ ë³‘ë ¬ë¡œ ì“°ê¸°
    partitions.par.map { case (partitionId, records) =>
      val file = fileLayout.getLocalPartitionFile(partitionId)
      FileOperations.atomicWriteStream(file) { out =>
        records.foreach(r => out.write(r.toBytes))
      }
      file
    }.seq.toSeq
  }
}
```

#### **Phase 4: Concurrent Shuffle**

```scala
class ShuffleManager(maxConcurrentTransfers: Int = 5) {
  private val transferSemaphore = new Semaphore(maxConcurrentTransfers)

  def shuffleInParallel(transferTasks: Seq[TransferTask]): Unit = {
    val futures = transferTasks.map { task =>
      Future {
        transferSemaphore.acquire()
        try {
          sendPartitionFile(task.file, task.target)
        } finally {
          transferSemaphore.release()
        }
      }
    }

    Await.result(Future.sequence(futures), 30.minutes)
  }
}
```

#### **Phase 5: Parallel Merge (ì„ íƒì‚¬í•­)**

```scala
class MergeManager(numThreads: Int) {
  def mergeMultiplePartitionsInParallel(
    partitionIds: Seq[Int]
  ): Unit = {
    // ì—¬ëŸ¬ partitionì„ ë™ì‹œì— merge
    partitionIds.par.foreach { partitionId =>
      mergePartition(partitionId)
    }
  }
}
```

### 2.4 Thread Pool í¬ê¸° ê²°ì •

```scala
object ThreadPoolConfig {
  private val numCores = Runtime.getRuntime.availableProcessors()

  // Sorting: CPU-bound â†’ numCores
  val sortingThreads: Int = numCores

  // Shuffle: I/O-bound â†’ 2 * numCores (ë˜ëŠ” worker ìˆ˜ ì œí•œ)
  val shuffleThreads: Int = math.min(numCores * 2, 10)

  // Merge: I/O-bound with some CPU â†’ 1.5 * numCores
  val mergeThreads: Int = (numCores * 1.5).toInt

  // Concurrent transfers limit (network bandwidth ê³ ë ¤)
  val maxConcurrentTransfers: Int = 5
}
```

---

## 3. Master ì¶œë ¥ í˜•ì‹ ê²°ì •

### 3.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸** (PDF Algorithm Phase 0):
```
print "Master IP:Port"
for each worker in workerList do
    print worker.IP
end for
```

**PDF ì˜ˆì‹œ**:
```
141.223.91.81        <- Master IP:Port
141.223.91.81        <- Worker 1 IP (í¬íŠ¸ ì—†ìŒ)
141.223.91.82        <- Worker 2 IP
141.223.91.83        <- Worker 3 IP
```

### 3.2 êµ¬í˜„ ê²°ì •

```
ì¶œë ¥ í˜•ì‹: 2 ì¤„
  Line 1: Masterì˜ IP:port
  Line 2: ëª¨ë“  Workerì˜ IP (ì½¤ë§ˆë¡œ êµ¬ë¶„, í¬íŠ¸ ì œì™¸)

ì˜ˆì‹œ (3 workers):
192.168.1.100:5000
192.168.1.10, 192.168.1.11, 192.168.1.12
```

**ê·¼ê±°**:
- PDF Algorithmì— ëª…í™•íˆ ëª…ì‹œë¨
- Master ì£¼ì†ŒëŠ” IP:Port í˜•ì‹
- Worker ì£¼ì†ŒëŠ” IPë§Œ (í¬íŠ¸ ì œì™¸)
- "in order" â†’ Worker ID ì˜¤ë¦„ì°¨ìˆœ (W0, W1, W2, ...)
- **ê° WorkerëŠ” ì—¬ëŸ¬ ì—°ì†ëœ partition ë‹´ë‹¹** (Nâ†’M Strategy)
  - ì´ìœ : **Merge ë‹¨ê³„ì—ì„œ ë©€í‹°ì½”ì–´ í™œìš©**
  - ì˜ˆ: 3 workers, 9 partitions
    - Worker 0: partition.0, 1, 2 (ì—°ì†ëœ 3ê°œ)
    - Worker 1: partition.3, 4, 5 (ì—°ì†ëœ 3ê°œ)
    - Worker 2: partition.6, 7, 8 (ì—°ì†ëœ 3ê°œ)
  - Range-based assignment: `workerID = partitionID / (numPartitions / numWorkers)`
  - ìì„¸í•œ ë‚´ìš©: `6-parallelization.md` Section 1.3 ì°¸ì¡°

### 3.3 ì •í™•í•œ êµ¬í˜„

```scala
class MasterNode {
  def outputFinalResult(): Unit = {
    // All workers completed merging
    phaseTracker.waitForPhase(WorkerPhase.PHASE_MERGING)

    logger.info("All workers completed. Outputting final result.")

    // Sort workers by ID (W0, W1, W2, ...)
    val sortedWorkers = registeredWorkers.asScala.toSeq
      .sortBy(_.workerId)

    // Output to STDOUT (not logger)
    // Line 1: Master IP:port
    println(s"${getMasterAddress()}:${actualPort}")

    // Line 2: Worker IPs (ì½¤ë§ˆë¡œ êµ¬ë¶„, í•œ ì¤„ì—, í¬íŠ¸ ì œì™¸)
    val workerIPs = sortedWorkers.map(_.address).mkString(", ")
    println(workerIPs)

    logger.info("Distributed sorting completed successfully!")
  }

  private def getMasterAddress(): String = {
    // Get the actual IP address of the master node
    // Option 1: Use environment variable
    sys.env.getOrElse("MASTER_HOST", {
      // Option 2: Auto-detect local IP
      InetAddress.getLocalHost.getHostAddress
    })
  }
}
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
$ sbt "runMain distsort.Main master 3"
[... logs ...]
192.168.1.100:5000
192.168.1.10, 192.168.1.11, 192.168.1.12
[2025-10-24 18:45:23] INFO: Distributed sorting completed successfully!
```

**í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©ë²•**:
```bash
# Master ì‹¤í–‰ ë° ì¶œë ¥ íŒŒì‹±
output=$(sbt "runMain distsort.Main master 3" 2>/dev/null)

# Line 1: Master address
master_addr=$(echo "$output" | sed -n '1p')
echo "Master: $master_addr"

# Line 2: Worker IPë“¤ (ì½¤ë§ˆë¡œ êµ¬ë¶„)
worker_line=$(echo "$output" | sed -n '2p')
IFS=', ' read -r -a workers <<< "$worker_line"

# ê° Workerì˜ partition íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ
for i in "${!workers[@]}"; do
  worker_ip="${workers[$i]}"
  echo "Downloading partitions from Worker $i at $worker_ip"

  # Workerê°€ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“  partition íŒŒì¼ ë‹¤ìš´ë¡œë“œ
  # Strategy B (range-based): Worker iëŠ” ì—°ì†ëœ partition ë²”ìœ„ ë‹´ë‹¹
  # ì˜ˆ: 3 workers, 9 partitions â†’ Worker 0 â†’ partition.0, partition.1, partition.2
  scp "${worker_ip}:/path/to/output/partition.*" ./
done
```

---

## 4. ì…ë ¥ íŒŒì¼ ì²˜ë¦¬ ë°©ì‹ ê²°ì •

### 4.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "Input files are distributed across workers in 32MB blocks"

**í•´ì„**:
- ê° WorkerëŠ” ì—¬ëŸ¬ input ë””ë ‰í† ë¦¬ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ
- ê° ë””ë ‰í† ë¦¬ ì•ˆì— ì—¬ëŸ¬ íŒŒì¼ ì¡´ì¬ ê°€ëŠ¥
- íŒŒì¼ë“¤ì€ ëŒ€ëµ 32MB ë‹¨ìœ„ë¡œ ë¶„í• ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ (í•˜ì§€ë§Œ ë³´ì¥ì€ ì•„ë‹˜)

### 4.2 êµ¬í˜„ ê²°ì •

```
ì…ë ¥ íŒŒì¼ ë°œê²¬ ë°©ì‹:
  1. ëª¨ë“  input ë””ë ‰í† ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
  2. ëª¨ë“  ì¼ë°˜ íŒŒì¼ì„ ì½ê¸° (ë””ë ‰í† ë¦¬ ì œì™¸)
  3. íŒŒì¼ í¬ê¸° ë¬´ê´€ (32MBëŠ” ì°¸ê³  ì‚¬í•­ì¼ ë¿)
```

```scala
class FileLayout {
  def getInputFiles: Seq[File] = {
    inputDirs.flatMap { dir =>
      val dirFile = new File(dir)
      if (!dirFile.exists()) {
        logger.warn(s"Input directory does not exist: $dir")
        Seq.empty
      } else if (!dirFile.isDirectory) {
        logger.warn(s"Input path is not a directory: $dir")
        Seq.empty
      } else {
        listFilesRecursively(dirFile)
          .filter(f => f.isFile)  // ì¼ë°˜ íŒŒì¼ë§Œ
      }
    }
  }

  private def listFilesRecursively(dir: File): Seq[File] = {
    val (files, subdirs) = dir.listFiles()
      .partition(_.isFile)

    val subFiles = subdirs.flatMap(listFilesRecursively)

    files ++ subFiles
  }
}
```

### 4.3 ì…ë ¥ ì½ê¸° ë°©ì‹

```scala
class InputReader(useAscii: Boolean) {
  private val recordReader = if (useAscii) {
    new AsciiRecordReader()
  } else {
    new BinaryRecordReader()
  }

  def readAllRecords(files: Seq[File]): Iterator[Record] = {
    files.iterator.flatMap { file =>
      logger.info(s"Reading file: ${file.getName}, size: ${file.length() / (1024 * 1024)} MB")

      val inputStream = new BufferedInputStream(
        new FileInputStream(file),
        BufferedIO.LARGE_BUFFER_SIZE  // 1 MB buffer
      )

      Iterator.continually {
        recordReader.readRecord(inputStream)
      }.takeWhile(_.isDefined).map(_.get)
    }
  }
}
```

---

## 5. Worker ìˆ˜ ê²°ì • ë¡œì§

### 5.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**ì›ë¬¸**:
> "Master expects N workers"

### 5.2 êµ¬í˜„ ê²°ì •

```
Master ì‹œì‘ ì‹œ Nì„ ëª…ì‹œ:
  - Masterê°€ ì •í™•íˆ Nê°œ worker ëŒ€ê¸°
  - Nê°œ ëª¨ë‘ ë“±ë¡ë  ë•Œê¹Œì§€ blocking
  - Timeout: 5ë¶„

Worker ë“±ë¡:
  - First N workersë§Œ ë°›ìŒ
  - N ì´ˆê³¼ ë“±ë¡ ì‹œë„ ê±°ë¶€
  - N ë¯¸ë§Œ ë“±ë¡ ì‹œ timeout í›„ ì—ëŸ¬
```

```scala
class MasterNode(expectedWorkers: Int) {
  private val registrationLatch = new CountDownLatch(expectedWorkers)
  private val registrationDeadline = System.currentTimeMillis() + (5 * 60 * 1000)

  override def registerWorker(request: RegisterRequest): Future[RegisterResponse] = {
    synchronized {
      val now = System.currentTimeMillis()

      // Check timeout
      if (now > registrationDeadline) {
        return Future.successful(RegisterResponse(
          success = false,
          workerId = "",
          totalWorkers = expectedWorkers,
          message = "Registration deadline exceeded"
        ))
      }

      // Check if already full
      if (registeredWorkers.size() >= expectedWorkers) {
        return Future.successful(RegisterResponse(
          success = false,
          workerId = "",
          totalWorkers = expectedWorkers,
          message = s"Already have $expectedWorkers workers registered"
        ))
      }

      // Register worker
      val workerId = s"W${workerIdCounter.getAndIncrement()}"
      val workerInfo = request.workerInfo.get.copy(workerId = workerId)

      registeredWorkers.add(workerInfo)
      registrationLatch.countDown()

      logger.info(s"Worker $workerId registered (${registeredWorkers.size()}/$expectedWorkers)")

      Future.successful(RegisterResponse(
        success = true,
        workerId = workerId,
        totalWorkers = expectedWorkers,
        message = "Registration successful"
      ))
    }
  }

  def waitForAllWorkers(): Unit = {
    logger.info(s"Waiting for $expectedWorkers workers to register...")

    val registered = registrationLatch.await(5, TimeUnit.MINUTES)

    if (!registered) {
      val actual = registeredWorkers.size()
      throw new TimeoutException(
        s"Only $actual/$expectedWorkers workers registered within timeout"
      )
    }

    logger.info(s"All $expectedWorkers workers registered successfully")
  }
}
```

---

## 6. ê¸°íƒ€ ëª…í™•í™” ì‚¬í•­

### 6.1 ASCII vs Binary í˜•ì‹

**PDF ìš”êµ¬ì‚¬í•­**:
> "Should work on both ASCII and binary input **without requiring an option**"

**êµ¬í˜„ ê²°ì •**: **ìë™ ê°ì§€ (Auto-detection)**

#### 6.1.1 ì…ë ¥ í˜•ì‹ ì •ì˜

```
ASCII:
  - ê° recordëŠ” 100 characters
  - Key: 10 characters (ASCII printable)
  - Value: 90 characters (ASCII printable)
  - Line ending: \n (Unix) or \r\n (Windows) - ë‘˜ ë‹¤ ì§€ì›

Binary:
  - ê° recordëŠ” ì •í™•íˆ 100 bytes
  - Key: 10 bytes (any byte value)
  - Value: 90 bytes (any byte value)
  - No line endings
```

**ì¤‘ìš”**:
- **ì •ë ¬ ì‹œ ì²˜ìŒ 10 bytes (Key)ë§Œ ë¹„êµ**ì— ì‚¬ìš©ë©ë‹ˆë‹¤
- ë‚˜ë¨¸ì§€ 90 bytes (Value)ëŠ” ì •ë ¬ì— ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë©°, ë‹¨ìˆœíˆ Keyì™€ í•¨ê»˜ ì´ë™í•©ë‹ˆë‹¤
- ì´ëŠ” gensort í‘œì¤€ í˜•ì‹ìœ¼ë¡œ, valsortë¡œ ê²€ì¦í•  ë•Œë„ ì´ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤

#### 6.1.2 ìë™ ê°ì§€ ì•Œê³ ë¦¬ì¦˜

```scala
object InputFormatDetector {
  /**
   * íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ë¥¼ ì½ì–´ì„œ ASCII vs Binary íŒë³„
   *
   * ì•Œê³ ë¦¬ì¦˜:
   *   1. íŒŒì¼ì˜ ì²« 1000 ë°”ì´íŠ¸ ì½ê¸°
   *   2. ASCII printable ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
   *   3. ë¹„ìœ¨ > 0.9 â†’ ASCII, ê·¸ ì™¸ â†’ Binary
   */
  def detectFormat(file: File): InputFormat = {
    val buffer = new Array[Byte](1000)
    val inputStream = new FileInputStream(file)

    try {
      val bytesRead = inputStream.read(buffer)

      if (bytesRead <= 0) {
        logger.warn(s"Empty file: ${file.getName}, defaulting to Binary")
        return InputFormat.BINARY
      }

      // ASCII printable: 0x20-0x7E (space ~ ~), plus \n (0x0A), \r (0x0D)
      val asciiLikeCount = buffer.take(bytesRead).count { b =>
        (b >= 32 && b <= 126) || b == '\n' || b == '\r'
      }

      val asciiRatio = asciiLikeCount.toDouble / bytesRead

      logger.debug(s"File: ${file.getName}, ASCII ratio: $asciiRatio")

      if (asciiRatio > 0.9) {
        logger.info(s"Detected ASCII format for file: ${file.getName}")
        InputFormat.ASCII
      } else {
        logger.info(s"Detected Binary format for file: ${file.getName}")
        InputFormat.BINARY
      }
    } finally {
      inputStream.close()
    }
  }
}

sealed trait InputFormat
object InputFormat {
  case object ASCII extends InputFormat
  case object BINARY extends InputFormat
}
```

#### 6.1.3 RecordReader êµ¬í˜„

```scala
trait RecordReader {
  def readRecord(input: InputStream): Option[Record]
}

class BinaryRecordReader extends RecordReader {
  override def readRecord(input: InputStream): Option[Record] = {
    val buffer = new Array[Byte](100)
    val bytesRead = input.read(buffer)

    if (bytesRead == -1) {
      None
    } else if (bytesRead < 100) {
      logger.warn(s"Incomplete record: only $bytesRead bytes")
      None
    } else {
      val key = buffer.slice(0, 10)
      val value = buffer.slice(10, 100)
      Some(Record(key, value))
    }
  }
}

class AsciiRecordReader extends RecordReader {
  private val lineReader = new BufferedReader(new InputStreamReader(input))

  override def readRecord(input: InputStream): Option[Record] = {
    val line = lineReader.readLine()

    if (line == null) {
      None
    } else if (line.length < 100) {
      logger.warn(s"Incomplete line: only ${line.length} characters")
      None
    } else {
      val key = line.substring(0, 10).getBytes(StandardCharsets.UTF_8)
      val value = line.substring(10, 100).getBytes(StandardCharsets.UTF_8)
      Some(Record(key, value))
    }
  }
}
```

#### 6.1.4 ì‚¬ìš© ì˜ˆì‹œ

```scala
class InputReader {
  def readAllRecords(files: Seq[File]): Iterator[Record] = {
    files.iterator.flatMap { file =>
      // Auto-detect format for each file
      val format = InputFormatDetector.detectFormat(file)

      val recordReader = format match {
        case InputFormat.ASCII => new AsciiRecordReader()
        case InputFormat.BINARY => new BinaryRecordReader()
      }

      logger.info(s"Reading file: ${file.getName} (format: $format)")

      val inputStream = new BufferedInputStream(
        new FileInputStream(file),
        BufferedIO.LARGE_BUFFER_SIZE
      )

      Iterator.continually {
        recordReader.readRecord(inputStream)
      }.takeWhile(_.isDefined).map(_.get)
    }
  }
}
```

#### 6.1.5 í˜¼í•© ì…ë ¥ ì²˜ë¦¬

```
ì‹œë‚˜ë¦¬ì˜¤: Workerê°€ ì—¬ëŸ¬ input ë””ë ‰í† ë¦¬ë¥¼ ê°€ì§ˆ ë•Œ
  - /data/input1/file1.bin (Binary)
  - /data/input1/file2.txt (ASCII)
  - /data/input2/file3.bin (Binary)

ì²˜ë¦¬ ë°©ì‹:
  âœ… ê° íŒŒì¼ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ format detection
  âœ… ASCIIì™€ Binary íŒŒì¼ í˜¼ì¬ ê°€ëŠ¥
  âœ… ë™ì¼í•œ Key/Value ë°”ì´íŠ¸ í‘œí˜„ìœ¼ë¡œ ì •ë ¬
```

**ì¤‘ìš”**: ëª…ë ¹í–‰ ì˜µì…˜ìœ¼ë¡œ `--ascii` ë˜ëŠ” `--binary`ë¥¼ ì§€ì •í•˜ì§€ **ì•ŠìŒ**. ëª¨ë“  íŒŒì¼ì€ ìë™ ê°ì§€ë¨.

### 6.2 Temporary íŒŒì¼ ìœ„ì¹˜

```
ê¸°ë³¸ê°’:
  - <output-dir>/.temp/

ëª…ë ¹í–‰ ì˜µì…˜:
  --temp <directory>

ì˜ˆì‹œ:
  sbt "runMain distsort.Main worker master:5000 \
    -I /data/input1,/data/input2 \
    -O /data/output \
    --temp /tmp/sorting"
```

### 6.3 í¬íŠ¸ ë²ˆí˜¸

```
Master:
  - ê¸°ë³¸ê°’: ì—†ìŒ (ëª…ë ¹í–‰ì—ì„œ ì§€ì • ë¶ˆí•„ìš”)
  - gRPC ì„œë²„ ëœë¤ í¬íŠ¸ bind
  - ì‹¤ì œ bindëœ í¬íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥

Worker:
  - ê¸°ë³¸ê°’: ì—†ìŒ
  - gRPC ì„œë²„ ëœë¤ í¬íŠ¸ bind
  - RegisterRequestì— ì‹¤ì œ í¬íŠ¸ í¬í•¨
```

```scala
class MasterNode(expectedWorkers: Int) {
  private val server = ServerBuilder.forPort(0)  // 0 = random port
    .addService(MasterServiceGrpc.bindService(this, executionContext))
    .build()
    .start()

  val actualPort: Int = server.getPort

  logger.info(s"Master started on port $actualPort")
  println(s"MASTER_PORT=$actualPort")  // For scripting
}
```

---

## 7. ì •ë¦¬ ë° ìš°ì„ ìˆœìœ„

### 7.1 í•µì‹¬ ê²°ì • ì‚¬í•­

| í•­ëª© | ê²°ì • | ë³µì¡ë„ | ìš°ì„ ìˆœìœ„ |
|------|------|--------|---------|
| Fault Tolerance | ì „ì²´ ì¬ì‹œì‘ (Phase 1-2ëŠ” ë¶€ë¶„ ë³µêµ¬) | Medium | ğŸ”´ High |
| ë³‘ë ¬ ì²˜ë¦¬ | Phaseë³„ ë³‘ë ¬í™”, numCores ê¸°ë°˜ | Low | ğŸ”´ High |
| Master ì¶œë ¥ | `address:port` í˜•ì‹, Worker ID ìˆœ | Low | ğŸ”´ High |
| ì…ë ¥ íŒŒì¼ | ì¬ê·€ íƒìƒ‰ | Low | ğŸ”´ High |
| Worker ìˆ˜ | ê³ ì • N, 5ë¶„ timeout | Low | ğŸ”´ High |
| ASCII/Binary | ë³„ë„ RecordReader, 100 char/byte | Low | ğŸ”´ High |

### 7.2 êµ¬í˜„ ìˆœì„œ

```
Milestone 1: Infrastructure
  âœ… Master/Worker skeleton
  âœ… gRPC setup
  âœ… Worker registration (N workers, 5min timeout)
  âœ… Port randomization
  âœ… Master ì¶œë ¥ í˜•ì‹

Milestone 2: Basic Sorting (Strategy A)
  âœ… ì…ë ¥ íŒŒì¼ ì¬ê·€ íƒìƒ‰
  âœ… ASCII/Binary RecordReader
  âœ… Sampling
  âœ… External sorting (sequential)
  âœ… Partitioning (Nâ†’N)
  âœ… Shuffle
  âœ… Merge

Milestone 3: Parallelization
  âœ… Parallel chunk sorting
  âœ… Concurrent shuffle transfers
  âœ… Thread pool configuration

Milestone 4: Fault Tolerance
  âœ… Heartbeat mechanism
  âœ… Health checking
  âœ… Graceful degradation (Phase 1-2)
  âœ… Job restart (Phase 3-4)
  âœ… State-based cleanup

Milestone 5: Advanced Features (Optional)
  âš ï¸ Strategy B (Nâ†’M partitioning)
  âš ï¸ Shuffle output replication
  âš ï¸ Checkpoint-based recovery
```

---

## 8. Additional Requirements (ì¶”ê°€ ìš”êµ¬ì‚¬í•­) âš ï¸ ë§¤ìš° ì¤‘ìš”

### 8.1 ì…ë ¥/ì¶œë ¥ í¬ë§· ìë™ ê°ì§€ (Requirement 1)

**ìš”êµ¬ì‚¬í•­**:
> "Should work on both ASCII and binary input **without requiring an option**"

**êµ¬í˜„**:
- âœ… ì´ë¯¸ êµ¬í˜„ë¨ (Section 6.1 ì°¸ì¡°)
- `InputFormatDetector`ê°€ íŒŒì¼ ë‚´ìš© ë¶„ì„í•˜ì—¬ ìë™ ê°ì§€
- ASCII ratio > 0.9 â†’ ASCII, ê·¸ ì™¸ â†’ Binary

### 8.2 ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸ (Requirement 2) âš ï¸

**ìš”êµ¬ì‚¬í•­**:
> "Input directories are shared and should not be updated"
> - **do not try to delete input files**
> - **do not create new files in the input directories**

**êµ¬í˜„ ì›ì¹™**:
```scala
class FileLayout(inputDirs: Seq[Path], outputDir: Path, tempBaseDir: Path) {
  // âœ… CORRECT: Read-only access to input directories
  def getInputFiles: Seq[File] = {
    inputDirs.flatMap { dir =>
      Files.walk(dir)
        .filter(Files.isRegularFile(_))
        .map(_.toFile)
        .toSeq
    }
  }

  // âŒ NEVER DO THIS: Modify input directories
  // def deleteInputFile(file: File): Unit = file.delete()  // ê¸ˆì§€!
  // def createTempInInput(dir: Path): File = ...          // ê¸ˆì§€!
}
```

**ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ì„ ì½ê¸°ë§Œ í•˜ëŠ”ê°€?
- [ ] ì…ë ¥ ë””ë ‰í† ë¦¬ì— ì„ì‹œ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠëŠ”ê°€?
- [ ] ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì„ ì‚­ì œ/ì´ë™í•˜ì§€ ì•ŠëŠ”ê°€?
- [ ] ëª¨ë“  ì„ì‹œ íŒŒì¼ì€ `tempBaseDir`ì—ë§Œ ìƒì„±í•˜ëŠ”ê°€?

### 8.3 ì„ì‹œ íŒŒì¼ ì •ë¦¬ (Requirement 3)

**ìš”êµ¬ì‚¬í•­**:
> "Output directories should contain only final output files"
> - It is okay to create temporary files/directories
> - **but delete them in the end**

**êµ¬í˜„**:
```scala
// ëª¨ë“  ì„ì‹œ íŒŒì¼ì€ .tmp í™•ì¥ì ì‚¬ìš©
val tempFile = new File(outputDir, s"partition.$id.tmp")

// Phase ì™„ë£Œ ì‹œ cleanup
fileLayout.cleanupSortedChunks()      // Sorting phase í›„
fileLayout.cleanupLocalPartitions()   // Shuffle phase í›„
fileLayout.cleanupReceivedPartitions() // Merge phase í›„

// Job ì™„ë£Œ ë˜ëŠ” ì—ëŸ¬ ì‹œ ì „ì²´ cleanup
fileLayout.cleanupTemporaryFiles()
```

**ìµœì¢… ê²€ì¦**:
```bash
# Output directory should only contain partition.* files
ls $OUTPUT_DIR
# ì˜ˆìƒ ì¶œë ¥: partition.0, partition.1, partition.2, ...
# ê¸ˆì§€: partition.0.tmp, chunk_0000.sorted, .sorting/, etc.
```

**ìì„¸í•œ ë‚´ìš©**: `5-file-management.md` Section 3, 7 ì°¸ì¡°

### 8.4 í¬íŠ¸ ë™ì  í• ë‹¹ (Requirement 4)

**ìš”êµ¬ì‚¬í•­**:
> "Do not assume a specific port (e.g., hard-coded)"
> - Running multiple workers (with different input/output directories) should work

**êµ¬í˜„**:
```scala
// âŒ WRONG: Hard-coded port
val server = ServerBuilder.forPort(50051).build()

// âœ… CORRECT: OS-assigned port (port 0)
val server = ServerBuilder.forPort(0).build()
val actualPort = server.getPort  // OSê°€ í• ë‹¹í•œ ì‹¤ì œ í¬íŠ¸

// Masterì— ë“±ë¡ ì‹œ ì‹¤ì œ í¬íŠ¸ ì „ë‹¬
val request = RegisterRequest(
  workerInfo = Some(WorkerInfo(
    workerId = "",
    address = getLocalAddress(),
    port = actualPort  // ë™ì  í• ë‹¹ëœ í¬íŠ¸
  ))
)
```

**ì´ìœ **:
- ê°™ì€ ë¨¸ì‹ ì—ì„œ ì—¬ëŸ¬ Worker ì‹¤í–‰ ê°€ëŠ¥
- í¬íŠ¸ ì¶©ëŒ ë°©ì§€
- í…ŒìŠ¤íŠ¸ ìë™í™” ìš©ì´

### 8.5 ì…ë ¥ ë¸”ë¡ í¬ê¸° ê°€ì • ê¸ˆì§€ (Requirement 5) âš ï¸

**ìš”êµ¬ì‚¬í•­**:
> "Input blocks of 32MB each on each worker"
> - **do not rely on the assumption of 32MB**

**ì˜ëª»ëœ ê°€ì •**:
```scala
// âŒ WRONG: Assuming 32MB blocks
val chunkSize = 32 * 1024 * 1024  // 32MB
val numChunks = totalSize / chunkSize

// âŒ WRONG: Allocating fixed arrays
val buffer = new Array[Byte](32 * 1024 * 1024)
```

**ì˜¬ë°”ë¥¸ êµ¬í˜„**:
```scala
// âœ… CORRECT: Dynamic block size based on memory
val availableMemory = Runtime.getRuntime.maxMemory()
val chunkSize = math.min(availableMemory / 10, 100 * 1024 * 1024)

// âœ… CORRECT: Read files regardless of size
def readInputFiles(): Iterator[Record] = {
  inputFiles.iterator.flatMap { file =>
    val reader = new RecordReader(file)
    Iterator.continually(reader.readRecord())
      .takeWhile(_.isDefined)
      .map(_.get)
  }
}
```

**ì¤‘ìš”**:
- ì…ë ¥ íŒŒì¼ í¬ê¸°ëŠ” **ì•Œ ìˆ˜ ì—†ìŒ**
- 1KB ~ 10GB ì–´ë–¤ í¬ê¸°ë„ ê°€ëŠ¥
- ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²­í¬ í¬ê¸° ë™ì  ê²°ì •
- ì „ì²´ íŒŒì¼ í¬ê¸°ì— ì˜ì¡´í•˜ì§€ ë§ ê²ƒ

### 8.6 Worker í¬ë˜ì‹œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (Fault Tolerance)

**í…ŒìŠ¤íŠ¸ ìš”êµ¬ì‚¬í•­**:
> "We will kill one of your workers during the experiment"

**ì‹œë‚˜ë¦¬ì˜¤**:
1. Worker í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ ê°‘ìê¸° ì¢…ë£Œ (kill -9)
2. ëª¨ë“  ì¤‘ê°„ ë°ì´í„° ì†ì‹¤
3. ìƒˆ Workerê°€ ê°™ì€ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œì‘
4. ê¸°ëŒ€: ìµœì¢… ê²°ê³¼ëŠ” ë™ì¼

**ëŒ€ì‘ ì „ëµ**:
- âœ… Worker Re-registration (Section 1.4 ì°¸ì¡°)
- âœ… State-based cleanup on restart
- âœ… Idempotent operations
- âœ… Atomic file operations

**ìì„¸í•œ ë‚´ìš©**: `4-error-recovery.md` ì°¸ì¡°

---

## 9. ë¬¸ì„œ í˜„í™©

### ì„¤ê³„ ë¬¸ì„œ (Design Docs)
- âœ… docs/0-implementation-decisions.md (êµ¬í˜„ ê²°ì • ì‚¬í•­ + Additional Requirements)
- âœ… docs/1-phase-coordination.md (Phase ë™ê¸°í™”)
- âœ… docs/2-worker-state-machine.md (Worker ìƒíƒœ ë¨¸ì‹ )
- âœ… docs/3-grpc-sequences.md (gRPC ì‹œí€€ìŠ¤)
- âœ… docs/4-error-recovery.md (ì¥ì•  ë³µêµ¬)
- âœ… docs/5-file-management.md (íŒŒì¼ ê´€ë¦¬)
- âœ… docs/6-parallelization.md (ë³‘ë ¬ ì²˜ë¦¬ + Nâ†’M Strategy)
- âœ… docs/7-testing-strategy.md (í…ŒìŠ¤íŠ¸ ì „ëµ + TDD ê°€ì´ë“œ) â­ NEW

### ê³„íš ë¬¸ì„œ (Planning Docs)
- âœ… plan/2025-10-24_plan_ver3.md (ì „ì²´ ì„¤ê³„ + Critical Requirements)
- âœ… plan/quickstart-tdd-guide.md (TDD ê°œë°œ QuickStart) â­ NEW

### ì£¼ìš” ê°œì„  ì‚¬í•­ (v4)
- âœ… gensort/valsort ì™„ì „ ë¬¸ì„œí™” (ê³µì‹ ì‚¬ì´íŠ¸, ì‚¬ìš©ë²•, í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤)
- âœ… Nâ†’M Strategy í•µì‹¬ ê°œë… ëª…í™•í™” (Merge ë³‘ë ¬í™” ëª©ì )
- âœ… 6ê°€ì§€ Critical Requirements ì¶”ê°€ (ì…ë ¥ ë””ë ‰í† ë¦¬ ë³´í˜¸, 32MB ê°€ì • ê¸ˆì§€ ë“±)
- âœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì „ëµ ë¬¸ì„œ (Unit/Integration/E2E/Performance/Fault Tolerance)
- âœ… TDD QuickStart ê°€ì´ë“œ (í”„ë¡œì íŠ¸ ì„¤ì • â†’ ì²« í…ŒìŠ¤íŠ¸ â†’ Red-Green-Refactor)

---

**ë¬¸ì„œ ì™„ì„±ë„**: v4 - êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ (TDD ê°€ì´ë“œ í¬í•¨) ğŸš€
