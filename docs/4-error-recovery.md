# Error Recovery & Fault Tolerance

**ë¬¸ì„œ ëª©ì **: ë¶„ì‚° ì •ë ¬ ì‹œìŠ¤í…œì˜ ëª¨ë“  ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ì™€ ë³µêµ¬ ì „ëµì„ ìƒì„¸íˆ ë¬¸ì„œí™”

**ì°¸ì¡° ë¬¸ì„œ**:
- `2025-10-24_plan_ver3.md` - ì „ì²´ ì‹œìŠ¤í…œ ì„¤ê³„
- `2-worker-state-machine.md` - Worker ìƒíƒœ ë¨¸ì‹ 
- `3-grpc-sequences.md` - gRPC í˜¸ì¶œ ì‹œí€€ìŠ¤

---

## 1. Failure Taxonomy (ì¥ì•  ë¶„ë¥˜)

### 1.1 Network Failures
- **Transient Network Failure**: ì¼ì‹œì ì¸ ë„¤íŠ¸ì›Œí¬ ì§€ì—°/íŒ¨í‚· ì†ì‹¤
- **Network Partition**: Workerì™€ Master ê°„ ì—°ê²° ì™„ì „ ë‹¨ì ˆ
- **Slow Network**: ê·¹ì‹¬í•œ ë„¤íŠ¸ì›Œí¬ ì§€ì—° (timeout ê·¼ì²˜)

### 1.2 Process Failures
- **Worker Crash**: Worker í”„ë¡œì„¸ìŠ¤ ë¹„ì •ìƒ ì¢…ë£Œ
- **Master Crash**: Master í”„ë¡œì„¸ìŠ¤ ë¹„ì •ìƒ ì¢…ë£Œ
- **Worker Hang**: Workerê°€ ì‘ë‹µ ì—†ì´ ë¬´í•œ ëŒ€ê¸°
- **Out-of-Memory**: Worker ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì¢…ë£Œ

### 1.3 Data Failures
- **Corrupt Input File**: ì…ë ¥ íŒŒì¼ ì†ìƒ
- **Disk Full**: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±
- **Partial Write**: íŒŒì¼ ì“°ê¸° ì¤‘ ì‹¤íŒ¨ (ë¶ˆì™„ì „í•œ íŒŒì¼)
- **Missing Partition**: Shuffle ì‹œ partition íŒŒì¼ ëˆ„ë½

### 1.4 Logic Failures
- **Invalid State Transition**: Workerê°€ ì˜ëª»ëœ ìƒíƒœ ì „í™˜ ì‹œë„
- **Protocol Violation**: gRPC í˜¸ì¶œ ìˆœì„œ ìœ„ë°˜
- **Configuration Mismatch**: Workerë“¤ ê°„ ì„¤ì • ë¶ˆì¼ì¹˜
- **Boundary Calculation Error**: ì˜ëª»ëœ partition boundary

---

## 2. Failure Detection (ì¥ì•  ê°ì§€)

### 2.1 Master-side Detection

```scala
class MasterNode {
  // Health check mechanism
  private val workerHealthChecker = new ScheduledThreadPoolExecutor(1)
  private val workerLastSeen = new ConcurrentHashMap[String, Long]()

  def startHealthChecking(): Unit = {
    workerHealthChecker.scheduleAtFixedRate(
      new Runnable {
        override def run(): Unit = checkWorkerHealth()
      },
      0, 10, TimeUnit.SECONDS
    )
  }

  private def checkWorkerHealth(): Unit = {
    val now = System.currentTimeMillis()
    val staleThreshold = 60 * 1000  // 60 seconds

    registeredWorkers.asScala.foreach { worker =>
      val lastSeen = workerLastSeen.getOrDefault(worker.workerId, now)
      val staleDuration = now - lastSeen

      if (staleDuration > staleThreshold) {
        logger.warn(s"Worker ${worker.workerId} appears to be dead " +
          s"(last seen ${staleDuration / 1000}s ago)")
        handleWorkerFailure(worker)
      }
    }
  }

  // Update last seen time when receiving any RPC from worker
  override def notifyPhaseComplete(request: PhaseCompleteRequest): Future[Ack] = {
    workerLastSeen.put(request.workerId, System.currentTimeMillis())
    // ... rest of implementation
  }

  override def submitSample(request: SampleData): Future[Ack] = {
    // Extract worker ID from metadata or request
    val workerId = extractWorkerIdFromContext()
    workerLastSeen.put(workerId, System.currentTimeMillis())
    // ... rest of implementation
  }
}
```

### 2.2 Worker-side Detection

```scala
class WorkerNode {
  // Heartbeat to master
  private val heartbeatExecutor = new ScheduledThreadPoolExecutor(1)

  def startHeartbeat(): Unit = {
    heartbeatExecutor.scheduleAtFixedRate(
      new Runnable {
        override def run(): Unit = sendHeartbeat()
      },
      0, 30, TimeUnit.SECONDS
    )
  }

  private def sendHeartbeat(): Unit = {
    try {
      val request = HeartbeatRequest(
        workerId = workerId,
        currentState = stateMachine.getState.toString,
        timestamp = System.currentTimeMillis()
      )

      val ack = masterStub.heartbeat(request)

      if (!ack.success) {
        logger.warn(s"Master rejected heartbeat: ${ack.message}")
      }
    } catch {
      case ex: StatusRuntimeException if ex.getStatus.getCode == Status.Code.UNAVAILABLE =>
        logger.error("Master appears to be down!")
        handleMasterFailure()

      case ex: Exception =>
        logger.error(s"Heartbeat failed: $ex")
    }
  }
}
```

### 2.3 Timeout-based Detection

```scala
class TimeoutDetector {
  case class OperationTimeout(
    phase: WorkerPhase,
    timeoutMs: Long,
    action: () => Unit
  )

  private val timeouts = Map(
    WorkerPhase.PHASE_SAMPLING -> OperationTimeout(
      WorkerPhase.PHASE_SAMPLING,
      10 * 60 * 1000,  // 10 minutes
      () => handleSamplingTimeout()
    ),
    WorkerPhase.PHASE_SORTING -> OperationTimeout(
      WorkerPhase.PHASE_SORTING,
      30 * 60 * 1000,  // 30 minutes
      () => handleSortingTimeout()
    ),
    WorkerPhase.PHASE_SHUFFLING -> OperationTimeout(
      WorkerPhase.PHASE_SHUFFLING,
      30 * 60 * 1000,  // 30 minutes
      () => handleShufflingTimeout()
    ),
    WorkerPhase.PHASE_MERGING -> OperationTimeout(
      WorkerPhase.PHASE_MERGING,
      60 * 60 * 1000,  // 60 minutes
      () => handleMergingTimeout()
    )
  )

  def startPhaseTimeout(phase: WorkerPhase): ScheduledFuture[_] = {
    val timeout = timeouts(phase)

    scheduler.schedule(
      new Runnable {
        override def run(): Unit = {
          logger.error(s"Phase $phase timed out after ${timeout.timeoutMs}ms")
          timeout.action()
        }
      },
      timeout.timeoutMs,
      TimeUnit.MILLISECONDS
    )
  }
}
```

---

## 3. Fault Tolerance Strategy (ëª…í™•í•œ ì „ëµ)

### 3.1 PDF ìš”êµ¬ì‚¬í•­ í•´ì„

**PDF ì›ë¬¸**:
> "The system must be fault-tolerant, which means that if a worker crashes and restarts, the overall computation should still produce correct results."

### 3.2 êµ¬í˜„ ì „ëµ: Graceful Degradation with Job Restart

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fault Tolerance Strategy                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 1 (Registration):                                 â”‚
â”‚   âŒ Worker ë¶€ì¡± â†’ Job ì¤‘ë‹¨                              â”‚
â”‚   â±ï¸  Timeout: 5ë¶„                                      â”‚
â”‚                                                         â”‚
â”‚ Phase 2 (Sampling):                                     â”‚
â”‚   âœ… >50% Worker ì‚´ì•„ìˆìŒ â†’ ê³„ì† ì§„í–‰                    â”‚
â”‚   âœ… Boundary ì¬ê³„ì‚° (ë‚¨ì€ sample ì‚¬ìš©)                  â”‚
â”‚   âŒ <50% Worker â†’ Job ì¤‘ë‹¨                             â”‚
â”‚                                                         â”‚
â”‚ Phase 3 (Sorting):                                      â”‚
â”‚   âœ… >50% Worker ì‚´ì•„ìˆìŒ â†’ ê³„ì† ì§„í–‰                    â”‚
â”‚   âœ… numPartitions = ë‚¨ì€ worker ìˆ˜ë¡œ ì¡°ì •               â”‚
â”‚   âŒ <50% Worker â†’ Job ì¤‘ë‹¨                             â”‚
â”‚                                                         â”‚
â”‚ Phase 4 (Shuffle):                                      â”‚
â”‚   âŒ ì–´ë–¤ Workerë“  ì‹¤íŒ¨ â†’ ì „ì²´ Job ì¤‘ë‹¨                  â”‚
â”‚   ğŸ“ ì´ìœ : Partition ë°ì´í„° ìœ ì‹¤, ë³µêµ¬ ë¶ˆê°€              â”‚
â”‚                                                         â”‚
â”‚ Phase 5 (Merge):                                        â”‚
â”‚   âŒ ì–´ë–¤ Workerë“  ì‹¤íŒ¨ â†’ ì „ì²´ Job ì¤‘ë‹¨                  â”‚
â”‚   ğŸ“ ì´ìœ : ìµœì¢… output ë¶ˆì™„ì „, ë³µêµ¬ ë¶ˆê°€                 â”‚
â”‚                                                         â”‚
â”‚ Master Crash:                                           â”‚
â”‚   âŒ ë³µêµ¬ ë¶ˆê°€ â†’ ì „ì²´ Job ì¬ì‹œì‘                         â”‚
â”‚   ğŸ“ í˜„ì¬ ì„¤ê³„: MasterëŠ” SPOF (Single Point of Failure)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 PDF ìš”êµ¬ì‚¬í•­ í•´ì„: "Same Output Expected" ëª…í™•í™”

**PDF ì›ë¬¸ (page 19)**:
> "If a worker crashes and restarts, the new worker should generate the **same output expected of the initial worker**."

**í•µì‹¬ ì§ˆë¬¸**: ì´ ë¬¸ì¥ì˜ ì •í™•í•œ ì˜ë¯¸ëŠ”?

#### **Interpretation A: ìµœì¢… ê²°ê³¼ì˜ ì •í™•ì„±** (ìš°ë¦¬ì˜ í˜„ì¬ êµ¬í˜„)

```
í•´ì„:
  "Same output" = ì „ì²´ distributed sortì˜ ìµœì¢… ê²°ê³¼ê°€ ì •í™•í•´ì•¼ í•¨
  = Worker restart í›„ì—ë„ ì „ì²´ jobì´ correct sorted outputì„ ìƒì„±

êµ¬í˜„:
  âœ… Worker crash â†’ ì „ì²´ job restart
  âœ… ëª¨ë“  workerê°€ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘
  âœ… ìµœì¢… partition.0, partition.1, ... íŒŒì¼ë“¤ì€ ì •í™•í•˜ê²Œ ì •ë ¬ë¨

ì •ë‹¹í™”:
  - "overall computation should still produce correct results" â† ì „ì²´ ê²°ê³¼ ê°•ì¡°
  - Workerê°€ ì–´ë–¤ partitionì„ ë‹´ë‹¹í•˜ë“ , ìµœì¢… ì •ë ¬ ê²°ê³¼ë§Œ ë§ìœ¼ë©´ ë¨
  - ì‹¤ì œ ì‹œìŠ¤í…œ: Hadoop ì´ˆê¸° ë²„ì „ë„ JobTracker SPOF + job restart
```

#### **Interpretation B: Worker-Partition ì¼ê´€ì„±** (Advanced Option)

```
í•´ì„:
  "Same output" = ì¬ì‹œì‘ëœ workerê°€ ì›ë˜ workerì™€ ë™ì¼í•œ partition ìƒì„±
  = Worker W2ê°€ crash â†’ ì¬ì‹œì‘ ì‹œ ì—¬ì „íˆ partition.2ë¥¼ ë‹´ë‹¹í•´ì•¼ í•¨

êµ¬í˜„ ìš”êµ¬ì‚¬í•­:
  âŒ Jobì„ ì²˜ìŒë¶€í„° ì¬ì‹œì‘í•˜ë©´ worker IDê°€ ë°”ë€” ìˆ˜ ìˆìŒ
  âŒ ìƒˆë¡œìš´ worker ID â†’ ë‹¤ë¥¸ partition í• ë‹¹
  âœ… Worker re-registration í•„ìš” (ê°™ì€ input/output dir â†’ ê°™ì€ worker ID)

êµ¬í˜„ ë°©ë²•:
  âœ… Worker ì¬ì‹œì‘ ì‹œ input/output dirë¡œ identity í™•ì¸
  âœ… ë™ì¼í•œ worker ID ì¬í• ë‹¹
  âœ… í•´ë‹¹ workerê°€ ë‹´ë‹¹í–ˆë˜ partition ì¬ìƒì„±
```

#### **êµ¬í˜„: Worker Re-registration (Interpretation Bë¥¼ ìœ„í•œ ì„ íƒì‚¬í•­)**

```scala
class MasterNode {
  // Worker identity tracking
  private case class WorkerIdentity(
    inputDirs: Seq[String],
    outputDir: String
  )

  private val workerIdentities = new ConcurrentHashMap[WorkerIdentity, String]()  // identity â†’ workerId

  override def registerWorker(request: RegisterRequest): Future[RegisterResponse] = {
    synchronized {
      val identity = WorkerIdentity(
        inputDirs = request.inputDirectories.sorted,  // Canonical ordering
        outputDir = request.outputDirectory
      )

      // Check if this is a restarted worker
      val existingWorkerId = workerIdentities.get(identity)

      if (existingWorkerId != null) {
        // This worker is restarting with the same input/output directories
        logger.info(s"Worker $existingWorkerId restarting (detected by matching dirs)")

        // Find existing worker info
        val existingWorker = registeredWorkers.asScala.find(_.workerId == existingWorkerId)

        if (existingWorker.isDefined) {
          logger.info(s"Re-registering worker $existingWorkerId with same ID")

          // Update worker connection info (IP/port may have changed)
          val updatedWorker = existingWorker.get.copy(
            address = request.address,
            port = request.port
          )

          // Replace in registry
          registeredWorkers.remove(existingWorker.get)
          registeredWorkers.add(updatedWorker)

          // Reset worker state to re-join current phase
          resetWorkerState(existingWorkerId)

          return Future.successful(RegisterResponse(
            success = true,
            workerId = existingWorkerId,  // âœ… Same worker ID!
            totalWorkers = expectedWorkers,
            message = "Worker re-registered after restart"
          ))
        }
      }

      // New worker registration (normal case)
      val workerId = s"W${workerIdCounter.getAndIncrement()}"

      val workerInfo = WorkerInfo(
        workerId = workerId,
        address = request.address,
        port = request.port,
        inputDirectories = request.inputDirectories,
        outputDirectory = request.outputDirectory
      )

      registeredWorkers.add(workerInfo)
      workerIdentities.put(identity, workerId)
      registrationLatch.countDown()

      logger.info(s"Worker $workerId registered (${registeredWorkers.size()}/$expectedWorkers)")

      Future.successful(RegisterResponse(
        success = true,
        workerId = workerId,
        totalWorkers = expectedWorkers,
        message = "Registration successful"
      ))
    }
  }

  private def resetWorkerState(workerId: String): Unit = {
    logger.info(s"Resetting state for restarted worker $workerId")

    // Remove from failure tracking
    failedWorkers.remove(workerId)

    // Reset phase completion tracking
    phaseTracker.resetWorker(workerId)

    // Worker will receive fresh StartPhase RPCs and re-execute from current phase
  }
}
```

#### **í”„ë¡œì íŠ¸ ê²°ì •: Interpretation A ì±„íƒ, BëŠ” Optional**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Milestone 1-3: Interpretation A (ì „ì²´ ì¬ì‹œì‘)              â”‚
â”‚   âœ… êµ¬í˜„ ë‹¨ìˆœ                                              â”‚
â”‚   âœ… ì •í™•ì„± ë³´ì¥                                            â”‚
â”‚   âœ… ë””ë²„ê¹… ìš©ì´                                            â”‚
â”‚   âŒ Worker crash â†’ ì „ì²´ job ì†ì‹¤                           â”‚
â”‚                                                            â”‚
â”‚ Milestone 4-5: Interpretation B ì¶”ê°€ (ì„ íƒì‚¬í•­)            â”‚
â”‚   âœ… Worker re-registration ì§€ì›                           â”‚
â”‚   âœ… Partition consistency ìœ ì§€                            â”‚
â”‚   âœ… Partial recovery ê°€ëŠ¥ (Shuffle output replicationê³¼  â”‚
â”‚       í•¨ê»˜ ì‚¬ìš© ì‹œ)                                         â”‚
â”‚   âš ï¸  ë³µì¡ë„ ì¦ê°€                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PDF ìš”êµ¬ì‚¬í•­ ë§Œì¡± ì—¬ë¶€**:

| Interpretation | "Correct Results" | "Same Output Expected" | ë³µì¡ë„ | êµ¬í˜„ ìš°ì„ ìˆœìœ„ |
|----------------|-------------------|------------------------|--------|--------------|
| **A (ì „ì²´ ì¬ì‹œì‘)** | âœ… Yes | âš ï¸ Job-level yes, Worker-level unclear | Low | ğŸ”´ High (M1-3) |
| **B (Re-registration)** | âœ… Yes | âœ… Yes (Worker-level) | High | ğŸŸ¡ Medium (M4-5) |

**ìµœì¢… íŒë‹¨**: Interpretation Aê°€ PDFì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­("correct results")ì„ ì¶©ì¡±í•˜ë©°, Interpretation BëŠ” ë³´ë‹¤ ì—„ê²©í•œ í•´ì„ìœ¼ë¡œ í–¥í›„ ê°œì„  ì‹œ ì¶”ê°€ êµ¬í˜„

### 3.4 Worker Restart ì‹œë‚˜ë¦¬ì˜¤

#### **ì‹œë‚˜ë¦¬ì˜¤ A: Worker Crashes During Sampling**

```
Initial State:
  - 5 workers registered (W0, W1, W2, W3, W4)
  - W0, W1, W2 submitted samples
  - W3 crashes before submitting sample
  - W4 still sampling

Detection:
  1. Master: No heartbeat from W3 for 60s
  2. Master: Sampling phase timeout approaching (10 min)

Recovery Decision:
  - Alive workers: 4 out of 5 (80%)
  - Threshold: 50%
  - Decision: âœ… Continue with 4 workers

Recovery Actions:
  1. Master marks W3 as failed
  2. Wait for W4 to submit sample
  3. Recompute boundaries using 3 samples (W0, W1, W2)
  4. Set numPartitions = 4 (4 alive workers)
  5. Generate shuffleMap = {0â†’0, 1â†’1, 2â†’2, 3â†’3}
  6. Continue to Sorting phase with W0, W1, W2, W4

W3 Restarts:
  - Tries to re-register
  - Master rejects: "Job already in progress, registration closed"
  - W3 exits and waits for next job
```

#### **ì‹œë‚˜ë¦¬ì˜¤ B: Worker Crashes During Shuffle**

```
Initial State:
  - 4 workers (W0, W1, W2, W3) in shuffle phase
  - W1 is sending partition.1 to W2 (50% complete)
  - W1 crashes

Detection:
  1. W2: Partial file received, no completion signal
  2. W2: Receive timeout after 60s
  3. W2 cleans up temp file: partition.1.from_W1.tmp
  4. Master: No heartbeat from W1 for 60s

Recovery Decision:
  - Phase: Shuffle (Phase 4)
  - Critical data lost: partition.1 (only existed in W1's temp dir)
  - Other workers waiting for partition.1
  - Decision: âŒ Cannot continue

Recovery Actions:
  1. Master sends AbortRequest to all workers
  2. W0, W2, W3 cleanup temporary files
  3. W0, W2, W3 exit
  4. Job must be restarted from Phase 1

W1 Restarts:
  - Re-registers for new job (if master restarts)
  - Starts from Phase 1 with all other workers
```

### 3.5 ì •ë‹¹í™” (Why This Strategy?)

**Q: ì™œ Shuffle/Mergeì—ì„œëŠ” ë³µêµ¬ ë¶ˆê°€ì¸ê°€?**

A: **ë°ì´í„° ìœ„ì¹˜ ë¬¸ì œ**
```
Shuffle ì¤‘ Worker W1 crash:
  - W1ì´ ë§Œë“  partition.1ì€ W1ì˜ temp/ ë””ë ‰í† ë¦¬ì—ë§Œ ì¡´ì¬
  - ë‹¤ë¥¸ workerë“¤ì€ partition.1ì„ ê°€ì§€ê³  ìˆì§€ ì•ŠìŒ
  - partition.1ì„ ì¬ìƒì„±í•˜ë ¤ë©´:
      â†’ Phase 1ë¶€í„° ë‹¤ì‹œ ì‹œì‘ í•„ìš”
      â†’ Sampling â†’ Sorting â†’ Partitioningì„ ë‹¤ì‹œ í•´ì•¼ í•¨

ê²°ë¡ : ë¶€ë¶„ ë³µêµ¬ë³´ë‹¤ ì „ì²´ ì¬ì‹œì‘ì´ ë” ê°„ë‹¨í•˜ê³  ì•ˆì „
```

**Q: ì‹¤ì œ ë¶„ì‚° ì‹œìŠ¤í…œì€?**

A: **Lineage ë˜ëŠ” Replication ì‚¬ìš©**
```
Spark:
  âœ… RDD lineage ì¶”ì 
  âœ… ìœ ì‹¤ëœ partitionë§Œ ì¬ê³„ì‚°
  âœ… ë¶€ëª¨ RDDë¶€í„° ì¬ì‹¤í–‰

Hadoop:
  âœ… Map outputì„ ì—¬ëŸ¬ ê³³ì— ë³µì œ
  âœ… í•œ ê³³ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ ê³³ì—ì„œ ì½ê¸°

ìš°ë¦¬ í”„ë¡œì íŠ¸:
  âŒ Lineage tracking ë³µì¡ (í”„ë¡œì íŠ¸ ë²”ìœ„ ì´ˆê³¼)
  âŒ Replicationì€ Milestone 4-5ì—ì„œ ì„ íƒì  êµ¬í˜„
  âœ… ì „ì²´ ì¬ì‹œì‘ (ë‹¨ìˆœ, ì•ˆì „, êµ¬í˜„ ìš©ì´)
```

### 3.6 í–¥í›„ ê°œì„  ê°€ëŠ¥ì„±

**Milestone 4-5ì—ì„œ ì„ íƒì  êµ¬í˜„**:

```scala
// Option A: Shuffle Output Replication (ê°„ë‹¨í•œ ë²„ì „)
class ShuffleManagerWithReplication {
  def sendPartitionWithBackup(partition: File, partitionId: Int): Unit = {
    val primaryWorker = getPrimaryWorker(partitionId)
    val backupWorker = getBackupWorker(partitionId)

    // Send to both workers
    Future.sequence(Seq(
      sendTo(primaryWorker, partition),
      sendTo(backupWorker, partition)
    ))
  }

  // Primary ì‹¤íŒ¨ ì‹œ backupì—ì„œ ê°€ì ¸ì˜¤ê¸°
  def fetchPartition(partitionId: Int): File = {
    try {
      fetchFrom(primaryWorker, partitionId)
    } catch {
      case _: Exception =>
        logger.warn(s"Primary failed for partition $partitionId, using backup")
        fetchFrom(backupWorker, partitionId)
    }
  }
}
```

**Trade-off**:
- âœ… Shuffle ì‹¤íŒ¨ ì‹œ ë³µêµ¬ ê°€ëŠ¥
- âŒ Network traffic 2ë°°
- âŒ Disk space 2ë°° í•„ìš”

---

## 4. Error Recovery Matrix

### 4.1 Master Perspective

| Failure Type | Detection | Recovery Strategy | Fallback |
|--------------|-----------|-------------------|----------|
| **Worker fails during Registration** | Timeout (5 min) | Wait for timeout, then abort | Restart entire job |
| **Worker fails during Sampling** | NotifyPhaseComplete timeout (10 min) | Continue with N-1 workers if >50% alive | Abort if <50% |
| **Worker fails during Sorting** | NotifyPhaseComplete timeout (30 min) | Continue with N-1 workers if >50% alive | Abort if <50% |
| **Worker fails during Shuffle** | NotifyPhaseComplete timeout (30 min) | **Cannot recover** - data partitions incomplete | Abort job |
| **Worker fails during Merge** | NotifyPhaseComplete timeout (60 min) | **Cannot recover** - final output incomplete | Abort job |
| **Master crashes** | N/A | **No recovery** - restart entire job | N/A |

### 4.2 Worker Perspective

| Failure Type | Detection | Recovery Strategy | Fallback |
|--------------|-----------|-------------------|----------|
| **Master unavailable** | Heartbeat failure | Wait 5 min, then exit | None |
| **RPC timeout (transient)** | gRPC deadline exceeded | Retry up to 3 times with exponential backoff | Fail after 3 retries |
| **Corrupt input file** | IOException during read | Skip file and log warning | Continue with other files |
| **Disk full (temp dir)** | IOException during write | Clean old temp files, retry once | Fail if still full |
| **Disk full (output dir)** | IOException during merge | **Cannot recover** | Fail immediately |
| **OOM during sorting** | OutOfMemoryError | Reduce chunk size, retry with smaller chunks | Fail after 3 retries |
| **Partition file missing** | FileNotFoundException during shuffle | Request re-send from source worker | Fail after 3 retries |
| **Network partition** | gRPC unavailable | Wait for network recovery (up to 5 min) | Fail if no recovery |

### 4.3 Data Failure Recovery

| Failure Type | Detection | Recovery Strategy | Data Integrity |
|--------------|-----------|-------------------|----------------|
| **Partial write (temp file)** | File size mismatch | Delete temp file, retry write | Safe - atomic rename not yet done |
| **Partial write (final file)** | File size mismatch after rename | **Data corruption** | Unsafe - need to restart from scratch |
| **Corrupt partition during shuffle** | Checksum mismatch (optional) | Request re-send from source | Safe - receiver deletes corrupt file |
| **Missing partition file** | File not found during merge | Request from source worker | Safe if source still has it |
| **Duplicate partition files** | Multiple files for same partition | Merge all, assuming idempotency | Safe - duplicates produce same result |

---

## 5. Recovery Implementations

### 5.1 RPC Retry with Exponential Backoff

```scala
class RetryableRpcClient {
  def withRetry[T](
    operation: => Future[T],
    maxRetries: Int = 3,
    initialDelayMs: Long = 1000,
    operationName: String
  ): Future[T] = {
    def attempt(retriesLeft: Int, delayMs: Long): Future[T] = {
      operation.recoverWith {
        case ex: StatusRuntimeException if retriesLeft > 0 =>
          ex.getStatus.getCode match {
            case Status.Code.UNAVAILABLE | Status.Code.DEADLINE_EXCEEDED =>
              logger.warn(s"$operationName failed (${ex.getStatus.getCode}), " +
                s"retrying in ${delayMs}ms ($retriesLeft retries left)")

              val promise = Promise[T]()
              scheduler.schedule(
                new Runnable {
                  override def run(): Unit = {
                    attempt(retriesLeft - 1, delayMs * 2).onComplete {
                      case Success(result) => promise.success(result)
                      case Failure(err) => promise.failure(err)
                    }
                  }
                },
                delayMs,
                TimeUnit.MILLISECONDS
              )
              promise.future

            case _ =>
              logger.error(s"$operationName failed with non-retryable error: ${ex.getStatus}")
              Future.failed(ex)
          }

        case ex: Exception =>
          logger.error(s"$operationName failed with exception: $ex")
          Future.failed(ex)
      }
    }

    attempt(maxRetries, initialDelayMs)
  }
}
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```scala
class WorkerNode {
  private val retryClient = new RetryableRpcClient()

  def submitSampleWithRetry(sampleData: SampleData): Future[Ack] = {
    retryClient.withRetry(
      operation = masterStub.submitSample(sampleData),
      maxRetries = 3,
      initialDelayMs = 1000,
      operationName = "SubmitSample"
    )
  }
}
```

### 4.2 State-based Cleanup on Failure

```scala
class WorkerNode {
  def handleFailure(reason: String): Unit = {
    logger.error(s"Worker entering failed state: $reason")

    val currentState = stateMachine.getState

    // State-specific cleanup
    currentState match {
      case Sampling =>
        cleanupSamplingState()

      case Sorting | WaitingForShuffleSignal =>
        cleanupSortingState()
        cleanupLocalPartitions()

      case Shuffling | WaitingForMergeSignal =>
        cleanupReceivedPartitions()
        cleanupInFlightTransfers()

      case Merging =>
        cleanupTemporaryMergeFiles()
        cleanupPartialOutput()

      case _ =>
        logger.info(s"No cleanup needed for state $currentState")
    }

    // Notify master of failure
    try {
      masterStub.notifyWorkerFailure(WorkerFailureNotification(
        workerId = workerId,
        failedState = currentState.toString,
        reason = reason,
        timestamp = System.currentTimeMillis()
      ))
    } catch {
      case ex: Exception =>
        logger.error(s"Failed to notify master of failure: $ex")
    }

    stateMachine.transition(Failed, reason)
  }

  private def cleanupSamplingState(): Unit = {
    logger.info("Cleaning up sampling state")
    // No files to clean, just release memory
    System.gc()
  }

  private def cleanupSortingState(): Unit = {
    logger.info("Cleaning up sorted chunks")
    val chunkPattern = "chunk_.*\\.tmp".r

    tempDir.listFiles().foreach { file =>
      if (chunkPattern.findFirstIn(file.getName).isDefined) {
        file.delete()
        logger.debug(s"Deleted sorted chunk: ${file.getName}")
      }
    }
  }

  private def cleanupLocalPartitions(): Unit = {
    logger.info("Cleaning up local partition files")
    val partitionPattern = "partition\\.\\d+".r

    tempDir.listFiles().foreach { file =>
      if (partitionPattern.findFirstIn(file.getName).isDefined) {
        file.delete()
        logger.debug(s"Deleted partition file: ${file.getName}")
      }
    }
  }

  private def cleanupReceivedPartitions(): Unit = {
    logger.info("Cleaning up received partition files")
    receivedPartitionsDir.listFiles().foreach { file =>
      file.delete()
      logger.debug(s"Deleted received file: ${file.getName}")
    }
  }

  private def cleanupInFlightTransfers(): Unit = {
    logger.info("Cancelling in-flight transfers")
    // Cancel ongoing gRPC streams
    // (implementation depends on how streams are tracked)
  }

  private def cleanupTemporaryMergeFiles(): Unit = {
    logger.info("Cleaning up temporary merge files")
    val tempPattern = "partition\\.\\d+\\.tmp".r

    outputDir.listFiles().foreach { file =>
      if (tempPattern.findFirstIn(file.getName).isDefined) {
        file.delete()
        logger.debug(s"Deleted temp merge file: ${file.getName}")
      }
    }
  }

  private def cleanupPartialOutput(): Unit = {
    logger.info("Cleaning up partial output files")
    // Delete any partition.X files that are incomplete
    // (this is tricky - might want to keep them for debugging)
  }
}
```

### 4.3 Master-side Worker Failure Handling

```scala
class MasterNode {
  private val failedWorkers = new ConcurrentHashMap[String, WorkerFailureInfo]()

  case class WorkerFailureInfo(
    workerId: String,
    failedPhase: WorkerPhase,
    failureTime: Long,
    reason: String
  )

  override def notifyWorkerFailure(notification: WorkerFailureNotification): Future[Ack] = {
    logger.error(s"Worker ${notification.workerId} failed in state ${notification.failedState}: " +
      s"${notification.reason}")

    val failureInfo = WorkerFailureInfo(
      workerId = notification.workerId,
      failedPhase = notification.failedState,
      failureTime = notification.timestamp,
      reason = notification.reason
    )

    failedWorkers.put(notification.workerId, failureInfo)

    // Decide whether to continue or abort
    val currentPhase = getCurrentPhase()
    val decision = makeRecoveryDecision(currentPhase, failedWorkers.size())

    decision match {
      case RecoveryDecision.ContinueWithRemainingWorkers =>
        logger.warn(s"Continuing with ${expectedWorkers - failedWorkers.size()} workers")
        Future.successful(Ack(success = true, message = "Failure acknowledged, continuing"))

      case RecoveryDecision.AbortJob =>
        logger.error("Too many worker failures, aborting job")
        abortAllWorkers()
        Future.successful(Ack(success = false, message = "Job aborted due to excessive failures"))
    }
  }

  sealed trait RecoveryDecision
  object RecoveryDecision {
    case object ContinueWithRemainingWorkers extends RecoveryDecision
    case object AbortJob extends RecoveryDecision
  }

  private def makeRecoveryDecision(phase: WorkerPhase, numFailed: Int): RecoveryDecision = {
    val numAlive = expectedWorkers - numFailed
    val alivePercentage = numAlive.toDouble / expectedWorkers

    phase match {
      case WorkerPhase.PHASE_SAMPLING | WorkerPhase.PHASE_SORTING =>
        // Can tolerate up to 50% failures in early phases
        if (alivePercentage >= 0.5) {
          RecoveryDecision.ContinueWithRemainingWorkers
        } else {
          RecoveryDecision.AbortJob
        }

      case WorkerPhase.PHASE_SHUFFLING | WorkerPhase.PHASE_MERGING =>
        // Cannot tolerate any failures in shuffle/merge phases
        // (data is already partitioned and assigned)
        RecoveryDecision.AbortJob

      case _ =>
        RecoveryDecision.AbortJob
    }
  }

  private def abortAllWorkers(): Unit = {
    logger.warn("Sending abort signal to all workers")

    registeredWorkers.asScala.foreach { worker =>
      try {
        val stub = createWorkerStub(worker)
        stub.abort(AbortRequest(reason = "Master initiated abort due to excessive failures"))
      } catch {
        case ex: Exception =>
          logger.error(s"Failed to send abort to ${worker.workerId}: $ex")
      }
    }

    System.exit(1)
  }
}
```

### 4.4 Worker-side Graceful Shutdown

```scala
class WorkerNode {
  private val shutdownHook = new Thread {
    override def run(): Unit = {
      logger.info("Shutdown hook triggered, cleaning up...")
      gracefulShutdown()
    }
  }

  def registerShutdownHook(): Unit = {
    Runtime.getRuntime.addShutdownHook(shutdownHook)
  }

  def gracefulShutdown(): Unit = {
    logger.info("Starting graceful shutdown")

    // Stop accepting new work
    grpcServer.shutdown()

    // Wait for in-flight operations to complete
    try {
      grpcServer.awaitTermination(30, TimeUnit.SECONDS)
    } catch {
      case _: InterruptedException =>
        logger.warn("Shutdown interrupted, forcing termination")
        grpcServer.shutdownNow()
    }

    // Perform cleanup based on current state
    val currentState = stateMachine.getState
    if (!currentState.isTerminal) {
      handleFailure("Worker shutting down")
    }

    // Close resources
    closeAllConnections()

    logger.info("Graceful shutdown complete")
  }

  override def abort(request: AbortRequest): Future[Ack] = {
    logger.error(s"Received abort request: ${request.reason}")

    // Trigger graceful shutdown
    Future {
      Thread.sleep(100)  // Give time to send ack
      gracefulShutdown()
      System.exit(1)
    }

    Future.successful(Ack(success = true, message = "Abort acknowledged"))
  }
}
```

---

## 5. Specific Failure Scenarios

### 5.1 Scenario: Worker Crashes During Sorting

**ë°œìƒ ì‹œì **: Workerê°€ external sort ì¤‘ OutOfMemoryErrorë¡œ crash

**ê°ì§€**:
- Master: Workerë¡œë¶€í„° 60ì´ˆ ë™ì•ˆ heartbeat ì—†ìŒ
- Master: NotifyPhaseComplete(SORTING) timeout (30ë¶„)

**ë³µêµ¬ ì ˆì°¨**:

1. **Master ì¸¡**:
```scala
def handleWorkerCrashDuringSorting(workerId: String): Unit = {
  logger.warn(s"Worker $workerId crashed during sorting")

  val numAlive = expectedWorkers - failedWorkers.size()

  if (numAlive >= expectedWorkers / 2) {
    logger.info(s"Continuing with $numAlive workers (>50% alive)")

    // Recalculate partition boundaries with fewer samples
    val remainingSamples = collectSamplesFromAliveWorkers()
    val newBoundaries = computeBoundaries(remainingSamples)

    // Update numPartitions to match alive workers
    val newNumPartitions = numAlive
    val newShuffleMap = createShuffleMap(numAlive, newNumPartitions)

    // Redistribute configuration to alive workers
    redistributeConfig(newBoundaries, newShuffleMap)
  } else {
    logger.error("Less than 50% workers alive, aborting job")
    abortAllWorkers()
  }
}
```

2. **ì¬ì‹œì‘ëœ Worker ì¸¡** (optional):
```scala
def attemptRecovery(): Unit = {
  logger.info("Attempting to recover from crash")

  // Clean up any partial state
  cleanupAllTemporaryFiles()

  // Re-register with master
  initialize()

  // Master will reassign work in reconfiguration
}
```

### 5.2 Scenario: Network Partition During Shuffle

**ë°œìƒ ì‹œì **: Worker Aê°€ Worker Bì—ê²Œ partition íŒŒì¼ ì „ì†¡ ì¤‘ ë„¤íŠ¸ì›Œí¬ ë‹¨ì ˆ

**ê°ì§€**:
- Worker A: gRPC status UNAVAILABLE
- Worker B: Partial partition file received, no completion signal

**ë³µêµ¬ ì ˆì°¨**:

1. **Sender (Worker A)**:
```scala
def handleShuffleNetworkFailure(targetWorker: WorkerInfo, partitionId: Int): Boolean = {
  logger.error(s"Network failure while sending partition $partitionId to ${targetWorker.workerId}")

  // Wait for network recovery
  var attempt = 0
  val maxAttempts = 3

  while (attempt < maxAttempts) {
    Thread.sleep(30000)  // Wait 30 seconds

    logger.info(s"Retry attempt ${attempt + 1}/$maxAttempts for partition $partitionId")

    try {
      // Check if target is reachable
      val healthCheck = createWorkerStub(targetWorker).healthCheck(HealthCheckRequest())

      if (healthCheck.healthy) {
        // Retry sending
        return sendPartitionFile(partitionId, partitionFiles(partitionId), targetWorker)
      }
    } catch {
      case _: Exception =>
        logger.warn(s"Target ${targetWorker.workerId} still unreachable")
    }

    attempt += 1
  }

  // Failed after all retries
  logger.error(s"Failed to send partition $partitionId after $maxAttempts attempts")
  false
}
```

2. **Receiver (Worker B)**:
```scala
override def receivePartition(
  responseObserver: StreamObserver[PartitionFileAck]
): StreamObserver[PartitionFile] = {
  new StreamObserver[PartitionFile] {
    private var lastReceivedTime = System.currentTimeMillis()
    private val timeoutMs = 60 * 1000  // 60 seconds

    // Timeout checker
    private val timeoutChecker = scheduler.scheduleAtFixedRate(
      new Runnable {
        override def run(): Unit = {
          val elapsed = System.currentTimeMillis() - lastReceivedTime
          if (elapsed > timeoutMs) {
            logger.error(s"Receive timeout for partition $partitionId, cleaning up")
            cleanup()
          }
        }
      },
      timeoutMs, timeoutMs, TimeUnit.MILLISECONDS
    )

    override def onNext(chunk: PartitionFile): Unit = {
      lastReceivedTime = System.currentTimeMillis()
      // ... write chunk
    }

    override def onError(t: Throwable): Unit = {
      logger.error(s"Receive error: $t, cleaning up partial file")
      cleanup()
      timeoutChecker.cancel(false)
    }

    private def cleanup(): Unit = {
      outputStream.foreach(_.close())
      tempFile.foreach(_.delete())
    }
  }
}
```

### 5.3 Scenario: Disk Full During Merge

**ë°œìƒ ì‹œì **: Workerê°€ ìµœì¢… partition íŒŒì¼ write ì¤‘ disk full

**ê°ì§€**:
- Worker: IOException with "No space left on device"

**ë³µêµ¬ ì ˆì°¨**:

```scala
def performMergeWithDiskCheck(): Unit = {
  val config = partitionConfig.get
  val myPartitions = getMyPartitions(config.shuffleMap)

  myPartitions.foreach { partitionId =>
    try {
      // Pre-check disk space
      val estimatedOutputSize = estimateOutputSize(partitionId)
      val availableSpace = outputDir.getFreeSpace

      if (availableSpace < estimatedOutputSize * 1.2) {  // 20% buffer
        throw new IOException(s"Insufficient disk space: need $estimatedOutputSize, " +
          s"available $availableSpace")
      }

      mergePartition(partitionId)

    } catch {
      case ex: IOException if ex.getMessage.contains("No space left") =>
        logger.error(s"Disk full while merging partition $partitionId")

        // Attempt cleanup
        cleanupTemporaryMergeFiles()

        val freedSpace = outputDir.getFreeSpace
        logger.info(s"Freed space, now have $freedSpace bytes")

        if (freedSpace > estimatedOutputSize) {
          // Retry once
          logger.info(s"Retrying merge for partition $partitionId")
          mergePartition(partitionId)
        } else {
          // Cannot recover
          handleFailure(s"Disk full, cannot merge partition $partitionId")
        }
    }
  }
}

private def estimateOutputSize(partitionId: Int): Long = {
  val partitionFiles = listReceivedPartitionFiles(partitionId)
  partitionFiles.map(_.length()).sum
}
```

### 5.4 Scenario: Master Crash

**ë°œìƒ ì‹œì **: Master í”„ë¡œì„¸ìŠ¤ê°€ ì¤‘ê°„ì— crash

**ê°ì§€**:
- Workers: Heartbeat failures
- Workers: Cannot send NotifyPhaseComplete

**ë³µêµ¬ ì ˆì°¨**:

**í˜„ì¬ ì„¤ê³„**: Master crashëŠ” ë³µêµ¬ ë¶ˆê°€ â†’ ì „ì²´ job ì¬ì‹œì‘ í•„ìš”

**í–¥í›„ ê°œì„  (Checkpoint-based Recovery)**:
```scala
class MasterNode {
  // Periodically checkpoint state
  def saveCheckpoint(): Unit = {
    val checkpoint = MasterCheckpoint(
      registeredWorkers = registeredWorkers.asScala.toSeq,
      currentPhase = getCurrentPhase(),
      partitionConfig = currentPartitionConfig,
      phaseCompletions = phaseTracker.getCompletions,
      timestamp = System.currentTimeMillis()
    )

    val checkpointFile = new File("/tmp/master_checkpoint.json")
    val json = Json.toJson(checkpoint).toString()
    Files.write(checkpointFile.toPath, json.getBytes(StandardCharsets.UTF_8))

    logger.debug("Saved master checkpoint")
  }

  // Recover from checkpoint on restart
  def recoverFromCheckpoint(): Option[MasterCheckpoint] = {
    val checkpointFile = new File("/tmp/master_checkpoint.json")

    if (checkpointFile.exists()) {
      try {
        val json = new String(Files.readAllBytes(checkpointFile.toPath), StandardCharsets.UTF_8)
        val checkpoint = Json.parse(json).as[MasterCheckpoint]

        logger.info(s"Recovered checkpoint from ${new Date(checkpoint.timestamp)}")
        Some(checkpoint)
      } catch {
        case ex: Exception =>
          logger.error(s"Failed to recover checkpoint: $ex")
          None
      }
    } else {
      None
    }
  }
}
```

**Worker ì¸¡ ëŒ€ì‘**:
```scala
class WorkerNode {
  private def handleMasterFailure(): Unit = {
    logger.error("Master appears to be down")

    // Wait for master recovery
    var waitTime = 0
    val maxWaitTime = 5 * 60 * 1000  // 5 minutes

    while (waitTime < maxWaitTime) {
      Thread.sleep(30000)  // Check every 30 seconds
      waitTime += 30000

      try {
        val healthCheck = masterStub.healthCheck(HealthCheckRequest())
        if (healthCheck.healthy) {
          logger.info("Master recovered, re-registering")
          initialize()  // Re-register
          return
        }
      } catch {
        case _: Exception =>
          logger.debug(s"Master still down, waiting... ($waitTime/$maxWaitTime ms)")
      }
    }

    // Master did not recover
    logger.error("Master did not recover within timeout, exiting")
    gracefulShutdown()
    System.exit(1)
  }
}
```

---

## 6. Data Integrity Guarantees

### 6.1 Atomic Operations

**íŒŒì¼ ì“°ê¸°**:
```scala
def atomicWrite(data: Array[Byte], targetFile: File): Unit = {
  val tempFile = new File(targetFile.getParent, s"${targetFile.getName}.tmp")

  try {
    // Write to temp file
    val outputStream = new BufferedOutputStream(new FileOutputStream(tempFile))
    outputStream.write(data)
    outputStream.close()

    // Atomic rename
    Files.move(tempFile.toPath, targetFile.toPath, StandardCopyOption.ATOMIC_MOVE)

    logger.debug(s"Atomically wrote ${data.length} bytes to ${targetFile.getName}")

  } catch {
    case ex: Exception =>
      tempFile.delete()  // Cleanup on failure
      throw ex
  }
}
```

**Idempotent Operations**:
```scala
// Sampling: Idempotent - same input produces same samples (with fixed seed)
class ReservoirSampler(sampleSize: Int, seed: Long = 12345) {
  private val random = new Random(seed)
  // ... deterministic sampling
}

// Partitioning: Idempotent - same boundaries produce same partitions
class Partitioner(boundaries: Seq[Array[Byte]]) {
  def getPartition(key: Array[Byte]): Int = {
    // Deterministic binary search
    // ...
  }
}

// Merging: Idempotent - same inputs produce same output
class KWayMerger(files: Seq[File]) {
  // Deterministic merge using total ordering
  // ...
}
```

### 6.2 Checksum Verification (Optional)

```scala
import java.security.MessageDigest

class ChecksummedPartitionSender {
  def sendWithChecksum(partitionFile: File, target: WorkerInfo): Future[Boolean] = {
    // Compute checksum
    val checksum = computeChecksum(partitionFile)

    val metadata = PartitionMetadata(
      partitionId = extractPartitionId(partitionFile),
      fileSize = partitionFile.length(),
      checksum = checksum
    )

    // Send metadata first
    val stub = createWorkerStub(target)
    stub.sendPartitionMetadata(metadata).flatMap { ack =>
      if (ack.success) {
        // Then send actual data
        sendPartitionFileStreaming(partitionFile, target)
      } else {
        Future.successful(false)
      }
    }
  }

  private def computeChecksum(file: File): String = {
    val md5 = MessageDigest.getInstance("MD5")
    val inputStream = new BufferedInputStream(new FileInputStream(file))

    val buffer = new Array[Byte](8192)
    var bytesRead = 0

    while ({bytesRead = inputStream.read(buffer); bytesRead != -1}) {
      md5.update(buffer, 0, bytesRead)
    }

    inputStream.close()
    md5.digest().map("%02x".format(_)).mkString
  }
}

class ChecksummedPartitionReceiver {
  private var expectedMetadata: Option[PartitionMetadata] = None

  override def sendPartitionMetadata(metadata: PartitionMetadata): Future[Ack] = {
    logger.info(s"Expecting partition ${metadata.partitionId}: " +
      s"${metadata.fileSize} bytes, checksum ${metadata.checksum}")
    expectedMetadata = Some(metadata)
    Future.successful(Ack(success = true, message = "Metadata received"))
  }

  override def receivePartition(...): StreamObserver[PartitionFile] = {
    new StreamObserver[PartitionFile] {
      override def onCompleted(): Unit = {
        // Verify checksum
        val actualChecksum = computeChecksum(finalFile)
        val expectedChecksum = expectedMetadata.get.checksum

        if (actualChecksum == expectedChecksum) {
          logger.info(s"Checksum verified for partition $partitionId")
          responseObserver.onNext(PartitionFileAck(success = true, ...))
        } else {
          logger.error(s"Checksum mismatch! Expected $expectedChecksum, got $actualChecksum")
          finalFile.delete()
          responseObserver.onNext(PartitionFileAck(
            success = false,
            errorMessage = "Checksum mismatch"
          ))
        }

        responseObserver.onCompleted()
      }
    }
  }
}
```

### 6.3 Exactly-Once vs At-Least-Once Semantics

**í˜„ì¬ ì„¤ê³„**:
- **Sampling**: At-most-once (ì‹¤íŒ¨ ì‹œ ì¬ì „ì†¡ ì•ˆí•¨, ì¶©ë¶„í•œ ìƒ˜í”Œì´ ìˆìœ¼ë©´ OK)
- **Sorting**: Exactly-once (local operation, crashì‹œ ì¬ì‹œì‘)
- **Shuffle**: At-least-once â†’ **Exactly-once** (idempotent operations + atomic writes)
- **Merge**: Exactly-once (deterministic merge + atomic writes)

**Exactly-once ë³´ì¥ ë©”ì»¤ë‹ˆì¦˜**:
```scala
class ExactlyOnceShuffleReceiver {
  private val receivedPartitions = new ConcurrentHashMap[Int, String]()  // partitionId â†’ checksum

  override def receivePartition(...): StreamObserver[PartitionFile] = {
    new StreamObserver[PartitionFile] {
      override def onNext(chunk: PartitionFile): Unit = {
        if (chunk.offset == 0) {
          // First chunk - check for duplicate
          val existingChecksum = receivedPartitions.get(chunk.partitionId)

          if (existingChecksum != null) {
            logger.warn(s"Partition ${chunk.partitionId} already received, " +
              s"ignoring duplicate")
            responseObserver.onNext(PartitionFileAck(
              success = true,
              partitionId = chunk.partitionId,
              receiverWorkerId = workerId,
              errorMessage = "Duplicate, already received"
            ))
            responseObserver.onCompleted()
            return  // Ignore duplicate
          }
        }

        // ... normal processing
      }

      override def onCompleted(): Unit = {
        val checksum = computeChecksum(finalFile)
        receivedPartitions.put(partitionId, checksum)

        logger.info(s"Partition $partitionId received exactly-once (checksum: $checksum)")

        // ... send ack
      }
    }
  }
}
```

---

## 7. Testing Failure Scenarios

### 7.1 Fault Injection Framework

```scala
object FaultInjector {
  private val random = new Random()

  sealed trait FaultType
  case object NetworkDelay extends FaultType
  case object NetworkFailure extends FaultType
  case object ProcessCrash extends FaultType
  case object DiskFull extends FaultType
  case object CorruptData extends FaultType

  case class FaultConfig(
    faultType: FaultType,
    probability: Double,  // 0.0 to 1.0
    phase: Option[WorkerPhase] = None
  )

  private var enabledFaults = List.empty[FaultConfig]

  def enable(faults: FaultConfig*): Unit = {
    enabledFaults = faults.toList
    logger.warn(s"Fault injection enabled: ${faults.mkString(", ")}")
  }

  def disable(): Unit = {
    enabledFaults = List.empty
    logger.info("Fault injection disabled")
  }

  def maybeInjectFault(currentPhase: WorkerPhase): Unit = {
    enabledFaults.filter(_.phase.forall(_ == currentPhase)).foreach { config =>
      if (random.nextDouble() < config.probability) {
        logger.warn(s"Injecting fault: ${config.faultType} in phase $currentPhase")

        config.faultType match {
          case NetworkDelay =>
            Thread.sleep(5000)  // 5 second delay

          case NetworkFailure =>
            throw new StatusRuntimeException(Status.UNAVAILABLE)

          case ProcessCrash =>
            logger.error("FAULT INJECTION: Simulating process crash")
            System.exit(99)

          case DiskFull =>
            throw new IOException("No space left on device (simulated)")

          case CorruptData =>
            // Handled by caller - corrupt data before sending
            logger.warn("Data corruption should be handled by caller")
        }
      }
    }
  }
}
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```scala
// Test: Worker crashes during sorting
FaultInjector.enable(
  FaultConfig(ProcessCrash, probability = 0.1, phase = Some(WorkerPhase.PHASE_SORTING))
)

// Test: Network failures during shuffle
FaultInjector.enable(
  FaultConfig(NetworkFailure, probability = 0.2, phase = Some(WorkerPhase.PHASE_SHUFFLING))
)
```

### 7.2 Chaos Engineering Tests

```scala
class ChaosTest extends FunSuite {
  test("System survives 30% worker crashes during sampling") {
    val master = new MasterNode(expectedWorkers = 10)
    val workers = (0 until 10).map(_ => new WorkerNode(master.address))

    // Inject crashes
    FaultInjector.enable(
      FaultConfig(ProcessCrash, probability = 0.3, phase = Some(WorkerPhase.PHASE_SAMPLING))
    )

    // Run job
    val result = runDistributedSort(master, workers)

    // Verify: job completes with at least 7 workers (70%)
    assert(result.completedWorkers >= 7)
    assert(result.outputValid)
  }

  test("System handles network partitions during shuffle") {
    val master = new MasterNode(expectedWorkers = 5)
    val workers = (0 until 5).map(_ => new WorkerNode(master.address))

    // Inject network failures
    FaultInjector.enable(
      FaultConfig(NetworkFailure, probability = 0.15, phase = Some(WorkerPhase.PHASE_SHUFFLING))
    )

    val result = runDistributedSort(master, workers)

    // Verify: all shuffle transfers eventually succeed (with retries)
    assert(result.allShufflesComplete)
    assert(result.outputValid)
  }

  test("System detects and recovers from disk full") {
    // ... test disk full scenario
  }
}
```

---

## 8. Monitoring & Alerting

### 8.1 Health Metrics

```scala
case class WorkerHealthMetrics(
  workerId: String,
  state: WorkerState,
  cpuUsage: Double,
  memoryUsage: Long,
  diskUsage: Long,
  networkThroughput: Double,
  lastHeartbeat: Long,
  errorsLast5Min: Int
)

class HealthMonitor {
  private val metrics = new ConcurrentHashMap[String, WorkerHealthMetrics]()

  def updateMetrics(workerId: String, newMetrics: WorkerHealthMetrics): Unit = {
    metrics.put(workerId, newMetrics)

    // Check for anomalies
    checkHealthAnomaly(newMetrics)
  }

  private def checkHealthAnomaly(metrics: WorkerHealthMetrics): Unit = {
    if (metrics.memoryUsage > 0.9 * Runtime.getRuntime.maxMemory()) {
      logger.warn(s"Worker ${metrics.workerId} memory usage high: " +
        s"${metrics.memoryUsage / (1024 * 1024)}MB")
    }

    if (metrics.errorsLast5Min > 10) {
      logger.error(s"Worker ${metrics.workerId} error rate high: " +
        s"${metrics.errorsLast5Min} errors in last 5 minutes")
    }

    val staleDuration = System.currentTimeMillis() - metrics.lastHeartbeat
    if (staleDuration > 60000) {
      logger.error(s"Worker ${metrics.workerId} heartbeat stale: " +
        s"last seen ${staleDuration / 1000}s ago")
    }
  }

  def generateHealthReport(): String = {
    val report = new StringBuilder
    report.append("=== Worker Health Report ===\n")

    metrics.asScala.toSeq.sortBy(_._1).foreach { case (workerId, m) =>
      report.append(s"$workerId: ${m.state}, " +
        s"CPU ${m.cpuUsage}%, " +
        s"Mem ${m.memoryUsage / (1024 * 1024)}MB, " +
        s"Errors ${m.errorsLast5Min}\n")
    }

    report.toString()
  }
}
```

### 8.2 Alerting Rules

```scala
sealed trait Alert {
  def severity: AlertSeverity
  def message: String
}

sealed trait AlertSeverity
case object Critical extends AlertSeverity
case object Warning extends AlertSeverity
case object Info extends AlertSeverity

case class WorkerDownAlert(workerId: String) extends Alert {
  def severity = Critical
  def message = s"Worker $workerId is down"
}

case class HighErrorRateAlert(workerId: String, errorCount: Int) extends Alert {
  def severity = Warning
  def message = s"Worker $workerId has high error rate: $errorCount errors"
}

case class SlowProgressAlert(phase: WorkerPhase, duration: Long) extends Alert {
  def severity = Warning
  def message = s"Phase $phase is progressing slowly: ${duration / 1000}s elapsed"
}

class AlertManager {
  private val alertHandlers = new ConcurrentLinkedQueue[Alert => Unit]()

  def registerHandler(handler: Alert => Unit): Unit = {
    alertHandlers.add(handler)
  }

  def fireAlert(alert: Alert): Unit = {
    logger.log(severityToLogLevel(alert.severity), alert.message)

    alertHandlers.asScala.foreach { handler =>
      try {
        handler(alert)
      } catch {
        case ex: Exception =>
          logger.error(s"Alert handler failed: $ex")
      }
    }
  }

  private def severityToLogLevel(severity: AlertSeverity): Level = severity match {
    case Critical => Level.ERROR
    case Warning => Level.WARN
    case Info => Level.INFO
  }
}
```

---

## 9. Error Recovery Best Practices

### 9.1 Design Principles

1. **Fail Fast**: ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬
2. **Graceful Degradation**: ì¼ë¶€ Worker ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰ (ê°€ëŠ¥í•œ ê²½ìš°)
3. **Idempotency**: ëª¨ë“  operationsì€ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„
4. **Atomic Operations**: íŒŒì¼ ì“°ê¸° ë“±ì€ atomicí•˜ê²Œ ìˆ˜í–‰
5. **Clear State Management**: State machineìœ¼ë¡œ ëª…í™•í•œ ìƒíƒœ ì¶”ì 
6. **Comprehensive Logging**: ëª¨ë“  ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ ë¡œê¹…
7. **Timeout Everything**: ëª¨ë“  RPC ë° operationsì— timeout ì„¤ì •

### 9.2 Recovery Decision Tree

```
Error Occurred
    |
    â”œâ”€ Is it transient? (network timeout, etc.)
    â”‚   â”œâ”€ YES â†’ Retry with exponential backoff (max 3 times)
    â”‚   â”‚           |
    â”‚   â”‚           â”œâ”€ Success â†’ Continue
    â”‚   â”‚           â””â”€ Failed after retries â†’ Proceed to next check
    â”‚   â”‚
    â”‚   â””â”€ NO â†’ Proceed to next check
    |
    â”œâ”€ Is it recoverable? (disk full, OOM, etc.)
    â”‚   â”œâ”€ YES â†’ Attempt recovery (cleanup, reduce memory, etc.)
    â”‚   â”‚           |
    â”‚   â”‚           â”œâ”€ Success â†’ Continue
    â”‚   â”‚           â””â”€ Failed â†’ Proceed to next check
    â”‚   â”‚
    â”‚   â””â”€ NO â†’ Proceed to next check
    |
    â”œâ”€ Can we continue without this worker?
    â”‚   â”œâ”€ YES (early phases, >50% alive) â†’ Continue with remaining workers
    â”‚   â””â”€ NO (late phases, critical worker) â†’ Abort entire job
    |
    â””â”€ Abort entire job and cleanup
```

### 9.3 Implementation Checklist

- [ ] All RPC calls have explicit timeouts
- [ ] All RPC calls have retry logic for transient failures
- [ ] All file writes use atomic operations (temp + rename)
- [ ] All operations are idempotent
- [ ] Worker state machine tracks all states
- [ ] State-based cleanup on failure
- [ ] Master tracks worker health with heartbeats
- [ ] Master can detect and handle worker failures
- [ ] Workers can detect and handle master failures
- [ ] Graceful shutdown hooks registered
- [ ] Comprehensive error logging
- [ ] Fault injection framework for testing
- [ ] Chaos engineering tests
- [ ] Health monitoring and alerting
- [ ] Recovery decision logic implemented

---

## ë¬¸ì„œ ì™„ì„±ë„: 95%

**ì™„ë£Œëœ ë¶€ë¶„**:
- âœ… Failure taxonomy (ì¥ì•  ë¶„ë¥˜)
- âœ… Failure detection mechanisms (ê°ì§€ ë©”ì»¤ë‹ˆì¦˜)
- âœ… Comprehensive error recovery matrix
- âœ… RPC retry with exponential backoff
- âœ… State-based cleanup implementations
- âœ… Specific failure scenarios and recovery procedures
- âœ… Data integrity guarantees (atomic operations, checksums)
- âœ… Fault injection framework
- âœ… Monitoring and alerting
- âœ… Best practices and decision trees

**ì¶”ê°€ ê³ ë ¤ì‚¬í•­** (Nice-to-have):
- [ ] Master checkpoint/recovery (í˜„ì¬ëŠ” Master crash â†’ job restart)
- [ ] Worker task migration (failed workerì˜ workë¥¼ ë‹¤ë¥¸ workerì— ì¬í• ë‹¹)
- [ ] Distributed consensus (Raft/Paxos for Master HA)
- [ ] Automated rollback mechanisms

**ë‹¤ìŒ ë¬¸ì„œ**: `5-file-management.md`

---

## Appendix: Removed Section

**Note**: Section 10 (Configurable Recovery Strategies) was removed as it is not required by the project specification. The system uses a simple global restart strategy for worker failures
